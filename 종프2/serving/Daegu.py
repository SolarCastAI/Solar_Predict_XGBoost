import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F 
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import os
import time
import pickle
import json
from datetime import datetime, timedelta
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.impute import SimpleImputer
import xgboost as xgb

# GPU/CUDA ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸš€ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")
if torch.cuda.is_available():
    print(f"   GPU: {torch.cuda.get_device_name(0)}")
    print(f"   CUDA ë²„ì „: {torch.version.cuda}")
    print(f"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    torch.cuda.empty_cache()
    torch.backends.cudnn.benchmark = True
else:
    print("   âš ï¸  CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

warnings.filterwarnings("ignore")

# ë””ë ‰í† ë¦¬ ì„¤ì •
output_dir = "./plots_daegu_transfer"
model_dir = "./saved_models"  # ì œì£¼ ë°±ë³¸ ëª¨ë¸ì´ ì €ì¥ëœ ë””ë ‰í† ë¦¬

os.makedirs(output_dir, exist_ok=True)

print(f"ğŸ“ Plot ì €ì¥ ê²½ë¡œ: {output_dir}")
print(f"ğŸ“ ì œì£¼ ë°±ë³¸ ëª¨ë¸ ê²½ë¡œ: {model_dir}")

plt.style.use('seaborn-v0_8-whitegrid')

# í•œê¸€ í°íŠ¸ ì„¤ì •
import matplotlib.font_manager as fm
import platform

def set_korean_font():
    system = platform.system()
    korean_fonts = ['Malgun Gothic', 'AppleGothic', 'NanumGothic', 
                   'NanumBarunGothic', 'Nanum Gothic', 'DejaVu Sans']
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    
    for font in korean_fonts:
        if font in available_fonts:
            plt.rcParams['font.family'] = font
            print(f"âœ… í•œê¸€ í°íŠ¸ ì„¤ì • ì™„ë£Œ: {font}")
            break
    else:
        print("âš ï¸  í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    plt.rcParams['axes.unicode_minus'] = False

set_korean_font()


# === í‰ê°€ ì§€í‘œ í•¨ìˆ˜ë“¤ ===
def calculate_rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

def calculate_r2(y_true, y_pred):
    return r2_score(y_true, y_pred)

def calculate_mape(y_true, y_pred, method='improved'):
    y_true = np.array(y_true, dtype=np.float64)
    y_pred = np.array(y_pred, dtype=np.float64)
    
    valid_mask = np.isfinite(y_true) & np.isfinite(y_pred)
    y_true_clean = y_true[valid_mask]
    y_pred_clean = y_pred[valid_mask]
    
    if len(y_true_clean) == 0:
        return np.nan
    
    if method == 'improved':
        threshold = np.percentile(y_true_clean, 10)
        significant_mask = y_true_clean >= threshold
        
        if not np.any(significant_mask):
            abs_errors = np.abs(y_true_clean - y_pred_clean)
            mean_actual = np.mean(y_true_clean)
            if mean_actual > 0:
                return (np.mean(abs_errors) / mean_actual) * 100
            else:
                return 0.0
        
        y_true_sig = y_true_clean[significant_mask]
        y_pred_sig = y_pred_clean[significant_mask]
        
        weights = y_true_sig / np.sum(y_true_sig)
        percentage_errors = np.abs((y_true_sig - y_pred_sig) / y_true_sig)
        percentage_errors = np.clip(percentage_errors, 0, 2)
        
        mape_value = np.sum(weights * percentage_errors) * 100
        
    return mape_value

def calculate_all_metrics(y_true, y_pred, print_details=False):
    metrics = {
        'mae': mean_absolute_error(y_true, y_pred),
        'rmse': calculate_rmse(y_true, y_pred),
        'r2': calculate_r2(y_true, y_pred),
        'mape': calculate_mape(y_true, y_pred, method='improved'),
    }
    
    data_range = np.max(y_true) - np.min(y_true)
    metrics['nmae'] = metrics['mae'] / data_range if data_range > 0 else 0
    metrics['nrmse'] = metrics['rmse'] / data_range if data_range > 0 else 0
    
    if print_details:
        print(f"\n=== í‰ê°€ ì§€í‘œ ===")
        print(f"MAE: {metrics['mae']:.4f}")
        print(f"RMSE: {metrics['rmse']:.4f}")
        print(f"NMAE: {metrics['nmae']:.4f}")
        print(f"NRMSE: {metrics['nrmse']:.4f}")
        print(f"RÂ²: {metrics['r2']:.4f}")
        print(f"MAPE: {metrics['mape']:.2f}%")
    
    return metrics


# load_daegu_data í•¨ìˆ˜ ë‚´ì—ì„œ feature_cols ë¶€ë¶„ì„ ìˆ˜ì •
def load_daegu_data(file_path, sequence_length=24):
    """
    ëŒ€êµ¬ CSV ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
    """
    print("\n" + "="*80)
    print("ëŒ€êµ¬ ë°ì´í„° ë¡œë”© ì¤‘...")
    print("="*80)
    
    df = pd.read_csv(file_path)
    print(f"ì›ë³¸ ë°ì´í„° í¬ê¸°: {df.shape}")
    print(f"ì»¬ëŸ¼: {df.columns.tolist()}")
    
    # ë‚ ì§œ ì²˜ë¦¬
    df['ë°œì „ì¼ì'] = pd.to_datetime(df['ë°œì „ì¼ì'])
    
    # ì»¬ëŸ¼ ë§¤í•‘
    column_mapping = {
        'ë°œì „ì¼ì': 'datetime',
        'ê¸°ì˜¨': 'temperature',
        'ê°•ìš°ëŸ‰(mm)': 'precipitation',
        'ìŠµë„': 'humidity',
        'ì ì„¤ëŸ‰(mm)': 'snow',
        'ì ìš´ëŸ‰(10ë¶„ìœ„)': 'cloud_cover',
        'ì¼ì¡°(hr)': 'sunshine_duration',
        'ì¼ì‚¬ëŸ‰': 'solar_radiation',
        'ì„¤ë¹„ìš©ëŸ‰(MW)': 'solar_capacity',
        'ë°œì „ëŸ‰(MWh)': 'solar_generation'
    }
    
    df_renamed = df.rename(columns=column_mapping)
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    df_renamed['precipitation'] = df_renamed['precipitation'].fillna(0)
    df_renamed['snow'] = df_renamed['snow'].fillna(0)
    df_renamed['sunshine_duration'] = df_renamed['sunshine_duration'].fillna(0)
    df_renamed['solar_radiation'] = df_renamed['solar_radiation'].fillna(0)
    df_renamed['humidity'] = df_renamed['humidity'].fillna(df_renamed['humidity'].mean())
    df_renamed['temperature'] = df_renamed['temperature'].fillna(df_renamed['temperature'].mean())
    df_renamed['cloud_cover'] = df_renamed['cloud_cover'].fillna(5)
    
    # ì‹œê°„ íŠ¹ì„± ì¶”ê°€
    df_renamed['hour'] = df_renamed['datetime'].dt.hour
    df_renamed['month'] = df_renamed['datetime'].dt.month
    df_renamed['day_of_year'] = df_renamed['datetime'].dt.dayofyear
    df_renamed['is_daytime'] = ((df_renamed['hour'] >= 6) & (df_renamed['hour'] <= 18)).astype(int)
    
    # íƒœì–‘ ê³ ë„ê° (ëŒ€êµ¬ ìœ„ë„ 35.87)
    latitude = 35.87
    df_renamed['solar_altitude'] = np.sin(np.radians(
        90 - latitude + 23.45 * np.sin(np.radians(360/365 * (df_renamed['day_of_year'] - 81)))
    )) * np.sin(np.radians(15 * (df_renamed['hour'] - 12)))
    
    print(f"\në°ì´í„° ê¸°ê°„: {df_renamed['datetime'].min()} ~ {df_renamed['datetime'].max()}")
    print(f"í‰ê·  ë°œì „ëŸ‰: {df_renamed['solar_generation'].mean():.2f} MWh")
    print(f"ì„¤ë¹„ìš©ëŸ‰: {df_renamed['solar_capacity'].iloc[0]:.2f} MW")
    
    # â­ ì œì£¼ ëª¨ë¸ê³¼ ë™ì¼í•œ 8ê°œ íŠ¹ì„±ë§Œ ì„ íƒ
    feature_cols = [
        'temperature', 'precipitation', 'humidity', 'cloud_cover',
        'sunshine_duration', 'solar_radiation', 'solar_capacity', 'hour'
    ]
    
    print(f"\nâš ï¸  ì œì£¼ ëª¨ë¸ í˜¸í™˜ì„ ìœ„í•´ {len(feature_cols)}ê°œ íŠ¹ì„± ì‚¬ìš©:")
    print(f"   {feature_cols}")
    
    target_col = 'solar_generation'
    
    # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì„ íƒ
    df_valid = df_renamed[df_renamed[target_col].notna()].copy()
    
    X = df_valid[feature_cols].values
    y = df_valid[target_col].values.reshape(-1, 1)
    dates = df_valid['datetime'].values
    
    # ìŠ¤ì¼€ì¼ë§
    scaler_X = MinMaxScaler()
    scaler_y = MinMaxScaler()
    
    X_scaled = scaler_X.fit_transform(X)
    y_scaled = scaler_y.fit_transform(y)
    
    # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
    X_seq, y_seq, date_seq = [], [], []
    for i in range(len(X_scaled) - sequence_length):
        X_seq.append(X_scaled[i:i+sequence_length])
        y_seq.append(y_scaled[i+sequence_length])
        date_seq.append(dates[i+sequence_length])
    
    X_seq = np.array(X_seq)
    y_seq = np.array(y_seq)
    
    print(f"\nì‹œí€€ìŠ¤ ë°ì´í„° shape: X={X_seq.shape}, y={y_seq.shape}")
    
    # Train/Val/Test ë¶„í•  (80/10/10)
    X_temp, X_test, y_temp, y_test, date_temp, date_test = train_test_split(
        X_seq, y_seq, date_seq, test_size=0.1, random_state=42
    )
    X_train, X_val, y_train, y_val, date_train, date_val = train_test_split(
        X_temp, y_temp, date_temp, test_size=0.111, random_state=42
    )
    
    print(f"\në°ì´í„° ë¶„í• :")
    print(f"  Train: {X_train.shape} ({len(X_train)/len(X_seq)*100:.1f}%)")
    print(f"  Val: {X_val.shape} ({len(X_val)/len(X_seq)*100:.1f}%)")
    print(f"  Test: {X_test.shape} ({len(X_test)/len(X_seq)*100:.1f}%)")
    
    return (X_train, X_val, X_test, y_train, y_val, y_test,
            scaler_X, scaler_y, feature_cols, df_valid, date_test)


# === PyTorch Dataset ===
class TimeSeriesDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


# === LSTM ëª¨ë¸ ===
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(
            input_size, hidden_size, num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        self.fc = nn.Linear(hidden_size, 1)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        lstm_out = self.dropout(lstm_out[:, -1, :])
        output = self.fc(lstm_out)
        return output


# === GRU ëª¨ë¸ ===
class GRUModel(nn.Module):
    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2):
        super(GRUModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.gru = nn.GRU(
            input_size, hidden_size, num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        self.fc = nn.Linear(hidden_size, 1)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        gru_out, _ = self.gru(x)
        gru_out = self.dropout(gru_out[:, -1, :])
        output = self.fc(gru_out)
        return output


# === ì œì£¼ ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ ===
def load_jeju_pretrained_models(model_dir='./saved_models', timestamp=None):
    """
    ì œì£¼ ë°ì´í„°ë¡œ ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
    """
    print(f"\n{'='*80}")
    print("ì œì£¼ ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    print(f"{'='*80}")
    
    # ìµœì‹  ëª¨ë¸ ì •ë³´ ë¡œë“œ
    if timestamp is None:
        latest_path = os.path.join(model_dir, 'latest_models.json')
        if not os.path.exists(latest_path):
            raise FileNotFoundError(f"ìµœì‹  ëª¨ë¸ ì •ë³´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {latest_path}")
        
        with open(latest_path, 'r', encoding='utf-8') as f:
            model_info = json.load(f)
        
        timestamp = model_info['timestamp']
        print(f"ìµœì‹  ëª¨ë¸ íƒ€ì„ìŠ¤íƒ¬í”„: {timestamp}")
    
    # ë©”íƒ€ë°ì´í„° ë¡œë“œ
    metadata_path = os.path.join(model_dir, f'metadata_{timestamp}.json')
    with open(metadata_path, 'r', encoding='utf-8') as f:
        metadata = json.load(f)
    
    print(f"âœ… ë©”íƒ€ë°ì´í„° ë¡œë“œ: {metadata_path}")
    
    # LSTM ëª¨ë¸ ë¡œë“œ
    lstm_path = os.path.join(model_dir, f'lstm_model_{timestamp}.pth')
    lstm_checkpoint = torch.load(lstm_path, map_location=device)
    lstm_config = lstm_checkpoint['model_config']
    
    lstm_model = LSTMModel(
        input_size=lstm_config['input_size'],
        hidden_size=lstm_config['hidden_size'],
        num_layers=lstm_config['num_layers']
    ).to(device)
    lstm_model.load_state_dict(lstm_checkpoint['model_state_dict'])
    print(f"âœ… LSTM ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë“œ: {lstm_path}")
    
    # GRU ëª¨ë¸ ë¡œë“œ
    gru_path = os.path.join(model_dir, f'gru_model_{timestamp}.pth')
    gru_checkpoint = torch.load(gru_path, map_location=device)
    gru_config = gru_checkpoint['model_config']
    
    gru_model = GRUModel(
        input_size=gru_config['input_size'],
        hidden_size=gru_config['hidden_size'],
        num_layers=gru_config['num_layers']
    ).to(device)
    gru_model.load_state_dict(gru_checkpoint['model_state_dict'])
    print(f"âœ… GRU ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë“œ: {gru_path}")
    
    print(f"\n{'='*80}")
    print(f"âœ¨ ì œì£¼ ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    print(f"{'='*80}")
    
    return lstm_model, gru_model, metadata


# === ì „ì´í•™ìŠµ í•¨ìˆ˜ ===
def transfer_learning(model, train_loader, val_loader, criterion, 
                     num_epochs=50, patience=10, learning_rate=0.0001, 
                     freeze_layers=False, device='cpu', model_name='Model'):
    """
    ì „ì´í•™ìŠµ (Fine-tuning)
    """
    print(f"\n{'='*80}")
    print(f"{model_name} ì „ì´í•™ìŠµ ì‹œì‘")
    print(f"{'='*80}")
    print(f"í•™ìŠµë¥ : {learning_rate}")
    print(f"ë ˆì´ì–´ ë™ê²°: {freeze_layers}")
    
    # ë ˆì´ì–´ ë™ê²° ì˜µì…˜
    if freeze_layers:
        # LSTM/GRU ë ˆì´ì–´ëŠ” ë™ê²°í•˜ê³  FC ë ˆì´ì–´ë§Œ í•™ìŠµ
        for param in model.parameters():
            param.requires_grad = False
        for param in model.fc.parameters():
            param.requires_grad = True
        print("âš ï¸  ìˆœí™˜ ë ˆì´ì–´ ë™ê²°, FC ë ˆì´ì–´ë§Œ í•™ìŠµ")
    else:
        # ëª¨ë“  ë ˆì´ì–´ í•™ìŠµ
        for param in model.parameters():
            param.requires_grad = True
        print("âœ… ì „ì²´ ë ˆì´ì–´ ë¯¸ì„¸ì¡°ì •")
    
    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), 
                          lr=learning_rate)
    
    best_val_loss = float('inf')
    patience_counter = 0
    train_losses = []
    val_losses = []
    
    start_time = time.time()
    
    for epoch in range(num_epochs):
        # Training
        model.train()
        train_loss = 0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
        
        avg_train_loss = train_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        
        # Validation
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
                val_loss += loss.item()
        
        avg_val_loss = val_loss / len(val_loader)
        val_losses.append(avg_val_loss)
        
        if (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{num_epochs}] "
                  f"Train Loss: {avg_train_loss:.4f} | "
                  f"Val Loss: {avg_val_loss:.4f}")
        
        # Early Stopping
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            best_model_state = model.state_dict().copy()
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"\nEarly stopping at epoch {epoch+1}")
                break
    
    model.load_state_dict(best_model_state)
    
    elapsed_time = time.time() - start_time
    print(f"{model_name} ì „ì´í•™ìŠµ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
    
    return model, train_losses, val_losses


# === ì˜ˆì¸¡ í•¨ìˆ˜ ===
def predict(model, test_loader, device='cpu'):
    model.eval()
    predictions = []
    actuals = []
    
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            X_batch = X_batch.to(device)
            outputs = model(X_batch)
            predictions.extend(outputs.cpu().numpy())
            actuals.extend(y_batch.numpy())
    
    return np.array(predictions), np.array(actuals)


# === ë¯¸ë˜ ì˜ˆì¸¡ í•¨ìˆ˜ ===
def predict_future(model, scaler_X, scaler_y, last_sequence, 
                   target_datetime, solar_capacity, device='cpu'):
    """
    íŠ¹ì • ì‹œê°„ì˜ ë°œì „ëŸ‰ ì˜ˆì¸¡
    """
    model.eval()
    
    # ê¸°ìƒ ë°ì´í„° ìƒì„± (ëŒ€êµ¬ ê¸°ì¤€)
    month = target_datetime.month
    hour = target_datetime.hour
    
    # ê³„ì ˆë³„ ê¸°ìƒ íŒ¨í„´
    if month in [11, 12, 1, 2]:
        base_temp = 5
        base_humidity = 60
        base_cloud = 5
    elif month in [3, 4, 5]:
        base_temp = 15
        base_humidity = 55
        base_cloud = 4
    elif month in [6, 7, 8]:
        base_temp = 25
        base_humidity = 70
        base_cloud = 6
    else:
        base_temp = 15
        base_humidity = 65
        base_cloud = 5
    
    # ì‹œê°„ëŒ€ë³„ ì˜¨ë„ ì¡°ì •
    if 6 <= hour <= 12:
        temperature = base_temp + (hour - 6) * 1.5
    elif 12 < hour <= 18:
        temperature = base_temp + 9 - (hour - 12) * 1.0
    else:
        temperature = base_temp - 3
    
    # ì¼ì¡°ì‹œê°„ ë° ì¼ì‚¬ëŸ‰
    if 6 <= hour <= 18:
        sunshine_duration = 0.8 if 9 <= hour <= 15 else 0.3
        solar_radiation = 600 if 9 <= hour <= 15 else 200
    else:
        sunshine_duration = 0
        solar_radiation = 0
    
    # â­ ì œì£¼ ëª¨ë¸ê³¼ ë™ì¼í•œ 8ê°œ íŠ¹ì„±ë§Œ ìƒì„±
    # feature_cols = ['temperature', 'precipitation', 'humidity', 'cloud_cover',
    #                 'sunshine_duration', 'solar_radiation', 'solar_capacity', 'hour']
    new_features = np.array([[
        temperature,           # temperature
        0,                     # precipitation
        base_humidity,         # humidity
        base_cloud,            # cloud_cover
        sunshine_duration,     # sunshine_duration
        solar_radiation,       # solar_radiation
        solar_capacity,        # solar_capacity
        hour                   # hour
    ]])
    
    # ìŠ¤ì¼€ì¼ë§
    new_features_scaled = scaler_X.transform(new_features)
    
    # ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸
    new_sequence = np.vstack([last_sequence[1:], new_features_scaled])
    new_sequence_tensor = torch.FloatTensor(new_sequence).unsqueeze(0).to(device)
    
    # ì˜ˆì¸¡
    with torch.no_grad():
        prediction_scaled = model(new_sequence_tensor).cpu().numpy()
        prediction = scaler_y.inverse_transform(prediction_scaled)[0, 0]
    
    return max(0, prediction), new_sequence

# === ì „ì´í•™ìŠµ ëª¨ë¸ ì €ì¥ í•¨ìˆ˜ ìˆ˜ì • ===
def save_transfer_models(lstm_model, gru_model, scaler_X, scaler_y, 
                        lstm_metrics, gru_metrics, 
                        region_name, model_dir='./saved_models'):
    """
    ì „ì´í•™ìŠµëœ ëª¨ë¸ ì €ì¥
    """
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    save_dir = os.path.join(model_dir, f'transfer_{region_name}')
    os.makedirs(save_dir, exist_ok=True)
    
    # â­ Metricsë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
    def convert_to_native(obj):
        """NumPy íƒ€ì…ì„ Python ë„¤ì´í‹°ë¸Œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜"""
        if isinstance(obj, dict):
            return {key: convert_to_native(value) for key, value in obj.items()}
        elif isinstance(obj, (np.integer, np.int32, np.int64)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float32, np.float64)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return obj
    
    lstm_metrics_native = convert_to_native(lstm_metrics)
    gru_metrics_native = convert_to_native(gru_metrics)
    
    # LSTM ëª¨ë¸ ì €ì¥
    lstm_path = os.path.join(save_dir, f'lstm_transfer_{region_name}_{timestamp}.pth')
    torch.save({
        'model_state_dict': lstm_model.state_dict(),
        'model_config': {
            'input_size': lstm_model.lstm.input_size,
            'hidden_size': lstm_model.hidden_size,
            'num_layers': lstm_model.num_layers
        },
        'metrics': lstm_metrics_native,
        'timestamp': timestamp,
        'region': region_name
    }, lstm_path)
    print(f"âœ… LSTM ì „ì´í•™ìŠµ ëª¨ë¸ ì €ì¥: {lstm_path}")
    
    # GRU ëª¨ë¸ ì €ì¥
    gru_path = os.path.join(save_dir, f'gru_transfer_{region_name}_{timestamp}.pth')
    torch.save({
        'model_state_dict': gru_model.state_dict(),
        'model_config': {
            'input_size': gru_model.gru.input_size,
            'hidden_size': gru_model.hidden_size,
            'num_layers': gru_model.num_layers
        },
        'metrics': gru_metrics_native,
        'timestamp': timestamp,
        'region': region_name
    }, gru_path)
    print(f"âœ… GRU ì „ì´í•™ìŠµ ëª¨ë¸ ì €ì¥: {gru_path}")
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    scaler_path = os.path.join(save_dir, f'scalers_{region_name}_{timestamp}.pkl')
    with open(scaler_path, 'wb') as f:
        pickle.dump({'scaler_X': scaler_X, 'scaler_y': scaler_y}, f)
    print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥: {scaler_path}")
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        'timestamp': timestamp,
        'region': region_name,
        'lstm_metrics': lstm_metrics_native,
        'gru_metrics': gru_metrics_native,
        'models': {
            'lstm': lstm_path,
            'gru': gru_path,
            'scalers': scaler_path
        }
    }
    
    metadata_path = os.path.join(save_dir, f'metadata_{region_name}_{timestamp}.json')
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    print(f"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")
    
    # ìµœì‹  ëª¨ë¸ ì •ë³´ ì €ì¥
    latest_path = os.path.join(save_dir, f'latest_model_{region_name}.json')
    with open(latest_path, 'w', encoding='utf-8') as f:
        json.dump({'timestamp': timestamp, 'region': region_name}, f, indent=2)
    
    return timestamp

# === ë©”ì¸ ì‹¤í–‰ ===
if __name__ == "__main__":
    try:
        # 1. ì œì£¼ ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë“œ
        lstm_pretrained, gru_pretrained, jeju_metadata = load_jeju_pretrained_models(
            model_dir=model_dir
        )
        
        print(f"\nì œì£¼ ëª¨ë¸ ì„±ëŠ¥:")
        print(f"  LSTM RÂ²: {jeju_metadata['lstm_metrics']['r2']:.4f}")
        print(f"  GRU RÂ²: {jeju_metadata['gru_metrics']['r2']:.4f}")
        print(f"  Stacking RÂ²: {jeju_metadata['stacked_metrics']['r2']:.4f}")
        
        # 2. ëŒ€êµ¬ ë°ì´í„° ë¡œë”©
        SEQUENCE_LENGTH = 24
        daegu_csv_path = "./dataset/output_by_region/ëŒ€êµ¬.csv"  
        
        (X_train, X_val, X_test, y_train, y_val, y_test,
         scaler_X, scaler_y, feature_cols, df_valid, date_test) = load_daegu_data(
            daegu_csv_path, SEQUENCE_LENGTH
        )
        
        # 3. DataLoader ìƒì„±
        BATCH_SIZE = 32
        train_dataset = TimeSeriesDataset(X_train, y_train)
        val_dataset = TimeSeriesDataset(X_val, y_val)
        test_dataset = TimeSeriesDataset(X_test, y_test)
        
        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)
        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)
        
        # 4. LSTM ì „ì´í•™ìŠµ
        print("\n" + "="*80)
        print("LSTM ì „ì´í•™ìŠµ (ëŒ€êµ¬)")
        print("="*80)
        
        criterion = nn.MSELoss()
        lstm_model, lstm_train_losses, lstm_val_losses = transfer_learning(
            lstm_pretrained, train_loader, val_loader, criterion,
            num_epochs=50, patience=10, learning_rate=0.0001,
            freeze_layers=False, device=device, model_name='LSTM'
        )
        
        # 5. GRU ì „ì´í•™ìŠµ
        print("\n" + "="*80)
        print("GRU ì „ì´í•™ìŠµ (ëŒ€êµ¬)")
        print("="*80)
        
        gru_model, gru_train_losses, gru_val_losses = transfer_learning(
            gru_pretrained, train_loader, val_loader, criterion,
            num_epochs=50, patience=10, learning_rate=0.0001,
            freeze_layers=False, device=device, model_name='GRU'
        )
        
        # 6. ë¯¸ë˜ ë°œì „ëŸ‰ ì˜ˆì¸¡ (24H, 48H, 72H)
        print("\n" + "="*80)
        print("ë¯¸ë˜ ë°œì „ëŸ‰ ì˜ˆì¸¡ (ëŒ€êµ¬) - ì „ì´í•™ìŠµ ëª¨ë¸ ì‚¬ìš©")
        print("="*80)
        
        solar_capacity = df_valid['solar_capacity'].iloc[0]
        current_time = datetime.now()  # ì‹¤ì œ í˜„ì¬ ì‹œê° ì‚¬ìš©
        last_sequence = X_test[-1]  # ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ ì‚¬ìš©
        
        print(f"\nğŸ“ í˜„ì¬ ì‹œê°: {current_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ“Š ì„¤ë¹„ìš©ëŸ‰: {solar_capacity:.2f} MW")
        print(f"\nì „ì´í•™ìŠµëœ LSTM + GRU ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¯¸ë˜ ë°œì „ëŸ‰ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.")
        
        # 24H, 48H, 72H í›„ ì˜ˆì¸¡
        for hours_ahead in [24, 48, 72]:
            target_date = current_time + timedelta(hours=hours_ahead)
            print(f"\n{'='*70}")
            print(f"ğŸ“… {hours_ahead}ì‹œê°„ í›„ ì˜ˆì¸¡: {target_date.strftime('%Y-%m-%d %A')}")
            print(f"{'='*70}")
            
            daily_predictions_lstm = []
            daily_predictions_gru = []
            daily_predictions_ensemble = []
            temp_sequence = last_sequence.copy()
            hourly_details = []
            
            for h in range(24):
                target_time = current_time + timedelta(hours=hours_ahead+h)
                
                # LSTM ì˜ˆì¸¡
                lstm_pred, temp_sequence = predict_future(
                    lstm_model, scaler_X, scaler_y, temp_sequence,
                    target_time, solar_capacity, device
                )
                
                # GRU ì˜ˆì¸¡
                gru_pred, _ = predict_future(
                    gru_model, scaler_X, scaler_y, temp_sequence,
                    target_time, solar_capacity, device
                )
                
                # ì•™ìƒë¸” (LSTM + GRU í‰ê· )
                ensemble_pred = (lstm_pred + gru_pred) / 2
                
                daily_predictions_lstm.append(max(0, lstm_pred))
                daily_predictions_gru.append(max(0, gru_pred))
                daily_predictions_ensemble.append(max(0, ensemble_pred))
                
                # ì‹œê°„ë³„ ìƒì„¸ ì •ë³´ ì €ì¥
                hourly_details.append({
                    'time': target_time.strftime('%H:%M'),
                    'lstm': lstm_pred,
                    'gru': gru_pred,
                    'ensemble': ensemble_pred
                })
            
            # ì¼ì¼ í†µê³„ ê³„ì‚°
            total_lstm = sum(daily_predictions_lstm)
            total_gru = sum(daily_predictions_gru)
            total_ensemble = sum(daily_predictions_ensemble)
            
            peak_lstm = max(daily_predictions_lstm)
            peak_gru = max(daily_predictions_gru)
            peak_ensemble = max(daily_predictions_ensemble)
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"\n[LSTM ëª¨ë¸ ì˜ˆì¸¡]")
            print(f"  ì¼ì¼ ì´ ë°œì „ëŸ‰: {total_lstm:.2f} MWh")
            print(f"  í”¼í¬ ë°œì „ëŸ‰: {peak_lstm:.2f} MWh (ì‹œê°„ë‹¹)")
            print(f"  í‰ê·  ì‹œê°„ë‹¹: {total_lstm/24:.2f} MWh")
            print(f"  í‰ê·  ê°€ë™ë¥ : {(total_lstm/(solar_capacity*24))*100:.1f}%")
            
            print(f"\n[GRU ëª¨ë¸ ì˜ˆì¸¡]")
            print(f"  ì¼ì¼ ì´ ë°œì „ëŸ‰: {total_gru:.2f} MWh")
            print(f"  í”¼í¬ ë°œì „ëŸ‰: {peak_gru:.2f} MWh (ì‹œê°„ë‹¹)")
            print(f"  í‰ê·  ì‹œê°„ë‹¹: {total_gru/24:.2f} MWh")
            print(f"  í‰ê·  ê°€ë™ë¥ : {(total_gru/(solar_capacity*24))*100:.1f}%")
            
            print(f"\n[ì•™ìƒë¸” ì˜ˆì¸¡ (LSTM+GRU í‰ê· )] â­ ê¶Œì¥")
            print(f"  ì¼ì¼ ì´ ë°œì „ëŸ‰: {total_ensemble:.2f} MWh")
            print(f"  í”¼í¬ ë°œì „ëŸ‰: {peak_ensemble:.2f} MWh (ì‹œê°„ë‹¹)")
            print(f"  í‰ê·  ì‹œê°„ë‹¹: {total_ensemble/24:.2f} MWh")
            print(f"  í‰ê·  ê°€ë™ë¥ : {(total_ensemble/(solar_capacity*24))*100:.1f}%")
            
            # ì‹œê°„ë³„ ìƒì„¸ ì˜ˆì¸¡ (ì£¼ìš” ë°œì „ ì‹œê°„ëŒ€ë§Œ ì¶œë ¥)
            print(f"\nâ° ì‹œê°„ë³„ ë°œì „ëŸ‰ ìƒì„¸ (ì•™ìƒë¸” ê¸°ì¤€, ë°œì „ëŸ‰ > 0.5 MWh):")
            print("-" * 60)
            for detail in hourly_details:
                if detail['ensemble'] > 0.5:
                    print(f"  {detail['time']} - {detail['ensemble']:6.2f} MWh "
                          f"(LSTM: {detail['lstm']:5.2f}, GRU: {detail['gru']:5.2f})")
        
        # 7. ìµœì¢… ìš”ì•½
        
        # 7. ìµœì¢… ìš”ì•½
        print(f"\n{'='*80}")
        print("ëŒ€êµ¬ ì „ì´í•™ìŠµ ë° ë¯¸ë˜ ì˜ˆì¸¡ ì™„ë£Œ!")
        print(f"{'='*80}")

        # ê¸°ì¡´ ì½”ë“œì˜ 6ë²ˆ ì„¹ì…˜ ì´í›„ì— ì¶”ê°€

        # 7. í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€ ë° ëª¨ë¸ ì €ì¥
        print("\n" + "="*80)
        print("í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€")
        print("="*80)
        
        # LSTM í‰ê°€
        lstm_predictions, lstm_actuals = predict(lstm_model, test_loader, device)
        lstm_predictions_original = scaler_y.inverse_transform(lstm_predictions)
        lstm_actuals_original = scaler_y.inverse_transform(lstm_actuals)
        
        lstm_metrics = calculate_all_metrics(
            lstm_actuals_original, lstm_predictions_original, print_details=True
        )
        
        print("\n[LSTM ì „ì´í•™ìŠµ ëª¨ë¸ ì„±ëŠ¥]")
        print(f"  MAE: {lstm_metrics['mae']:.4f} MWh")
        print(f"  RMSE: {lstm_metrics['rmse']:.4f} MWh")
        print(f"  RÂ²: {lstm_metrics['r2']:.4f}")
        print(f"  MAPE: {lstm_metrics['mape']:.2f}%")
        
        # GRU í‰ê°€
        gru_predictions, gru_actuals = predict(gru_model, test_loader, device)
        gru_predictions_original = scaler_y.inverse_transform(gru_predictions)
        gru_actuals_original = scaler_y.inverse_transform(gru_actuals)
        
        gru_metrics = calculate_all_metrics(
            gru_actuals_original, gru_predictions_original, print_details=True
        )
        
        print("\n[GRU ì „ì´í•™ìŠµ ëª¨ë¸ ì„±ëŠ¥]")
        print(f"  MAE: {gru_metrics['mae']:.4f} MWh")
        print(f"  RMSE: {gru_metrics['rmse']:.4f} MWh")
        print(f"  RÂ²: {gru_metrics['r2']:.4f}")
        print(f"  MAPE: {gru_metrics['mape']:.2f}%")
        
        # ì•™ìƒë¸” í‰ê°€
        ensemble_predictions = (lstm_predictions_original + gru_predictions_original) / 2
        ensemble_metrics = calculate_all_metrics(
            lstm_actuals_original, ensemble_predictions, print_details=True
        )
        
        print("\n[ì•™ìƒë¸” (LSTM+GRU) ì„±ëŠ¥]")
        print(f"  MAE: {ensemble_metrics['mae']:.4f} MWh")
        print(f"  RMSE: {ensemble_metrics['rmse']:.4f} MWh")
        print(f"  RÂ²: {ensemble_metrics['r2']:.4f}")
        print(f"  MAPE: {ensemble_metrics['mape']:.2f}%")
        
        # 8. ì „ì´í•™ìŠµ ëª¨ë¸ ì €ì¥
        print("\n" + "="*80)
        print("ì „ì´í•™ìŠµ ëª¨ë¸ ì €ì¥")
        print("="*80)
        
        saved_timestamp = save_transfer_models(
            lstm_model, gru_model, scaler_X, scaler_y,
            lstm_metrics, gru_metrics,
            region_name='daegu',
            model_dir=model_dir
        )
        
        print(f"\nâœ… ëŒ€êµ¬ ì „ì´í•™ìŠµ ëª¨ë¸ ì €ì¥ ì™„ë£Œ! (Timestamp: {saved_timestamp})")
        
    except FileNotFoundError as e:
        print(f"Error: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. {e}")
        print("\ní™•ì¸ ì‚¬í•­:")
        print("1. ì œì£¼ ì‚¬ì „í•™ìŠµ ëª¨ë¸ì´ ./saved_models ë””ë ‰í† ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸")
        print("2. ëŒ€êµ¬ CSV íŒŒì¼ ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸")
    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()