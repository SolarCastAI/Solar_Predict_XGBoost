import torch
import torch.nn as nn
import pandas as pd
import numpy as np
import pickle
import json
import os
from datetime import datetime, timedelta
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader
import warnings

warnings.filterwarnings("ignore")

# GPU/CUDA ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸš€ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")
if torch.cuda.is_available():
    print(f"   GPU: {torch.cuda.get_device_name(0)}")
else:
    print("   âš ï¸  CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")


# === LSTM ëª¨ë¸ ì •ì˜ ===
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(
            input_size, hidden_size, num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        self.fc = nn.Linear(hidden_size, 1)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        lstm_out = self.dropout(lstm_out[:, -1, :])
        output = self.fc(lstm_out)
        return output


# === GRU ëª¨ë¸ ì •ì˜ ===
class GRUModel(nn.Module):
    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2):
        super(GRUModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.gru = nn.GRU(
            input_size, hidden_size, num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        self.fc = nn.Linear(hidden_size, 1)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        gru_out, _ = self.gru(x)
        gru_out = self.dropout(gru_out[:, -1, :])
        output = self.fc(gru_out)
        return output


# === PyTorch Dataset ===
class TimeSeriesDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


# === í‰ê°€ ì§€í‘œ í•¨ìˆ˜ë“¤ ===
def calculate_rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

def calculate_r2(y_true, y_pred):
    return r2_score(y_true, y_pred)

def calculate_mape(y_true, y_pred, method='improved'):
    y_true = np.array(y_true, dtype=np.float64)
    y_pred = np.array(y_pred, dtype=np.float64)
    
    valid_mask = np.isfinite(y_true) & np.isfinite(y_pred)
    y_true_clean = y_true[valid_mask]
    y_pred_clean = y_pred[valid_mask]
    
    if len(y_true_clean) == 0:
        return np.nan
    
    if method == 'improved':
        threshold = np.percentile(y_true_clean, 10)
        significant_mask = y_true_clean >= threshold
        
        if not np.any(significant_mask):
            abs_errors = np.abs(y_true_clean - y_pred_clean)
            mean_actual = np.mean(y_true_clean)
            if mean_actual > 0:
                return (np.mean(abs_errors) / mean_actual) * 100
            else:
                return 0.0
        
        y_true_sig = y_true_clean[significant_mask]
        y_pred_sig = y_pred_clean[significant_mask]
        
        weights = y_true_sig / np.sum(y_true_sig)
        percentage_errors = np.abs((y_true_sig - y_pred_sig) / y_true_sig)
        percentage_errors = np.clip(percentage_errors, 0, 2)
        
        mape_value = np.sum(weights * percentage_errors) * 100
        
    return mape_value

def calculate_all_metrics(y_true, y_pred, print_details=True):
    metrics = {
        'mae': mean_absolute_error(y_true, y_pred),
        'rmse': calculate_rmse(y_true, y_pred),
        'r2': calculate_r2(y_true, y_pred),
        'mape': calculate_mape(y_true, y_pred, method='improved'),
    }
    
    data_range = np.max(y_true) - np.min(y_true)
    metrics['nmae'] = metrics['mae'] / data_range if data_range > 0 else 0
    metrics['nrmse'] = metrics['rmse'] / data_range if data_range > 0 else 0
    
    if print_details:
        print(f"\n{'='*60}")
        print(f"ğŸ“Š í‰ê°€ ì§€í‘œ")
        print(f"{'='*60}")
        print(f"MAE (í‰ê·  ì ˆëŒ€ ì˜¤ì°¨):     {metrics['mae']:.4f} MWh")
        print(f"RMSE (í‰ê·  ì œê³±ê·¼ ì˜¤ì°¨):  {metrics['rmse']:.4f} MWh")
        print(f"NMAE (ì •ê·œí™” MAE):        {metrics['nmae']:.4f}")
        print(f"NRMSE (ì •ê·œí™” RMSE):      {metrics['nrmse']:.4f}")
        print(f"RÂ² (ê²°ì •ê³„ìˆ˜):            {metrics['r2']:.4f}")
        print(f"MAPE (í‰ê·  ì ˆëŒ€ ë°±ë¶„ìœ¨):  {metrics['mape']:.2f}%")
        print(f"{'='*60}")
    
    return metrics


# === ë¶€ì‚° ë°ì´í„° ë¡œë”© í•¨ìˆ˜ ===
def load_busan_data(file_path, sequence_length=24):
    """
    ë¶€ì‚° CSV ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
    """
    print("\n" + "="*80)
    print("ë¶€ì‚° ë°ì´í„° ë¡œë”© ì¤‘...")
    print("="*80)
    
    df = pd.read_csv(file_path, encoding='utf-8-sig')
    print(f"ì›ë³¸ ë°ì´í„° í¬ê¸°: {df.shape}")
    print(f"ì»¬ëŸ¼: {df.columns.tolist()}")
    
    # ë‚ ì§œ ì²˜ë¦¬
    df['ë°œì „ì¼ì'] = pd.to_datetime(df['ë°œì „ì¼ì'])
    
    # ì»¬ëŸ¼ ë§¤í•‘ (ë¶€ì‚° ë°ì´í„° í˜•ì‹ì— ë§ê²Œ)
    column_mapping = {
        'ë°œì „ì¼ì': 'datetime',
        'ê¸°ì˜¨': 'temperature',
        'ê°•ìš°ëŸ‰(mm)': 'precipitation',
        'ìŠµë„': 'humidity',
        'ì ì„¤ëŸ‰(mm)': 'snow',
        'ì ìš´ëŸ‰(10ë¶„ìœ„)': 'cloud_cover',
        'ì¼ì¡°(hr)': 'sunshine_duration',
        'ì¼ì‚¬ëŸ‰': 'solar_radiation',
        'ì„¤ë¹„ìš©ëŸ‰(MW)': 'solar_capacity',
        'ë°œì „ëŸ‰(MWh)': 'solar_generation'
    }
    
    df_renamed = df.rename(columns=column_mapping)
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    df_renamed['precipitation'] = df_renamed['precipitation'].fillna(0)
    df_renamed['snow'] = df_renamed['snow'].fillna(0)
    df_renamed['sunshine_duration'] = df_renamed['sunshine_duration'].fillna(0)
    df_renamed['solar_radiation'] = df_renamed['solar_radiation'].fillna(0)
    df_renamed['humidity'] = df_renamed['humidity'].fillna(df_renamed['humidity'].mean())
    df_renamed['temperature'] = df_renamed['temperature'].fillna(df_renamed['temperature'].mean())
    
    # cloud_cover ì²˜ë¦¬ (ì ìš´ëŸ‰(10ë¶„ìœ„) ì‚¬ìš©)
    if 'cloud_cover' in df_renamed.columns:
        df_renamed['cloud_cover'] = df_renamed['cloud_cover'].fillna(5)
    else:
        # ì ìš´ëŸ‰(3ë¶„ìœ„)ê°€ ìˆëŠ” ê²½ìš° 10ë¶„ìœ„ë¡œ ë³€í™˜
        if 'ì ìš´ëŸ‰(3ë¶„ìœ„)' in df.columns:
            df_renamed['cloud_cover'] = df['ì ìš´ëŸ‰(3ë¶„ìœ„)'].fillna(1) * 3.33
        else:
            df_renamed['cloud_cover'] = 5  # ê¸°ë³¸ê°’
    
    # ì‹œê°„ íŠ¹ì„± ì¶”ê°€
    df_renamed['hour'] = df_renamed['datetime'].dt.hour
    df_renamed['month'] = df_renamed['datetime'].dt.month
    df_renamed['day_of_year'] = df_renamed['datetime'].dt.dayofyear
    
    print(f"\në°ì´í„° ê¸°ê°„: {df_renamed['datetime'].min()} ~ {df_renamed['datetime'].max()}")
    print(f"í‰ê·  ë°œì „ëŸ‰: {df_renamed['solar_generation'].mean():.2f} MWh")
    print(f"ì„¤ë¹„ìš©ëŸ‰: {df_renamed['solar_capacity'].iloc[0]:.2f} MW")
    
    # â­ ì œì£¼/ëŒ€êµ¬ ëª¨ë¸ê³¼ ë™ì¼í•œ 8ê°œ íŠ¹ì„±ë§Œ ì„ íƒ
    feature_cols = [
        'temperature', 'precipitation', 'humidity', 'cloud_cover',
        'sunshine_duration', 'solar_radiation', 'solar_capacity', 'hour'
    ]
    
    print(f"\nì‚¬ìš© íŠ¹ì„± ({len(feature_cols)}ê°œ):")
    for i, col in enumerate(feature_cols, 1):
        print(f"  {i}. {col}")
    
    target_col = 'solar_generation'
    
    # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì„ íƒ
    df_valid = df_renamed[df_renamed[target_col].notna()].copy()
    
    X = df_valid[feature_cols].values
    y = df_valid[target_col].values.reshape(-1, 1)
    dates = df_valid['datetime'].values
    
    # ìŠ¤ì¼€ì¼ë§
    scaler_X = MinMaxScaler()
    scaler_y = MinMaxScaler()
    
    X_scaled = scaler_X.fit_transform(X)
    y_scaled = scaler_y.fit_transform(y)
    
    # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
    X_seq, y_seq, date_seq = [], [], []
    for i in range(len(X_scaled) - sequence_length):
        X_seq.append(X_scaled[i:i+sequence_length])
        y_seq.append(y_scaled[i+sequence_length])
        date_seq.append(dates[i+sequence_length])
    
    X_seq = np.array(X_seq)
    y_seq = np.array(y_seq)
    
    print(f"\nì‹œí€€ìŠ¤ ë°ì´í„° shape: X={X_seq.shape}, y={y_seq.shape}")
    
    # Train/Val/Test ë¶„í•  (80/10/10)
    X_temp, X_test, y_temp, y_test, date_temp, date_test = train_test_split(
        X_seq, y_seq, date_seq, test_size=0.1, random_state=42
    )
    X_train, X_val, y_train, y_val, date_train, date_val = train_test_split(
        X_temp, y_temp, date_temp, test_size=0.111, random_state=42
    )
    
    print(f"\në°ì´í„° ë¶„í• :")
    print(f"  Train: {X_train.shape} ({len(X_train)/len(X_seq)*100:.1f}%)")
    print(f"  Val: {X_val.shape} ({len(X_val)/len(X_seq)*100:.1f}%)")
    print(f"  Test: {X_test.shape} ({len(X_test)/len(X_seq)*100:.1f}%)")
    
    return (X_train, X_val, X_test, y_train, y_val, y_test,
            scaler_X, scaler_y, feature_cols, df_valid, date_test)


# === ëŒ€êµ¬ ì „ì´í•™ìŠµ ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ ===
def load_daegu_transfer_models(model_dir='./saved_models/transfer_daegu', timestamp=None):
    """
    ëŒ€êµ¬ ì „ì´í•™ìŠµ ëª¨ë¸ ë¡œë“œ
    """
    print(f"\n{'='*80}")
    print("ëŒ€êµ¬ ì „ì´í•™ìŠµ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    print(f"{'='*80}")
    
    # ìµœì‹  ëª¨ë¸ ì •ë³´ ë¡œë“œ
    if timestamp is None:
        latest_path = os.path.join(model_dir, 'latest_model_daegu.json')
        if not os.path.exists(latest_path):
            # latest íŒŒì¼ì´ ì—†ìœ¼ë©´ ë””ë ‰í† ë¦¬ì—ì„œ ê°€ì¥ ìµœê·¼ ë©”íƒ€ë°ì´í„° ì°¾ê¸°
            metadata_files = [f for f in os.listdir(model_dir) if f.startswith('metadata_daegu_') and f.endswith('.json')]
            if not metadata_files:
                raise FileNotFoundError(f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_dir}")
            
            # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì„ íƒ
            metadata_files.sort(reverse=True)
            metadata_path = os.path.join(model_dir, metadata_files[0])
            timestamp = metadata_files[0].replace('metadata_daegu_', '').replace('.json', '')
            print(f"âš ï¸  latest íŒŒì¼ ì—†ìŒ. ê°€ì¥ ìµœê·¼ ëª¨ë¸ ì‚¬ìš©: {timestamp}")
        else:
            with open(latest_path, 'r', encoding='utf-8') as f:
                model_info = json.load(f)
            timestamp = model_info['timestamp']
            print(f"ìµœì‹  ëª¨ë¸ íƒ€ì„ìŠ¤íƒ¬í”„: {timestamp}")
            metadata_path = os.path.join(model_dir, f'metadata_daegu_{timestamp}.json')
    else:
        metadata_path = os.path.join(model_dir, f'metadata_daegu_{timestamp}.json')
    
    # ë©”íƒ€ë°ì´í„° ë¡œë“œ
    with open(metadata_path, 'r', encoding='utf-8') as f:
        metadata = json.load(f)
    
    print(f"âœ… ë©”íƒ€ë°ì´í„° ë¡œë“œ: {metadata_path}")
    
    # LSTM ëª¨ë¸ ë¡œë“œ
    lstm_path = os.path.join(model_dir, f'lstm_transfer_daegu_{timestamp}.pth')
    lstm_checkpoint = torch.load(lstm_path, map_location=device)
    lstm_config = lstm_checkpoint['model_config']
    
    lstm_model = LSTMModel(
        input_size=lstm_config['input_size'],
        hidden_size=lstm_config['hidden_size'],
        num_layers=lstm_config['num_layers']
    ).to(device)
    lstm_model.load_state_dict(lstm_checkpoint['model_state_dict'])
    print(f"âœ… LSTM ì „ì´í•™ìŠµ ëª¨ë¸ ë¡œë“œ: {lstm_path}")
    
    # GRU ëª¨ë¸ ë¡œë“œ
    gru_path = os.path.join(model_dir, f'gru_transfer_daegu_{timestamp}.pth')
    gru_checkpoint = torch.load(gru_path, map_location=device)
    gru_config = gru_checkpoint['model_config']
    
    gru_model = GRUModel(
        input_size=gru_config['input_size'],
        hidden_size=gru_config['hidden_size'],
        num_layers=gru_config['num_layers']
    ).to(device)
    gru_model.load_state_dict(gru_checkpoint['model_state_dict'])
    print(f"âœ… GRU ì „ì´í•™ìŠµ ëª¨ë¸ ë¡œë“œ: {gru_path}")
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
    scaler_path = os.path.join(model_dir, f'scalers_daegu_{timestamp}.pkl')
    with open(scaler_path, 'rb') as f:
        scalers = pickle.load(f)
    print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ: {scaler_path}")
    
    print(f"\nëŒ€êµ¬ ëª¨ë¸ í•™ìŠµ ì„±ëŠ¥ (ë©”íƒ€ë°ì´í„°):")
    print(f"  LSTM RÂ²: {metadata['lstm_metrics']['r2']:.4f}")
    print(f"  LSTM MAPE: {metadata['lstm_metrics']['mape']:.2f}%")
    print(f"  GRU RÂ²: {metadata['gru_metrics']['r2']:.4f}")
    print(f"  GRU MAPE: {metadata['gru_metrics']['mape']:.2f}%")
    
    print(f"\n{'='*80}")
    print(f"âœ¨ ëŒ€êµ¬ ì „ì´í•™ìŠµ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    print(f"{'='*80}")
    
    return lstm_model, gru_model, scalers['scaler_X'], scalers['scaler_y'], metadata


# === ì˜ˆì¸¡ í•¨ìˆ˜ ===
def predict(model, test_loader, device='cpu'):
    model.eval()
    predictions = []
    actuals = []
    
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            X_batch = X_batch.to(device)
            outputs = model(X_batch)
            predictions.extend(outputs.cpu().numpy())
            actuals.extend(y_batch.numpy())
    
    return np.array(predictions), np.array(actuals)


# ==========================================
# â­ ë¯¸ë˜ ì˜ˆì¸¡ í•¨ìˆ˜ (í˜„ì¬ ë‚ ì§œ ê¸°ì¤€, ì‹œê°„ë³„ ëˆ„ì  ë°œì „ëŸ‰ í¬í•¨)
# ==========================================
def predict_future_single_step(model, scaler_X, scaler_y, last_sequence, 
                                target_time, solar_capacity, device='cpu'):
    """
    ë‹¨ì¼ ì‹œì  ë¯¸ë˜ ì˜ˆì¸¡ (1ì‹œê°„ í›„)
    """
    model.eval()
    
    # ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ì˜ í‰ê· ê°’ìœ¼ë¡œ ë‹¤ìŒ ì‹œì  íŠ¹ì„± ì¶”ì •
    last_features = last_sequence[-1].copy()
    
    # ì‹œê°„ íŠ¹ì„± ì—…ë°ì´íŠ¸ (hour)
    target_hour = target_time.hour
    hour_scaled = target_hour / 23.0  # MinMax ìŠ¤ì¼€ì¼ë§ ê·¼ì‚¬
    last_features[7] = hour_scaled  # hourëŠ” 8ë²ˆì§¸ íŠ¹ì„±
    
    # ìƒˆ ì‹œí€€ìŠ¤ ìƒì„± (sliding window)
    new_sequence = np.vstack([last_sequence[1:], last_features.reshape(1, -1)])
    
    # ì˜ˆì¸¡
    with torch.no_grad():
        X_tensor = torch.FloatTensor(new_sequence).unsqueeze(0).to(device)
        pred_scaled = model(X_tensor).cpu().numpy()
        pred_original = scaler_y.inverse_transform(pred_scaled)[0, 0]
    
    return max(0, pred_original), new_sequence


def generate_future_predictions(lstm_model, gru_model, scaler_X, scaler_y, 
                                 X_test, df_valid, device='cpu'):
    """
    24H, 48H, 72H ë¯¸ë˜ ì˜ˆì¸¡ ìƒì„± (í˜„ì¬ ë‚ ì§œ ê¸°ì¤€, ì‹œê°„ë³„ í˜„ì¬/ëˆ„ì  ë°œì „ëŸ‰ í¬í•¨)
    """
    print("\n" + "="*80)
    print("ğŸ”® ë¯¸ë˜ ì˜ˆì¸¡ ìƒì„± ì¤‘ (24H, 48H, 72H)...")
    print("="*80)
    
    # ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ ì„ íƒ
    last_sequence = X_test[-1].copy()
    
    # â­ í˜„ì¬ ë‚ ì§œ/ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •
    current_time = datetime.now()
    print(f"í˜„ì¬ ì‹œê°„: {current_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    solar_capacity = df_valid['solar_capacity'].iloc[0]
    
    predictions_list = []
    
    # ëˆ„ì  ë°œì „ëŸ‰ ì¶”ì  ë³€ìˆ˜
    lstm_cumulative = 0
    gru_cumulative = 0
    ensemble_cumulative = 0
    
    # 72ì‹œê°„ ì˜ˆì¸¡
    for hour in range(1, 73):
        target_time = current_time + timedelta(hours=hour)
        
        # LSTM ì˜ˆì¸¡
        lstm_pred, last_sequence = predict_future_single_step(
            lstm_model, scaler_X, scaler_y, last_sequence,
            target_time, solar_capacity, device
        )
        
        # GRU ì˜ˆì¸¡
        gru_pred, _ = predict_future_single_step(
            gru_model, scaler_X, scaler_y, last_sequence,
            target_time, solar_capacity, device
        )
        
        # ì•™ìƒë¸” (í‰ê· )
        ensemble_pred = (lstm_pred + gru_pred) / 2
        
        # ëˆ„ì  ë°œì „ëŸ‰ ì—…ë°ì´íŠ¸
        lstm_cumulative += lstm_pred
        gru_cumulative += gru_pred
        ensemble_cumulative += ensemble_pred
        
        predictions_list.append({
            'ì˜ˆì¸¡_ë‚ ì§œ': target_time.strftime('%Y-%m-%d'),
            'ì˜ˆì¸¡_ì‹œê°„': target_time.strftime('%H:%M'),
            'ì˜ˆì¸¡_ì¼ì‹œ': target_time.strftime('%Y-%m-%d %H:%M:%S'),
            'ê²½ê³¼_ì‹œê°„(H)': hour,
            'LSTM_í˜„ì¬_ë°œì „ëŸ‰(MWh)': round(lstm_pred, 4),
            'LSTM_ëˆ„ì _ë°œì „ëŸ‰(MWh)': round(lstm_cumulative, 4),
            'GRU_í˜„ì¬_ë°œì „ëŸ‰(MWh)': round(gru_pred, 4),
            'GRU_ëˆ„ì _ë°œì „ëŸ‰(MWh)': round(gru_cumulative, 4),
            'ì•™ìƒë¸”_í˜„ì¬_ë°œì „ëŸ‰(MWh)': round(ensemble_pred, 4),
            'ì•™ìƒë¸”_ëˆ„ì _ë°œì „ëŸ‰(MWh)': round(ensemble_cumulative, 4)
        })
        
        if hour % 24 == 0:
            print(f"  {hour}H ì˜ˆì¸¡ ì™„ë£Œ (ëˆ„ì : ì•™ìƒë¸” {ensemble_cumulative:.2f} MWh)")
    
    return pd.DataFrame(predictions_list)


def save_prediction_csvs(predictions_df, output_dir='./prediction_results'):
    """
    24H, 48H, 72H ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë³„ë„ CSV íŒŒì¼ë¡œ ì €ì¥ (ì‹œê°„ë³„ í˜„ì¬/ëˆ„ì  ë°œì „ëŸ‰ í¬í•¨)
    """
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # 24H ì˜ˆì¸¡
    pred_24h = predictions_df[predictions_df['ê²½ê³¼_ì‹œê°„(H)'] <= 24].copy()
    file_24h = os.path.join(output_dir, f'prediction_24H_{timestamp}.csv')
    pred_24h.to_csv(file_24h, index=False, encoding='utf-8-sig')
    print(f"âœ… 24H ì˜ˆì¸¡ ì €ì¥: {file_24h}")
    print(f"   - ì´ {len(pred_24h)}ê°œ ì‹œê°„ë³„ ë°ì´í„°")
    print(f"   - ëˆ„ì  ë°œì „ëŸ‰(ì•™ìƒë¸”): {pred_24h['ì•™ìƒë¸”_ëˆ„ì _ë°œì „ëŸ‰(MWh)'].iloc[-1]:.2f} MWh")
    
    # 48H ì˜ˆì¸¡
    pred_48h = predictions_df[predictions_df['ê²½ê³¼_ì‹œê°„(H)'] <= 48].copy()
    file_48h = os.path.join(output_dir, f'prediction_48H_{timestamp}.csv')
    pred_48h.to_csv(file_48h, index=False, encoding='utf-8-sig')
    print(f"âœ… 48H ì˜ˆì¸¡ ì €ì¥: {file_48h}")
    print(f"   - ì´ {len(pred_48h)}ê°œ ì‹œê°„ë³„ ë°ì´í„°")
    print(f"   - ëˆ„ì  ë°œì „ëŸ‰(ì•™ìƒë¸”): {pred_48h['ì•™ìƒë¸”_ëˆ„ì _ë°œì „ëŸ‰(MWh)'].iloc[-1]:.2f} MWh")
    
    # 72H ì˜ˆì¸¡
    pred_72h = predictions_df.copy()
    file_72h = os.path.join(output_dir, f'prediction_72H_{timestamp}.csv')
    pred_72h.to_csv(file_72h, index=False, encoding='utf-8-sig')
    print(f"âœ… 72H ì˜ˆì¸¡ ì €ì¥: {file_72h}")
    print(f"   - ì´ {len(pred_72h)}ê°œ ì‹œê°„ë³„ ë°ì´í„°")
    print(f"   - ëˆ„ì  ë°œì „ëŸ‰(ì•™ìƒë¸”): {pred_72h['ì•™ìƒë¸”_ëˆ„ì _ë°œì „ëŸ‰(MWh)'].iloc[-1]:.2f} MWh")
    
    # ì „ì²´ ì˜ˆì¸¡ (í†µí•©ë³¸)
    file_all = os.path.join(output_dir, f'prediction_ALL_{timestamp}.csv')
    predictions_df.to_csv(file_all, index=False, encoding='utf-8-sig')
    print(f"âœ… ì „ì²´ ì˜ˆì¸¡ ì €ì¥: {file_all}")
    
    return {
        '24H': file_24h,
        '48H': file_48h,
        '72H': file_72h,
        'ALL': file_all
    }


# === ë©”ì¸ ì‹¤í–‰ ===
if __name__ == "__main__":
    try:
        print("\n" + "="*80)
        print("ğŸ”¥ ëŒ€êµ¬ ì „ì´í•™ìŠµ ëª¨ë¸ì˜ ë¶€ì‚° ë°ì´í„° ì„±ëŠ¥ í‰ê°€ + ë¯¸ë˜ ì˜ˆì¸¡")
        print("="*80)
        
        # 1. ëŒ€êµ¬ ì „ì´í•™ìŠµ ëª¨ë¸ ë¡œë“œ
        lstm_model, gru_model, daegu_scaler_X, daegu_scaler_y, metadata = load_daegu_transfer_models(
            model_dir='./saved_models/transfer_daegu'
        )
        
        # 2. ë¶€ì‚° ë°ì´í„° ë¡œë”©
        SEQUENCE_LENGTH = 24
        busan_csv_path = "./dataset/output_by_region/ë¶€ì‚°.csv"
        
        if not os.path.exists(busan_csv_path):
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {busan_csv_path}")
            print("í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ CSV íŒŒì¼ì„ ì§€ì •í•´ì£¼ì„¸ìš”.")
            exit(1)
        
        (X_train, X_val, X_test, y_train, y_val, y_test,
         scaler_X, scaler_y, feature_cols, df_valid, date_test) = load_busan_data(
            busan_csv_path, SEQUENCE_LENGTH
        )
        
        # 3. DataLoader ìƒì„±
        BATCH_SIZE = 32
        test_dataset = TimeSeriesDataset(X_test, y_test)
        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)
        
        # 4. LSTM ëª¨ë¸ í‰ê°€
        print("\n" + "="*80)
        print("ğŸ“ˆ LSTM ëª¨ë¸ í‰ê°€ (ë¶€ì‚° ë°ì´í„°)")
        print("="*80)
        
        lstm_predictions, lstm_actuals = predict(lstm_model, test_loader, device)
        lstm_predictions_original = scaler_y.inverse_transform(lstm_predictions)
        lstm_actuals_original = scaler_y.inverse_transform(lstm_actuals)
        
        print("\n[LSTM ëª¨ë¸ ì„±ëŠ¥ - ë¶€ì‚° ë°ì´í„°]")
        lstm_metrics = calculate_all_metrics(
            lstm_actuals_original, lstm_predictions_original, print_details=True
        )
        
        # 5. GRU ëª¨ë¸ í‰ê°€
        print("\n" + "="*80)
        print("ğŸ“ˆ GRU ëª¨ë¸ í‰ê°€ (ë¶€ì‚° ë°ì´í„°)")
        print("="*80)
        
        gru_predictions, gru_actuals = predict(gru_model, test_loader, device)
        gru_predictions_original = scaler_y.inverse_transform(gru_predictions)
        gru_actuals_original = scaler_y.inverse_transform(gru_actuals)
        
        print("\n[GRU ëª¨ë¸ ì„±ëŠ¥ - ë¶€ì‚° ë°ì´í„°]")
        gru_metrics = calculate_all_metrics(
            gru_actuals_original, gru_predictions_original, print_details=True
        )
        
        # 6. ì•™ìƒë¸” ëª¨ë¸ í‰ê°€
        print("\n" + "="*80)
        print("ğŸ“ˆ ì•™ìƒë¸” ëª¨ë¸ í‰ê°€ (LSTM + GRU í‰ê· , ë¶€ì‚° ë°ì´í„°)")
        print("="*80)
        
        ensemble_predictions = (lstm_predictions_original + gru_predictions_original) / 2
        
        print("\n[ì•™ìƒë¸” ëª¨ë¸ ì„±ëŠ¥ - ë¶€ì‚° ë°ì´í„°] â­ ê¶Œì¥")
        ensemble_metrics = calculate_all_metrics(
            lstm_actuals_original, ensemble_predictions, print_details=True
        )
        
        # ==========================================
        # â­ 7. ë¯¸ë˜ ì˜ˆì¸¡ ìƒì„± (24H, 48H, 72H) - í˜„ì¬ ë‚ ì§œ ê¸°ì¤€
        # ==========================================
        predictions_df = generate_future_predictions(
            lstm_model, gru_model, scaler_X, scaler_y,
            X_test, df_valid, device
        )
        
        # ==========================================
        # â­ 8. ì˜ˆì¸¡ ê²°ê³¼ CSV ì €ì¥ (ì‹œê°„ë³„ í˜„ì¬/ëˆ„ì  ë°œì „ëŸ‰ í¬í•¨)
        # ==========================================
        print("\n" + "="*80)
        print("ğŸ’¾ ì˜ˆì¸¡ ê²°ê³¼ CSV ì €ì¥ ì¤‘...")
        print("="*80)
        
        saved_files = save_prediction_csvs(predictions_df, output_dir='./prediction_results')
        
        print("\n" + "="*80)
        print("ğŸ“ ì €ì¥ëœ íŒŒì¼ ëª©ë¡:")
        print("="*80)
        for period, filepath in saved_files.items():
            print(f"  [{period:>3}] {filepath}")
        
        # 9. ìµœì¢… ìš”ì•½
        print("\n" + "="*80)
        print("ğŸ“Š ì„±ëŠ¥ ë¹„êµ ìš”ì•½")
        print("="*80)
        
        print("\nëŒ€êµ¬ í•™ìŠµ ì„±ëŠ¥ (ì›ë³¸ ì§€ì—­):")
        print(f"  LSTM  - RÂ²: {metadata['lstm_metrics']['r2']:.4f}, MAPE: {metadata['lstm_metrics']['mape']:.2f}%")
        print(f"  GRU   - RÂ²: {metadata['gru_metrics']['r2']:.4f}, MAPE: {metadata['gru_metrics']['mape']:.2f}%")
        
        print("\në¶€ì‚° ì ìš© ì„±ëŠ¥ (ì „ì´ ì§€ì—­):")
        print(f"  LSTM      - RÂ²: {lstm_metrics['r2']:.4f}, NMAE: {lstm_metrics['nmae']:.4f}, MAPE: {lstm_metrics['mape']:.2f}%")
        print(f"  GRU       - RÂ²: {gru_metrics['r2']:.4f}, NMAE: {gru_metrics['nmae']:.4f}, MAPE: {gru_metrics['mape']:.2f}%")
        print(f"  ì•™ìƒë¸”    - RÂ²: {ensemble_metrics['r2']:.4f}, NMAE: {ensemble_metrics['nmae']:.4f}, MAPE: {ensemble_metrics['mape']:.2f}%")
        
        # 10. ì˜ˆì¸¡ ìƒ˜í”Œ ì¶œë ¥
        print("\n" + "="*80)
        print("ğŸ”® ë¯¸ë˜ ì˜ˆì¸¡ ìƒ˜í”Œ (ì²˜ìŒ 10ì‹œê°„)")
        print("="*80)
        print(predictions_df.head(10)[['ì˜ˆì¸¡_ì¼ì‹œ', 'ê²½ê³¼_ì‹œê°„(H)', 'ì•™ìƒë¸”_í˜„ì¬_ë°œì „ëŸ‰(MWh)', 'ì•™ìƒë¸”_ëˆ„ì _ë°œì „ëŸ‰(MWh)']].to_string(index=False))
        
        print("\n" + "="*80)
        print("ğŸ”® ë¯¸ë˜ ì˜ˆì¸¡ ìš”ì•½")
        print("="*80)
        print(f"24H í›„ ëˆ„ì  ë°œì „ëŸ‰: {predictions_df[predictions_df['ê²½ê³¼_ì‹œê°„(H)'] == 24]['ì•™ìƒë¸”_ëˆ„ì _ë°œì „ëŸ‰(MWh)'].values[0]:.2f} MWh")
        print(f"48H í›„ ëˆ„ì  ë°œì „ëŸ‰: {predictions_df[predictions_df['ê²½ê³¼_ì‹œê°„(H)'] == 48]['ì•™ìƒë¸”_ëˆ„ì _ë°œì „ëŸ‰(MWh)'].values[0]:.2f} MWh")
        print(f"72H í›„ ëˆ„ì  ë°œì „ëŸ‰: {predictions_df[predictions_df['ê²½ê³¼_ì‹œê°„(H)'] == 72]['ì•™ìƒë¸”_ëˆ„ì _ë°œì „ëŸ‰(MWh)'].values[0]:.2f} MWh")
        
        print("\n" + "="*80)
        print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        print("="*80)
        
    except FileNotFoundError as e:
        print(f"\nâŒ ì—ëŸ¬: {e}")
        print("\ní™•ì¸ ì‚¬í•­:")
        print("1. ëŒ€êµ¬ ì „ì´í•™ìŠµ ëª¨ë¸ì´ ./saved_models/transfer_daegu ë””ë ‰í† ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸")
        print("2. ë¶€ì‚° CSV íŒŒì¼ ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸")
    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()