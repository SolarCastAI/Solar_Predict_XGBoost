import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
import warnings
import os
import json
import pickle
from datetime import datetime, timedelta, timezone
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from torch.utils.data import Dataset, DataLoader

# --- ê¸°ë³¸ ì„¤ì • ---
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
warnings.filterwarnings("ignore")
model_dir = "./saved_models/transfer_daegu"

# ==========================================
# 1. Dataset í´ë˜ìŠ¤
# ==========================================
class TimeSeriesDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


# ==========================================
# 2. AI ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ (LSTM, GRU)
# ==========================================
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(
            input_size, hidden_size, num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        self.fc = nn.Linear(hidden_size, 1)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        lstm_out = self.dropout(lstm_out[:, -1, :])
        output = self.fc(lstm_out)
        return output


class GRUModel(nn.Module):
    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2):
        super(GRUModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.gru = nn.GRU(
            input_size, hidden_size, num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        self.fc = nn.Linear(hidden_size, 1)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        gru_out, _ = self.gru(x)
        gru_out = self.dropout(gru_out[:, -1, :])
        output = self.fc(gru_out)
        return output


# ==========================================
# 3. í‰ê°€ ì§€í‘œ í•¨ìˆ˜ë“¤
# ==========================================
def calculate_rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

def calculate_r2(y_true, y_pred):
    return r2_score(y_true, y_pred)

def calculate_mape(y_true, y_pred, method='improved'):
    y_true = np.array(y_true, dtype=np.float64)
    y_pred = np.array(y_pred, dtype=np.float64)
    
    valid_mask = np.isfinite(y_true) & np.isfinite(y_pred)
    y_true_clean = y_true[valid_mask]
    y_pred_clean = y_pred[valid_mask]
    
    if len(y_true_clean) == 0:
        return np.nan
    
    if method == 'improved':
        threshold = np.percentile(y_true_clean, 10)
        significant_mask = y_true_clean >= threshold
        
        if not np.any(significant_mask):
            abs_errors = np.abs(y_true_clean - y_pred_clean)
            mean_actual = np.mean(y_true_clean)
            if mean_actual > 0:
                return (np.mean(abs_errors) / mean_actual) * 100
            else:
                return 0.0
        
        y_true_sig = y_true_clean[significant_mask]
        y_pred_sig = y_pred_clean[significant_mask]
        
        weights = y_true_sig / np.sum(y_true_sig)
        percentage_errors = np.abs((y_true_sig - y_pred_sig) / y_true_sig)
        percentage_errors = np.clip(percentage_errors, 0, 2)
        
        mape_value = np.sum(weights * percentage_errors) * 100
        
    return mape_value


# ==========================================
# 4. ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜
# ==========================================
def load_daegu_transfer_models(model_dir='./saved_models/transfer_daegu', timestamp=None):
    """ëŒ€êµ¬ ì „ì´í•™ìŠµ ëª¨ë¸ ë¡œë“œ"""
    
    # ìµœì‹  ëª¨ë¸ ì •ë³´ ë¡œë“œ
    if timestamp is None:
        latest_path = os.path.join(model_dir, 'latest_model_daegu.json')
        if not os.path.exists(latest_path):
            # latest íŒŒì¼ì´ ì—†ìœ¼ë©´ ë””ë ‰í† ë¦¬ì—ì„œ ê°€ì¥ ìµœê·¼ ë©”íƒ€ë°ì´í„° ì°¾ê¸°
            metadata_files = [f for f in os.listdir(model_dir) 
                            if f.startswith('metadata_daegu_') and f.endswith('.json')]
            if not metadata_files:
                raise FileNotFoundError(f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_dir}")
            
            metadata_files.sort(reverse=True)
            metadata_path = os.path.join(model_dir, metadata_files[0])
            timestamp = metadata_files[0].replace('metadata_daegu_', '').replace('.json', '')
        else:
            with open(latest_path, 'r', encoding='utf-8') as f:
                model_info = json.load(f)
            timestamp = model_info['timestamp']
            metadata_path = os.path.join(model_dir, f'metadata_daegu_{timestamp}.json')
    else:
        metadata_path = os.path.join(model_dir, f'metadata_daegu_{timestamp}.json')
    
    # ë©”íƒ€ë°ì´í„° ë¡œë“œ
    with open(metadata_path, 'r', encoding='utf-8') as f:
        metadata = json.load(f)
    
    # LSTM ëª¨ë¸ ë¡œë“œ
    lstm_path = os.path.join(model_dir, f'lstm_transfer_daegu_{timestamp}.pth')
    lstm_checkpoint = torch.load(lstm_path, map_location=device, weights_only=False)
    lstm_config = lstm_checkpoint['model_config']
    
    lstm_model = LSTMModel(
        input_size=lstm_config['input_size'],
        hidden_size=lstm_config['hidden_size'],
        num_layers=lstm_config['num_layers']
    ).to(device)
    lstm_model.load_state_dict(lstm_checkpoint['model_state_dict'])
    
    # GRU ëª¨ë¸ ë¡œë“œ
    gru_path = os.path.join(model_dir, f'gru_transfer_daegu_{timestamp}.pth')
    gru_checkpoint = torch.load(gru_path, map_location=device, weights_only=False)
    gru_config = gru_checkpoint['model_config']
    
    gru_model = GRUModel(
        input_size=gru_config['input_size'],
        hidden_size=gru_config['hidden_size'],
        num_layers=gru_config['num_layers']
    ).to(device)
    gru_model.load_state_dict(gru_checkpoint['model_state_dict'])
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
    scaler_path = os.path.join(model_dir, f'scalers_daegu_{timestamp}.pkl')
    with open(scaler_path, 'rb') as f:
        scalers = pickle.load(f)
    
    return lstm_model, gru_model, scalers['scaler_X'], scalers['scaler_y'], metadata


# ==========================================
# 5. ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
# ==========================================
def preprocess_data_from_db(df, sequence_length=24):
    """
    [ì¶”ë¡ ìš©] DB ë°ì´í„°ë¥¼ ëª¨ë¸ ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜
    ë¶€ì‚° ë°ì´í„° í˜•ì‹ì— ë§ì¶° ì „ì²˜ë¦¬
    """
    df_renamed = df.copy()
    
    # ë‚ ì§œ ì²˜ë¦¬
    if 'datetime' in df_renamed.columns:
        df_renamed['datetime'] = pd.to_datetime(df_renamed['datetime'])
    elif 'ë°œì „ì¼ì' in df_renamed.columns:
        df_renamed['datetime'] = pd.to_datetime(df_renamed['ë°œì „ì¼ì'])
        
        # ì»¬ëŸ¼ ë§¤í•‘ (ë¶€ì‚° ë°ì´í„° í˜•ì‹)
        column_mapping = {
            'ë°œì „ì¼ì': 'datetime',
            'ê¸°ì˜¨': 'temperature',
            'ê°•ìš°ëŸ‰(mm)': 'precipitation',
            'ìŠµë„': 'humidity',
            'ì ì„¤ëŸ‰(mm)': 'snow',
            'ì ìš´ëŸ‰(10ë¶„ìœ„)': 'cloud_cover',
            'ì¼ì¡°(hr)': 'sunshine_duration',
            'ì¼ì‚¬ëŸ‰': 'solar_radiation',
            'ì„¤ë¹„ìš©ëŸ‰(MW)': 'solar_capacity',
            'ë°œì „ëŸ‰(MWh)': 'solar_generation'
        }
        df_renamed = df_renamed.rename(columns=column_mapping)
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    df_renamed['precipitation'] = df_renamed['precipitation'].fillna(0)
    df_renamed['snow'] = df_renamed['snow'].fillna(0)
    df_renamed['sunshine_duration'] = df_renamed['sunshine_duration'].fillna(0)
    df_renamed['solar_radiation'] = df_renamed['solar_radiation'].fillna(0)
    df_renamed['humidity'] = df_renamed['humidity'].fillna(df_renamed['humidity'].mean())
    df_renamed['temperature'] = df_renamed['temperature'].fillna(df_renamed['temperature'].mean())
    
    # cloud_cover ì²˜ë¦¬
    if 'cloud_cover' not in df_renamed.columns:
        if 'ì ìš´ëŸ‰(3ë¶„ìœ„)' in df.columns:
            df_renamed['cloud_cover'] = df['ì ìš´ëŸ‰(3ë¶„ìœ„)'].fillna(1) * 3.33
        else:
            df_renamed['cloud_cover'] = 5
    else:
        df_renamed['cloud_cover'] = df_renamed['cloud_cover'].fillna(5)
    
    # ì‹œê°„ íŠ¹ì„± ì¶”ê°€
    df_renamed['hour'] = df_renamed['datetime'].dt.hour
    
    # ëŒ€êµ¬ ëª¨ë¸ê³¼ ë™ì¼í•œ 8ê°œ íŠ¹ì„± ì‚¬ìš©
    feature_cols = [
        'temperature', 'precipitation', 'humidity', 'cloud_cover',
        'sunshine_duration', 'solar_radiation', 'solar_capacity', 'hour'
    ]
    
    # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì„ íƒ
    target_col = 'solar_generation'
    df_valid = df_renamed[df_renamed[target_col].notna()].copy()
    
    if len(df_valid) < sequence_length + 1:
        return None, None, None, feature_cols, None
    
    X = df_valid[feature_cols].values
    y = df_valid[target_col].values.reshape(-1, 1)
    
    # ìŠ¤ì¼€ì¼ë§
    scaler_X = MinMaxScaler()
    scaler_y = MinMaxScaler()
    
    X_scaled = scaler_X.fit_transform(X)
    y_scaled = scaler_y.fit_transform(y)
    
    # ì‹œí€€ìŠ¤ ìƒì„±
    X_seq = []
    for i in range(len(X_scaled) - sequence_length):
        X_seq.append(X_scaled[i:i+sequence_length])
    
    X_seq = np.array(X_seq)
    
    return X_seq, scaler_X, scaler_y, feature_cols, df_valid


def preprocess_train_data_from_db(df, sequence_length=24):
    """
    [ì¬í•™ìŠµìš©] ë°ì´í„° ì „ì²˜ë¦¬ ë° Train/Val/Test ë¶„í• 
    """
    df_renamed = df.copy()
    
    # ë‚ ì§œ ì²˜ë¦¬
    if 'datetime' in df_renamed.columns:
        df_renamed['datetime'] = pd.to_datetime(df_renamed['datetime'])
    elif 'ë°œì „ì¼ì' in df_renamed.columns:
        df_renamed['datetime'] = pd.to_datetime(df_renamed['ë°œì „ì¼ì'])
        
        column_mapping = {
            'ë°œì „ì¼ì': 'datetime',
            'ê¸°ì˜¨': 'temperature',
            'ê°•ìš°ëŸ‰(mm)': 'precipitation',
            'ìŠµë„': 'humidity',
            'ì ì„¤ëŸ‰(mm)': 'snow',
            'ì ìš´ëŸ‰(10ë¶„ìœ„)': 'cloud_cover',
            'ì¼ì¡°(hr)': 'sunshine_duration',
            'ì¼ì‚¬ëŸ‰': 'solar_radiation',
            'ì„¤ë¹„ìš©ëŸ‰(MW)': 'solar_capacity',
            'ë°œì „ëŸ‰(MWh)': 'solar_generation'
        }
        df_renamed = df_renamed.rename(columns=column_mapping)
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    df_renamed['precipitation'] = df_renamed['precipitation'].fillna(0)
    df_renamed['snow'] = df_renamed['snow'].fillna(0)
    df_renamed['sunshine_duration'] = df_renamed['sunshine_duration'].fillna(0)
    df_renamed['solar_radiation'] = df_renamed['solar_radiation'].fillna(0)
    df_renamed['humidity'] = df_renamed['humidity'].fillna(df_renamed['humidity'].mean())
    df_renamed['temperature'] = df_renamed['temperature'].fillna(df_renamed['temperature'].mean())
    
    if 'cloud_cover' not in df_renamed.columns:
        if 'ì ìš´ëŸ‰(3ë¶„ìœ„)' in df.columns:
            df_renamed['cloud_cover'] = df['ì ìš´ëŸ‰(3ë¶„ìœ„)'].fillna(1) * 3.33
        else:
            df_renamed['cloud_cover'] = 5
    else:
        df_renamed['cloud_cover'] = df_renamed['cloud_cover'].fillna(5)
    
    df_renamed['hour'] = df_renamed['datetime'].dt.hour
    
    feature_cols = [
        'temperature', 'precipitation', 'humidity', 'cloud_cover',
        'sunshine_duration', 'solar_radiation', 'solar_capacity', 'hour'
    ]
    
    target_col = 'solar_generation'
    df_valid = df_renamed[df_renamed[target_col].notna()].copy()
    
    if len(df_valid) < sequence_length + 10:
        return None, None, None, None, None, None, None, None, feature_cols, None, None
    
    X = df_valid[feature_cols].values
    y = df_valid[target_col].values.reshape(-1, 1)
    dates = df_valid['datetime'].values
    
    scaler_X = MinMaxScaler()
    scaler_y = MinMaxScaler()
    
    X_scaled = scaler_X.fit_transform(X)
    y_scaled = scaler_y.fit_transform(y)
    
    # ì‹œí€€ìŠ¤ ìƒì„±
    X_seq, y_seq, date_seq = [], [], []
    for i in range(len(X_scaled) - sequence_length):
        X_seq.append(X_scaled[i:i+sequence_length])
        y_seq.append(y_scaled[i+sequence_length])
        date_seq.append(dates[i+sequence_length])
    
    X_seq = np.array(X_seq)
    y_seq = np.array(y_seq)
    
    # Train/Val/Test ë¶„í•  (80/10/10)
    X_temp, X_test, y_temp, y_test, date_temp, date_test = train_test_split(
        X_seq, y_seq, date_seq, test_size=0.1, random_state=42
    )
    X_train, X_val, y_train, y_val, date_train, date_val = train_test_split(
        X_temp, y_temp, date_temp, test_size=0.111, random_state=42
    )
    
    return (X_train, X_val, X_test, y_train, y_val, y_test,
            scaler_X, scaler_y, feature_cols, df_valid, date_test)


# ==========================================
# 6. ë¯¸ë˜ ì˜ˆì¸¡ í•¨ìˆ˜
# ==========================================
def predict_future_single_step(model, scaler_X, scaler_y, last_sequence, 
                                target_time, solar_capacity, device='cpu'):
    """ë‹¨ì¼ ì‹œì  ë¯¸ë˜ ì˜ˆì¸¡ (1ì‹œê°„ í›„)"""
    model.eval()
    
    # ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ì˜ í‰ê· ê°’ìœ¼ë¡œ ë‹¤ìŒ ì‹œì  íŠ¹ì„± ì¶”ì •
    last_features = last_sequence[-1].copy()
    
    # ì‹œê°„ íŠ¹ì„± ì—…ë°ì´íŠ¸
    target_hour = target_time.hour
    hour_scaled = target_hour / 23.0
    last_features[7] = hour_scaled  # hourëŠ” 8ë²ˆì§¸ íŠ¹ì„±
    
    # ìƒˆ ì‹œí€€ìŠ¤ ìƒì„± (sliding window)
    new_sequence = np.vstack([last_sequence[1:], last_features.reshape(1, -1)])
    
    # ì˜ˆì¸¡
    with torch.no_grad():
        X_tensor = torch.FloatTensor(new_sequence).unsqueeze(0).to(device)
        pred_scaled = model(X_tensor).cpu().numpy()
        pred_original = scaler_y.inverse_transform(pred_scaled)[0, 0]
    
    return max(0, pred_original), new_sequence


# ==========================================
# 7. ì „ì´í•™ìŠµ í•¨ìˆ˜
# ==========================================
def transfer_learning(model, train_loader, val_loader, criterion, 
                     num_epochs=10, patience=3, learning_rate=0.0001, 
                     freeze_layers=False, device='cpu', model_name='Model'):
    """ì „ì´í•™ìŠµ (Fine-tuning) ìˆ˜í–‰"""
    
    if freeze_layers:
        for param in model.parameters():
            param.requires_grad = False
        for param in model.fc.parameters():
            param.requires_grad = True
    else:
        for param in model.parameters():
            param.requires_grad = True
    
    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)
    
    best_val_loss = float('inf')
    patience_counter = 0
    best_model_state = model.state_dict().copy()
    
    for epoch in range(num_epochs):
        model.train()
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
        
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
                val_loss += loss.item()
        
        avg_val_loss = val_loss / len(val_loader)
        
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            best_model_state = model.state_dict().copy()
        else:
            patience_counter += 1
            if patience_counter >= patience:
                break
    
    model.load_state_dict(best_model_state)
    return model


# ==========================================
# 8. Main Entry Points (Celery Taskì—ì„œ í˜¸ì¶œ)
# ==========================================
def run_prediction(df_input, loaded_models=None):
    """
    [1ì‹œê°„ ì£¼ê¸°] ì˜ˆì¸¡ ì‹¤í–‰ í•¨ìˆ˜
    
    Args:
        df_input: DBì—ì„œ ê°€ì ¸ì˜¨ DataFrame
        loaded_models: (lstm_model, gru_model, scaler_X, scaler_y) íŠœí”Œ (ì„ íƒì‚¬í•­)
    
    Returns:
        list: 72ì‹œê°„ ì˜ˆì¸¡ ê²°ê³¼ (ì‹œê°„ë³„ í˜„ì¬/ëˆ„ì  ë°œì „ëŸ‰ í¬í•¨)
    """
    try:
        # 1. ëª¨ë¸ ë¡œë“œ
        if loaded_models:
            lstm_model, gru_model, scaler_X, scaler_y = loaded_models
        else:
            lstm_model, gru_model, scaler_X, scaler_y, _ = load_daegu_transfer_models(
                model_dir=model_dir
            )
        
        SEQUENCE_LENGTH = 24
        
        # 2. ë°ì´í„° ì „ì²˜ë¦¬
        if df_input.empty:
            return []
        
        X_seq, _, _, _, df_valid = preprocess_data_from_db(df_input, SEQUENCE_LENGTH)
        
        if X_seq is None or len(X_seq) == 0:
            print("âš ï¸ ì‹œí€€ìŠ¤ë¥¼ ë§Œë“¤ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            return []
        
        # 3. í˜„ì¬ ì‹œê°„ ê¸°ì¤€ ì„¤ì •
        current_time = datetime.now()
        solar_capacity = df_valid['solar_capacity'].iloc[0]
        last_sequence = X_seq[-1]
        
        # 4. 72ì‹œê°„ ì˜ˆì¸¡ ìˆ˜í–‰
        all_predictions = []
        lstm_cumulative = 0
        gru_cumulative = 0
        ensemble_cumulative = 0
        
        temp_sequence = last_sequence.copy()
        
        for h in range(1, 73):
            target_time = current_time + timedelta(hours=h)
            
            # LSTM ì˜ˆì¸¡
            lstm_pred, temp_sequence = predict_future_single_step(
                lstm_model, scaler_X, scaler_y, temp_sequence,
                target_time, solar_capacity, device
            )
            
            # GRU ì˜ˆì¸¡
            gru_pred, _ = predict_future_single_step(
                gru_model, scaler_X, scaler_y, temp_sequence,
                target_time, solar_capacity, device
            )
            
            # ì•™ìƒë¸”
            ensemble_pred = (lstm_pred + gru_pred) / 2
            
            # ëˆ„ì  ë°œì „ëŸ‰ ì—…ë°ì´íŠ¸
            lstm_cumulative += lstm_pred
            gru_cumulative += gru_pred
            ensemble_cumulative += ensemble_pred
            
            all_predictions.append({
                'ì˜ˆì¸¡_ë‚ ì§œ': target_time.strftime('%Y-%m-%d'),
                'ì˜ˆì¸¡_ì‹œê°„': target_time.strftime('%H:%M'),
                'ì˜ˆì¸¡_ì¼ì‹œ': target_time.strftime('%Y-%m-%d %H:%M:%S'),
                'ê²½ê³¼_ì‹œê°„(H)': h,
                'LSTM_í˜„ì¬_ë°œì „ëŸ‰(MWh)': round(lstm_pred, 4),
                'LSTM_ëˆ„ì _ë°œì „ëŸ‰(MWh)': round(lstm_cumulative, 4),
                'GRU_í˜„ì¬_ë°œì „ëŸ‰(MWh)': round(gru_pred, 4),
                'GRU_ëˆ„ì _ë°œì „ëŸ‰(MWh)': round(gru_cumulative, 4),
                'ì•™ìƒë¸”_í˜„ì¬_ë°œì „ëŸ‰(MWh)': round(ensemble_pred, 4),
                'ì•™ìƒë¸”_ëˆ„ì _ë°œì „ëŸ‰(MWh)': round(ensemble_cumulative, 4)
            })
        
        return all_predictions
    
    except Exception as e:
        print(f"âŒ ì˜ˆì¸¡ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return []


def retrain_model(df_train):
    """
    [í•˜ë£¨ 1ë²ˆ] ì¬í•™ìŠµ ì‹¤í–‰ í•¨ìˆ˜
    
    Args:
        df_train: í•™ìŠµìš© DataFrame
    
    Returns:
        bool: ì¬í•™ìŠµ ì„±ê³µ ì—¬ë¶€
    """
    try:
        print("\nğŸš€ [Model Retraining] ì‹œì‘...")
        
        # 1. ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ
        lstm_model, gru_model, _, _, metadata = load_daegu_transfer_models(model_dir=model_dir)
        
        SEQUENCE_LENGTH = 24
        
        # 2. ë°ì´í„° ì „ì²˜ë¦¬
        result = preprocess_train_data_from_db(df_train, SEQUENCE_LENGTH)
        
        if result[0] is None:
            print("âš ï¸ í•™ìŠµí•  ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤.")
            return False
        
        (X_train, X_val, X_test, y_train, y_val, y_test,
         scaler_X, scaler_y, feature_cols, df_valid, date_test) = result
        
        # 3. DataLoader ìƒì„±
        BATCH_SIZE = 32
        train_dataset = TimeSeriesDataset(X_train, y_train)
        val_dataset = TimeSeriesDataset(X_val, y_val)
        
        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)
        
        # 4. ì „ì´í•™ìŠµ ìˆ˜í–‰
        criterion = nn.MSELoss()
        
        print("   >> LSTM ì „ì´í•™ìŠµ ì¤‘...")
        lstm_model = transfer_learning(
            lstm_model, train_loader, val_loader, criterion,
            num_epochs=10, patience=3, learning_rate=0.0001,
            freeze_layers=False, device=device, model_name='LSTM'
        )
        
        print("   >> GRU ì „ì´í•™ìŠµ ì¤‘...")
        gru_model = transfer_learning(
            gru_model, train_loader, val_loader, criterion,
            num_epochs=10, patience=3, learning_rate=0.0001,
            freeze_layers=False, device=device, model_name='GRU'
        )
        
        # 5. ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        new_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ëª¨ë¸ config ê°€ì ¸ì˜¤ê¸°
        lstm_config = {
            'input_size': 8,
            'hidden_size': lstm_model.hidden_size,
            'num_layers': lstm_model.num_layers,
            'dropout': 0.2
        }
        
        gru_config = {
            'input_size': 8,
            'hidden_size': gru_model.hidden_size,
            'num_layers': gru_model.num_layers,
            'dropout': 0.2
        }
        
        # LSTM ì €ì¥
        torch.save({
            'model_state_dict': lstm_model.state_dict(),
            'model_config': lstm_config
        }, os.path.join(model_dir, f'lstm_transfer_daegu_{new_timestamp}.pth'))
        
        # GRU ì €ì¥
        torch.save({
            'model_state_dict': gru_model.state_dict(),
            'model_config': gru_config
        }, os.path.join(model_dir, f'gru_transfer_daegu_{new_timestamp}.pth'))
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        with open(os.path.join(model_dir, f'scalers_daegu_{new_timestamp}.pkl'), 'wb') as f:
            pickle.dump({'scaler_X': scaler_X, 'scaler_y': scaler_y}, f)
        
        # ë©”íƒ€ë°ì´í„° ê°±ì‹ 
        metadata['timestamp'] = new_timestamp
        metadata['retrained_at'] = datetime.now().isoformat()
        metadata['lstm_config'] = lstm_config
        metadata['gru_config'] = gru_config
        
        with open(os.path.join(model_dir, f'metadata_daegu_{new_timestamp}.json'), 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=4, ensure_ascii=False)
        
        # latest íŒŒì¼ ê°±ì‹ 
        with open(os.path.join(model_dir, 'latest_model_daegu.json'), 'w', encoding='utf-8') as f:
            json.dump({'timestamp': new_timestamp}, f, indent=4)
        
        print(f"âœ… ì¬í•™ìŠµ ì™„ë£Œ! ìƒˆë¡œìš´ ëª¨ë¸ ë²„ì „: {new_timestamp}")
        return True
    
    except Exception as e:
        print(f"âŒ ì¬í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return False


# ==========================================
# 9. ëª¨ë¸ ì‚¬ì „ ë¡œë“œ í•¨ìˆ˜ (ì„ íƒì‚¬í•­)
# ==========================================
def preload_models():
    """
    ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ì„ ë¯¸ë¦¬ ë¡œë“œí•˜ì—¬ ë©”ëª¨ë¦¬ì— ìœ ì§€
    ë§¤ ìš”ì²­ë§ˆë‹¤ ë¡œë“œí•˜ëŠ” ì˜¤ë²„í—¤ë“œ ì œê±°
    
    Returns:
        tuple: (lstm_model, gru_model, scaler_X, scaler_y)
    """
    try:
        print("ğŸ”¥ ëª¨ë¸ ì‚¬ì „ ë¡œë“œ ì¤‘...")
        lstm_model, gru_model, scaler_X, scaler_y, metadata = load_daegu_transfer_models(
            model_dir=model_dir
        )
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ë²„ì „: {metadata.get('timestamp', 'unknown')})")
        return lstm_model, gru_model, scaler_X, scaler_y
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

'''

# ==========================================
# ì‚¬ìš© ì˜ˆì‹œ
# ==========================================
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ìš© ì½”ë“œ
    print(f"ğŸš€ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")
    
    # ëª¨ë¸ ì‚¬ì „ ë¡œë“œ í…ŒìŠ¤íŠ¸
    models = preload_models()
    
    if models:
        print("\nâœ… ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("   ì´ì œ run_prediction() ë˜ëŠ” retrain_model()ì„ í˜¸ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
'''