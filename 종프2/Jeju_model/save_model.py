import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F 
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')  # í„°ë¯¸ë„ í™˜ê²½ì„ ìœ„í•œ ë°±ì—”ë“œ ì„¤ì •
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import os
import time
import pickle
import json
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.cluster import KMeans
from sklearn.impute import SimpleImputer
import xgboost as xgb
from xgboost import plot_importance



# GPU/CUDA ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸš€ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")
if torch.cuda.is_available():
    print(f"   GPU: {torch.cuda.get_device_name(0)}")
    print(f"   CUDA ë²„ì „: {torch.version.cuda}")
    print(f"   GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    # GPU ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
    torch.cuda.empty_cache()
    torch.backends.cudnn.benchmark = True  # ì„±ëŠ¥ í–¥ìƒ
else:
    print("   âš ï¸  CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

# ë°ì´í„° ê²½ë¡œ
data_path = "./dataset/jeju_solar_utf8.csv"
warnings.filterwarnings("ignore")

# ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
output_dir = "./plots"
os.makedirs(output_dir, exist_ok=True)
print(f"ğŸ“ Plot ì €ì¥ ê²½ë¡œ: {output_dir}")

# ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
model_dir = "./saved_models"
os.makedirs(model_dir, exist_ok=True)
print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {model_dir}")

plt.style.use('seaborn-v0_8-whitegrid')

# í•œê¸€ í°íŠ¸ ì„¤ì • - ì‹œìŠ¤í…œì— ë§ëŠ” í°íŠ¸ ìë™ ì„ íƒ
import matplotlib.font_manager as fm
import platform

def set_korean_font():
    """
    ì‹œìŠ¤í…œì— ì„¤ì¹˜ëœ í•œê¸€ í°íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ì°¾ì•„ ì„¤ì •
    """
    system = platform.system()
    
    # ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ í•œê¸€ í°íŠ¸ ëª©ë¡
    korean_fonts = [
        'Malgun Gothic',      # Windows
        'AppleGothic',        # macOS
        'NanumGothic',        # ë‚˜ëˆ”ê³ ë”•
        'NanumBarunGothic',   # ë‚˜ëˆ”ë°”ë¥¸ê³ ë”•
        'Nanum Gothic',
        'DejaVu Sans'         # ê¸°ë³¸ ëŒ€ì²´ í°íŠ¸
    ]
    
    # ì„¤ì¹˜ëœ í°íŠ¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ í•œê¸€ í°íŠ¸ ì°¾ê¸°
    for font in korean_fonts:
        if font in available_fonts:
            plt.rcParams['font.family'] = font
            print(f"âœ… í•œê¸€ í°íŠ¸ ì„¤ì • ì™„ë£Œ: {font}")
            break
    else:
        # í•œê¸€ í°íŠ¸ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©
        print("âš ï¸  í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        print("   í•œê¸€ì´ ê¹¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‚˜ëˆ”ê³ ë”• ì„¤ì¹˜ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
    
    # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
    plt.rcParams['axes.unicode_minus'] = False

# í•œê¸€ í°íŠ¸ ì„¤ì • ì‹¤í–‰
set_korean_font()


def calculate_rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

def calculate_r2(y_true, y_pred):
    return r2_score(y_true, y_pred)

def calculate_mape(y_true, y_pred, method='improved'):
    """
    ê°œì„ ëœ MAPE ê³„ì‚° - íƒœì–‘ê´‘ ë°œì „ëŸ‰ íŠ¹ì„±ì„ ê³ ë ¤í•œ ì—¬ëŸ¬ ë°©ë²• ì œê³µ
    
    Args:
        y_true: ì‹¤ì œê°’
        y_pred: ì˜ˆì¸¡ê°’
        method: ê³„ì‚° ë°©ë²•
            - 'improved': ê°œì„ ëœ MAPE (ê¸°ë³¸ê°’)
            - 'threshold': ì„ê³„ê°’ ê¸°ë°˜ MAPE
            - 'weighted': ê°€ì¤‘ MAPE
            - 'symmetric': ëŒ€ì¹­ MAPE
    """
    y_true = np.array(y_true, dtype=np.float64)
    y_pred = np.array(y_pred, dtype=np.float64)
    
    # NaN ë° ë¬´í•œê°’ í™•ì¸ ë° ì œê±°
    valid_mask = np.isfinite(y_true) & np.isfinite(y_pred)
    y_true_clean = y_true[valid_mask]
    y_pred_clean = y_pred[valid_mask]
    
    if len(y_true_clean) == 0:
        print("Warning: ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ì–´ MAPEë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return np.nan
    
    if method == 'improved':
        """
        ê°œì„ ëœ MAPE: ì‘ì€ ê°’ë“¤ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ì—¬ 
        ì „ì²´ì ì¸ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë” ì˜ ë°˜ì˜
        """
        # ì „ì²´ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ê³ ë ¤í•œ ì„ê³„ê°’ ì„¤ì •
        threshold = np.percentile(y_true_clean, 10)  # í•˜ìœ„ 10% ê°’ì„ ì„ê³„ê°’ìœ¼ë¡œ ì‚¬ìš©
        
        # ì„ê³„ê°’ ì´ìƒì¸ ë°ì´í„°ì— ëŒ€í•´ì„œë§Œ MAPE ê³„ì‚°
        significant_mask = y_true_clean >= threshold
        
        if not np.any(significant_mask):
            # ëª¨ë“  ê°’ì´ ì„ê³„ê°’ ë¯¸ë§Œì¸ ê²½ìš°, ì ˆëŒ€ ì˜¤ì°¨ ê¸°ë°˜ ê³„ì‚°
            abs_errors = np.abs(y_true_clean - y_pred_clean)
            mean_actual = np.mean(y_true_clean)
            if mean_actual > 0:
                return (np.mean(abs_errors) / mean_actual) * 100
            else:
                return 0.0
        
        y_true_sig = y_true_clean[significant_mask]
        y_pred_sig = y_pred_clean[significant_mask]
        
        # ê°€ì¤‘ í‰ê·  MAPE ê³„ì‚°
        weights = y_true_sig / np.sum(y_true_sig)  # ì‹¤ì œê°’ì— ë¹„ë¡€í•œ ê°€ì¤‘ì¹˜
        percentage_errors = np.abs((y_true_sig - y_pred_sig) / y_true_sig)
        
        # ê·¹ë‹¨ì ì¸ ì˜¤ì°¨ ì œí•œ
        percentage_errors = np.clip(percentage_errors, 0, 2)  # ìµœëŒ€ 200% ì˜¤ì°¨ë¡œ ì œí•œ
        
        mape_value = np.sum(weights * percentage_errors) * 100
        
        # ì œê±°ëœ ë°ì´í„° ë¹„ìœ¨ ì¶œë ¥
        removed_count = len(y_true_clean) - len(y_true_sig)
        if removed_count > 0:
            removal_rate = (removed_count / len(y_true_clean)) * 100
            print(f"MAPE ê³„ì‚° ì‹œ ì‘ì€ ê°’ ì œì™¸: {removed_count}ê°œ ({removal_rate:.1f}%)")
    
    elif method == 'threshold':
        """
        ì„ê³„ê°’ ê¸°ë°˜ MAPE: ì¼ì • ê°’ ì´ìƒì˜ ë°ì´í„°ë§Œ ì‚¬ìš©
        """
        # ë™ì  ì„ê³„ê°’ ê³„ì‚° (í‰ê· ì˜ 10%)
        threshold = np.mean(y_true_clean) * 0.1
        
        above_threshold = y_true_clean > threshold
        
        if not np.any(above_threshold):
            return 0.0
            
        y_true_filtered = y_true_clean[above_threshold]
        y_pred_filtered = y_pred_clean[above_threshold]
        
        percentage_errors = np.abs((y_true_filtered - y_pred_filtered) / y_true_filtered)
        percentage_errors = np.clip(percentage_errors, 0, 1.5)  # 150% ì œí•œ
        
        mape_value = np.mean(percentage_errors) * 100
        
        removed_count = len(y_true_clean) - len(y_true_filtered)
        print(f"ì„ê³„ê°’({threshold:.3f}) ë¯¸ë§Œ ì œì™¸: {removed_count}ê°œ")
    
    elif method == 'weighted':
        """
        ê°€ì¤‘ MAPE: ê°’ì˜ í¬ê¸°ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ë¶€ì—¬
        """
        # 0ì— ê°€ê¹Œìš´ ê°’ ì œì™¸
        non_zero_mask = y_true_clean > np.percentile(y_true_clean, 5)
        
        if not np.any(non_zero_mask):
            return 0.0
            
        y_true_nz = y_true_clean[non_zero_mask]
        y_pred_nz = y_pred_clean[non_zero_mask]
        
        # ì‹¤ì œê°’ì˜ í¬ê¸°ì— ë¹„ë¡€í•œ ê°€ì¤‘ì¹˜
        weights = y_true_nz / np.sum(y_true_nz)
        
        percentage_errors = np.abs((y_true_nz - y_pred_nz) / y_true_nz)
        percentage_errors = np.clip(percentage_errors, 0, 1.0)  # 100% ì œí•œ
        
        mape_value = np.sum(weights * percentage_errors) * 100
    
    elif method == 'symmetric':
        """
        ëŒ€ì¹­ MAPE (SMAPE): ë¶„ëª¨ì— ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ í‰ê·  ì‚¬ìš©
        """
        # ë§¤ìš° ì‘ì€ ê°’ë“¤ ì œì™¸
        min_threshold = np.percentile(np.abs(y_true_clean), 5)
        valid_mask = np.abs(y_true_clean) > min_threshold
        
        if not np.any(valid_mask):
            return 0.0
            
        y_true_filtered = y_true_clean[valid_mask]
        y_pred_filtered = y_pred_clean[valid_mask]
        
        denominator = (np.abs(y_true_filtered) + np.abs(y_pred_filtered)) / 2
        percentage_errors = np.abs(y_true_filtered - y_pred_filtered) / denominator
        percentage_errors = np.clip(percentage_errors, 0, 1.0)  # 100% ì œí•œ
        
        mape_value = np.mean(percentage_errors) * 100
    
    else:
        raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” MAPE ê³„ì‚° ë°©ë²•ì…ë‹ˆë‹¤.")
    
    return mape_value

def calculate_normalized_mape(y_true, y_pred):
    """
    ì •ê·œí™”ëœ MAPE: ë°ì´í„° ë²”ìœ„ì— ë”°ë¼ ì •ê·œí™”
    """
    y_true = np.array(y_true, dtype=np.float64)
    y_pred = np.array(y_pred, dtype=np.float64)
    
    # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì„ íƒ
    valid_mask = np.isfinite(y_true) & np.isfinite(y_pred) & (y_true > 0)
    y_true_valid = y_true[valid_mask]
    y_pred_valid = y_pred[valid_mask]
    
    if len(y_true_valid) == 0:
        print("Warning: ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ì–´ ì •ê·œí™”ëœ MAPEë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return np.nan
    
    # ë°ì´í„° ë²”ìœ„ ê³„ì‚°
    data_range = np.max(y_true_valid) - np.min(y_true_valid)
    
    if data_range == 0:
        return 0.0
    
    # ì ˆëŒ€ ì˜¤ì°¨ì˜ í‰ê· ì„ ë°ì´í„° ë²”ìœ„ë¡œ ì •ê·œí™”
    abs_errors = np.abs(y_true_valid - y_pred_valid)
    normalized_mape = (np.mean(abs_errors) / data_range) * 100
    
    return normalized_mape


def calculate_all_metrics(y_true, y_pred, print_details=False):
    """
    ëª¨ë“  í‰ê°€ ì§€í‘œë¥¼ í•œ ë²ˆì— ê³„ì‚°í•˜ê³  ë°˜í™˜
    """
    metrics = {
        'mae': mean_absolute_error(y_true, y_pred),
        'rmse': calculate_rmse(y_true, y_pred),
        'r2': calculate_r2(y_true, y_pred),
        'mape_improved': calculate_mape(y_true, y_pred, method='improved'),
        'mape_threshold': calculate_mape(y_true, y_pred, method='threshold'),
        'mape_weighted': calculate_mape(y_true, y_pred, method='weighted'),
        'mape_symmetric': calculate_mape(y_true, y_pred, method='symmetric'),
        'normalized_mape': calculate_normalized_mape(y_true, y_pred),
    }
    
    # ë°ì´í„° ë²”ìœ„ ê³„ì‚°
    data_range = np.max(y_true) - np.min(y_true)
    
    # NMAE ë° NRMSE ê³„ì‚°
    metrics['nmae'] = metrics['mae'] / data_range if data_range > 0 else 0
    metrics['nrmse'] = metrics['rmse'] / data_range if data_range > 0 else 0
    
    if print_details:
        print("\n=== ìƒì„¸ í‰ê°€ ì§€í‘œ ===")
        print(f"MAE: {metrics['mae']:.4f}")
        print(f"RMSE: {metrics['rmse']:.4f}")
        print(f"NMAE: {metrics['nmae']:.4f}")
        print(f"NRMSE: {metrics['nrmse']:.4f}")
        print(f"RÂ²: {metrics['r2']:.4f}")
        print(f"\n=== MAPE ë³€í˜• ì§€í‘œ ===")
        print(f"ê°œì„ ëœ MAPE: {metrics['mape_improved']:.2f}%")
        print(f"ì„ê³„ê°’ MAPE: {metrics['mape_threshold']:.2f}%")
        print(f"ê°€ì¤‘ MAPE: {metrics['mape_weighted']:.2f}%")
        print(f"ëŒ€ì¹­ MAPE: {metrics['mape_symmetric']:.2f}%")
        print(f"ì •ê·œí™” MAPE: {metrics['normalized_mape']:.2f}%")
    
    return metrics


# === ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ í•¨ìˆ˜ ===
def load_and_preprocess_data(data_path, sequence_length=24):
    """
    ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
    """
    print("\në°ì´í„° ë¡œë”© ì¤‘...")
    df = pd.read_csv(data_path)
    print(f"ì›ë³¸ ë°ì´í„° í¬ê¸°: {df.shape}")
    
    # ê²°ì¸¡ê°’ ì²˜ë¦¬
    print("\nê²°ì¸¡ê°’ ì²˜ë¦¬ ì¤‘...")
    numeric_columns = df.select_dtypes(include=[np.number]).columns
    imputer = SimpleImputer(strategy='mean')
    df[numeric_columns] = imputer.fit_transform(df[numeric_columns])
    
    # íƒ€ê²Ÿ ë³€ìˆ˜ ì„ íƒ
    target_col = 'íƒœì–‘ê´‘ ë°œì „ëŸ‰(MWh)'
    if target_col not in df.columns:
        raise ValueError(f"íƒ€ê²Ÿ ì»¬ëŸ¼ '{target_col}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
    feature_cols = [col for col in numeric_columns if col != target_col]
    X = df[feature_cols].values
    y = df[target_col].values.reshape(-1, 1)
    
    print(f"íŠ¹ì„± ê°œìˆ˜: {len(feature_cols)}")
    print(f"íŠ¹ì„± ëª©ë¡: {feature_cols}")
    
    # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
    scaler_X = MinMaxScaler()
    scaler_y = MinMaxScaler()
    
    X_scaled = scaler_X.fit_transform(X)
    y_scaled = scaler_y.fit_transform(y)
    
    # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
    X_seq, y_seq = [], []
    for i in range(len(X_scaled) - sequence_length):
        X_seq.append(X_scaled[i:i+sequence_length])
        y_seq.append(y_scaled[i+sequence_length])
    
    X_seq = np.array(X_seq)
    y_seq = np.array(y_seq)
    
    print(f"\nì‹œí€€ìŠ¤ ë°ì´í„° shape: X={X_seq.shape}, y={y_seq.shape}")
    
    # Train/Val/Test ë¶„í• 
    X_temp, X_test, y_temp, y_test = train_test_split(
        X_seq, y_seq, test_size=0.2, random_state=42
    )
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=0.25, random_state=42
    )
    
    print(f"\në°ì´í„° ë¶„í• :")
    print(f"  Train: {X_train.shape}")
    print(f"  Val: {X_val.shape}")
    print(f"  Test: {X_test.shape}")
    
    return (X_train, X_val, X_test, y_train, y_val, y_test, 
            scaler_X, scaler_y, feature_cols)


# === PyTorch Dataset í´ë˜ìŠ¤ ===
class TimeSeriesDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


# === LSTM ëª¨ë¸ ===
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(
            input_size, 
            hidden_size, 
            num_layers, 
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        self.fc = nn.Linear(hidden_size, 1)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        # LSTM forward
        lstm_out, _ = self.lstm(x)
        lstm_out = self.dropout(lstm_out[:, -1, :])
        output = self.fc(lstm_out)
        return output


# === GRU ëª¨ë¸ ===
class GRUModel(nn.Module):
    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2):
        super(GRUModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.gru = nn.GRU(
            input_size, 
            hidden_size, 
            num_layers, 
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        self.fc = nn.Linear(hidden_size, 1)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        gru_out, _ = self.gru(x)
        gru_out = self.dropout(gru_out[:, -1, :])
        output = self.fc(gru_out)
        return output


# === í•™ìŠµ í•¨ìˆ˜ ===
def train_model(model, train_loader, val_loader, criterion, optimizer, 
                num_epochs=100, patience=15, device='cpu'):
    """
    ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜
    """
    best_val_loss = float('inf')
    patience_counter = 0
    train_losses = []
    val_losses = []
    
    print(f"\nëª¨ë¸ í•™ìŠµ ì‹œì‘ (ì´ {num_epochs} ì—í­)")
    start_time = time.time()
    
    for epoch in range(num_epochs):
        # Training
        model.train()
        train_loss = 0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
        
        avg_train_loss = train_loss / len(train_loader)
        train_losses.append(avg_train_loss)
        
        # Validation
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
                val_loss += loss.item()
        
        avg_val_loss = val_loss / len(val_loader)
        val_losses.append(avg_val_loss)
        
        # ì§„í–‰ìƒí™© ì¶œë ¥
        if (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{num_epochs}] "
                  f"Train Loss: {avg_train_loss:.4f} | "
                  f"Val Loss: {avg_val_loss:.4f}")
        
        # Early Stopping
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            best_model_state = model.state_dict().copy()
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"\nEarly stopping at epoch {epoch+1}")
                break
    
    # ìµœì  ëª¨ë¸ ë¡œë“œ
    model.load_state_dict(best_model_state)
    
    elapsed_time = time.time() - start_time
    print(f"í•™ìŠµ ì™„ë£Œ! ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
    
    return model, train_losses, val_losses


# === ì˜ˆì¸¡ í•¨ìˆ˜ ===
def predict(model, test_loader, device='cpu'):
    """
    ëª¨ë¸ ì˜ˆì¸¡ í•¨ìˆ˜
    """
    model.eval()
    predictions = []
    actuals = []
    
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            X_batch = X_batch.to(device)
            outputs = model(X_batch)
            predictions.extend(outputs.cpu().numpy())
            actuals.extend(y_batch.numpy())
    
    return np.array(predictions), np.array(actuals)


# === ëª¨ë¸ ì €ì¥ í•¨ìˆ˜ ===
def save_models(lstm_model, gru_model, xgb_model, scaler_X, scaler_y, 
                feature_cols, lstm_metrics, gru_metrics, stacked_metrics, 
                model_dir='./saved_models'):
    """
    í•™ìŠµëœ ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬, ë©”íƒ€ë°ì´í„° ì €ì¥
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    print(f"\n{'='*80}")
    print("ëª¨ë¸ ì €ì¥ ì¤‘...")
    print(f"{'='*80}")
    
    # 1. PyTorch ëª¨ë¸ ì €ì¥ (LSTM)
    lstm_path = os.path.join(model_dir, f'lstm_model_{timestamp}.pth')
    torch.save({
        'model_state_dict': lstm_model.state_dict(),
        'model_config': {
            'input_size': lstm_model.lstm.input_size,
            'hidden_size': lstm_model.hidden_size,
            'num_layers': lstm_model.num_layers,
        },
        'metrics': lstm_metrics
    }, lstm_path)
    print(f"âœ… LSTM ëª¨ë¸ ì €ì¥: {lstm_path}")
    
    # 2. PyTorch ëª¨ë¸ ì €ì¥ (GRU)
    gru_path = os.path.join(model_dir, f'gru_model_{timestamp}.pth')
    torch.save({
        'model_state_dict': gru_model.state_dict(),
        'model_config': {
            'input_size': gru_model.gru.input_size,
            'hidden_size': gru_model.hidden_size,
            'num_layers': gru_model.num_layers,
        },
        'metrics': gru_metrics
    }, gru_path)
    print(f"âœ… GRU ëª¨ë¸ ì €ì¥: {gru_path}")
    
    # 3. XGBoost ëª¨ë¸ ì €ì¥
    xgb_path = os.path.join(model_dir, f'xgboost_stacking_{timestamp}.json')
    xgb_model.save_model(xgb_path)
    print(f"âœ… XGBoost ìŠ¤íƒœí‚¹ ëª¨ë¸ ì €ì¥: {xgb_path}")
    
    # 4. ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    scaler_path = os.path.join(model_dir, f'scalers_{timestamp}.pkl')
    with open(scaler_path, 'wb') as f:
        pickle.dump({
            'scaler_X': scaler_X,
            'scaler_y': scaler_y
        }, f)
    print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥: {scaler_path}")
    
    # 5. ë©”íƒ€ë°ì´í„° ì €ì¥ (íŠ¹ì„± ì •ë³´, ì„±ëŠ¥ ì§€í‘œ ë“±)
    metadata = {
        'timestamp': timestamp,
        'feature_columns': feature_cols,
        'sequence_length': 24,
        'device': str(device),
        'lstm_metrics': {k: float(v) if not isinstance(v, str) else v 
                        for k, v in lstm_metrics.items()},
        'gru_metrics': {k: float(v) if not isinstance(v, str) else v 
                       for k, v in gru_metrics.items()},
        'stacked_metrics': {k: float(v) if not isinstance(v, str) else v 
                           for k, v in stacked_metrics.items()},
        'model_files': {
            'lstm': f'lstm_model_{timestamp}.pth',
            'gru': f'gru_model_{timestamp}.pth',
            'xgboost': f'xgboost_stacking_{timestamp}.json',
            'scalers': f'scalers_{timestamp}.pkl'
        }
    }
    
    metadata_path = os.path.join(model_dir, f'metadata_{timestamp}.json')
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=4, ensure_ascii=False)
    print(f"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")
    
    # 6. ìµœì‹  ëª¨ë¸ ê²½ë¡œë¥¼ ê°€ë¦¬í‚¤ëŠ” ë§í¬ íŒŒì¼ ìƒì„±
    latest_models_info = {
        'timestamp': timestamp,
        'lstm_model': lstm_path,
        'gru_model': gru_path,
        'xgboost_model': xgb_path,
        'scalers': scaler_path,
        'metadata': metadata_path
    }
    
    latest_path = os.path.join(model_dir, 'latest_models.json')
    with open(latest_path, 'w', encoding='utf-8') as f:
        json.dump(latest_models_info, f, indent=4, ensure_ascii=False)
    print(f"âœ… ìµœì‹  ëª¨ë¸ ì •ë³´ ì €ì¥: {latest_path}")
    
    print(f"\n{'='*80}")
    print(f"âœ¨ ëª¨ë“  ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"{'='*80}")
    
    return {
        'lstm': lstm_path,
        'gru': gru_path,
        'xgboost': xgb_path,
        'scalers': scaler_path,
        'metadata': metadata_path
    }


# === ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ ===
def load_models(model_dir='./saved_models', timestamp=None):
    """
    ì €ì¥ëœ ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
    
    Args:
        model_dir: ëª¨ë¸ì´ ì €ì¥ëœ ë””ë ‰í† ë¦¬
        timestamp: íŠ¹ì • ì‹œì ì˜ ëª¨ë¸ì„ ë¡œë“œí•˜ë ¤ë©´ íƒ€ì„ìŠ¤íƒ¬í”„ ì§€ì •
                  Noneì´ë©´ ìµœì‹  ëª¨ë¸ ë¡œë“œ
    """
    print(f"\n{'='*80}")
    print("ëª¨ë¸ ë¡œë“œ ì¤‘...")
    print(f"{'='*80}")
    
    # ìµœì‹  ëª¨ë¸ ì •ë³´ ë¡œë“œ
    if timestamp is None:
        latest_path = os.path.join(model_dir, 'latest_models.json')
        if not os.path.exists(latest_path):
            raise FileNotFoundError(f"ìµœì‹  ëª¨ë¸ ì •ë³´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {latest_path}")
        
        with open(latest_path, 'r', encoding='utf-8') as f:
            model_info = json.load(f)
        
        timestamp = model_info['timestamp']
        print(f"ìµœì‹  ëª¨ë¸ íƒ€ì„ìŠ¤íƒ¬í”„: {timestamp}")
    
    # ë©”íƒ€ë°ì´í„° ë¡œë“œ
    metadata_path = os.path.join(model_dir, f'metadata_{timestamp}.json')
    with open(metadata_path, 'r', encoding='utf-8') as f:
        metadata = json.load(f)
    
    print(f"âœ… ë©”íƒ€ë°ì´í„° ë¡œë“œ: {metadata_path}")
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
    scaler_path = os.path.join(model_dir, f'scalers_{timestamp}.pkl')
    with open(scaler_path, 'rb') as f:
        scalers = pickle.load(f)
    
    scaler_X = scalers['scaler_X']
    scaler_y = scalers['scaler_y']
    print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ: {scaler_path}")
    
    # LSTM ëª¨ë¸ ë¡œë“œ
    lstm_path = os.path.join(model_dir, f'lstm_model_{timestamp}.pth')
    lstm_checkpoint = torch.load(lstm_path, map_location=device)
    lstm_config = lstm_checkpoint['model_config']
    
    lstm_model = LSTMModel(
        input_size=lstm_config['input_size'],
        hidden_size=lstm_config['hidden_size'],
        num_layers=lstm_config['num_layers']
    ).to(device)
    lstm_model.load_state_dict(lstm_checkpoint['model_state_dict'])
    lstm_model.eval()
    print(f"âœ… LSTM ëª¨ë¸ ë¡œë“œ: {lstm_path}")
    
    # GRU ëª¨ë¸ ë¡œë“œ
    gru_path = os.path.join(model_dir, f'gru_model_{timestamp}.pth')
    gru_checkpoint = torch.load(gru_path, map_location=device)
    gru_config = gru_checkpoint['model_config']
    
    gru_model = GRUModel(
        input_size=gru_config['input_size'],
        hidden_size=gru_config['hidden_size'],
        num_layers=gru_config['num_layers']
    ).to(device)
    gru_model.load_state_dict(gru_checkpoint['model_state_dict'])
    gru_model.eval()
    print(f"âœ… GRU ëª¨ë¸ ë¡œë“œ: {gru_path}")
    
    # XGBoost ëª¨ë¸ ë¡œë“œ
    xgb_path = os.path.join(model_dir, f'xgboost_stacking_{timestamp}.json')
    xgb_model = xgb.XGBRegressor()
    xgb_model.load_model(xgb_path)
    print(f"âœ… XGBoost ìŠ¤íƒœí‚¹ ëª¨ë¸ ë¡œë“œ: {xgb_path}")
    
    print(f"\n{'='*80}")
    print(f"âœ¨ ëª¨ë“  ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"{'='*80}")
    
    return {
        'lstm_model': lstm_model,
        'gru_model': gru_model,
        'xgb_model': xgb_model,
        'scaler_X': scaler_X,
        'scaler_y': scaler_y,
        'metadata': metadata
    }


# === ë©”ì¸ ì‹¤í–‰ ì½”ë“œ ===
if __name__ == "__main__":
    try:
        # 1. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
        SEQUENCE_LENGTH = 24
        (X_train, X_val, X_test, y_train, y_val, y_test, 
         scaler_X, scaler_y, feature_cols) = load_and_preprocess_data(data_path, SEQUENCE_LENGTH)
        
        # 2. DataLoader ìƒì„±
        BATCH_SIZE = 64
        train_dataset = TimeSeriesDataset(X_train, y_train)
        val_dataset = TimeSeriesDataset(X_val, y_val)
        test_dataset = TimeSeriesDataset(X_test, y_test)
        
        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)
        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)
        
        # 3. LSTM ëª¨ë¸ í•™ìŠµ
        print("\n" + "="*80)
        print("LSTM ëª¨ë¸ í•™ìŠµ")
        print("="*80)
        
        input_size = X_train.shape[2]
        lstm_model = LSTMModel(input_size=input_size, hidden_size=128, 
                              num_layers=2, dropout=0.2).to(device)
        criterion = nn.MSELoss()
        optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)
        
        lstm_model, lstm_train_losses, lstm_val_losses = train_model(
            lstm_model, train_loader, val_loader, criterion, optimizer,
            num_epochs=100, patience=15, device=device
        )
        
        # LSTM ì˜ˆì¸¡
        lstm_predictions, lstm_actuals = predict(lstm_model, test_loader, device)
        
        # ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³µì›
        lstm_predictions_original = scaler_y.inverse_transform(lstm_predictions)
        lstm_actuals_original = scaler_y.inverse_transform(lstm_actuals)
        
        # LSTM í‰ê°€
        lstm_metrics = calculate_all_metrics(lstm_actuals_original, lstm_predictions_original, print_details=True)
        nmae_lstm = lstm_metrics['nmae']
        nrmse_lstm = lstm_metrics['nrmse']
        r2_lstm = lstm_metrics['r2']
        mape_lstm = lstm_metrics['mape_improved']
        
        print(f"\n=== LSTM ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ===")
        print(f"NMAE: {nmae_lstm:.4f}")
        print(f"NRMSE: {nrmse_lstm:.4f}")
        print(f"RÂ²: {r2_lstm:.4f}")
        print(f"MAPE: {mape_lstm:.4f}%")
        
        # 4. GRU ëª¨ë¸ í•™ìŠµ
        print("\n" + "="*80)
        print("GRU ëª¨ë¸ í•™ìŠµ")
        print("="*80)
        
        gru_model = GRUModel(input_size=input_size, hidden_size=128, 
                            num_layers=2, dropout=0.2).to(device)
        optimizer = optim.Adam(gru_model.parameters(), lr=0.001)
        
        gru_model, gru_train_losses, gru_val_losses = train_model(
            gru_model, train_loader, val_loader, criterion, optimizer,
            num_epochs=100, patience=15, device=device
        )
        
        # GRU ì˜ˆì¸¡
        gru_predictions, gru_actuals = predict(gru_model, test_loader, device)
        
        # ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³µì›
        gru_predictions_original = scaler_y.inverse_transform(gru_predictions)
        gru_actuals_original = scaler_y.inverse_transform(gru_actuals)
        
        # GRU í‰ê°€
        gru_metrics = calculate_all_metrics(gru_actuals_original, gru_predictions_original, print_details=True)
        nmae_gru = gru_metrics['nmae']
        nrmse_gru = gru_metrics['nrmse']
        r2_gru = gru_metrics['r2']
        mape_gru = gru_metrics['mape_improved']
        
        print(f"\n=== GRU ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ===")
        print(f"NMAE: {nmae_gru:.4f}")
        print(f"NRMSE: {nrmse_gru:.4f}")
        print(f"RÂ²: {r2_gru:.4f}")
        print(f"MAPE: {mape_gru:.4f}%")
        
        # 5. ìŠ¤íƒœí‚¹ì„ ìœ„í•œ ë©”íƒ€ íŠ¹ì„± ìƒì„±
        print("\n" + "="*80)
        print("ìŠ¤íƒœí‚¹ ëª¨ë¸ ì¤€ë¹„")
        print("="*80)
        
        # Test setì— ëŒ€í•œ LSTMê³¼ GRUì˜ ì˜ˆì¸¡ê°’ì„ íŠ¹ì„±ìœ¼ë¡œ ì‚¬ìš©
        X_test_stack = np.hstack([
            lstm_predictions_original.reshape(-1, 1),
            gru_predictions_original.reshape(-1, 1)
        ])
        y_test_stack = lstm_actuals_original.flatten()
        
        # Train setì—ì„œë„ ë™ì¼í•˜ê²Œ ë©”íƒ€ íŠ¹ì„± ìƒì„±
        lstm_train_predictions, _ = predict(lstm_model, train_loader, device)
        gru_train_predictions, _ = predict(gru_model, train_loader, device)
        
        lstm_train_predictions_original = scaler_y.inverse_transform(lstm_train_predictions)
        gru_train_predictions_original = scaler_y.inverse_transform(gru_train_predictions)
        
        X_train_stack = np.hstack([
            lstm_train_predictions_original.reshape(-1, 1),
            gru_train_predictions_original.reshape(-1, 1)
        ])
        y_train_stack = scaler_y.inverse_transform(
            train_dataset.y.numpy()
        ).flatten()
        
        # Validation set
        lstm_val_predictions, _ = predict(lstm_model, val_loader, device)
        gru_val_predictions, _ = predict(gru_model, val_loader, device)
        
        lstm_val_predictions_original = scaler_y.inverse_transform(lstm_val_predictions)
        gru_val_predictions_original = scaler_y.inverse_transform(gru_val_predictions)
        
        X_val_stack = np.hstack([
            lstm_val_predictions_original.reshape(-1, 1),
            gru_val_predictions_original.reshape(-1, 1)
        ])
        y_val_stack = scaler_y.inverse_transform(
            val_dataset.y.numpy()
        ).flatten()
        
        print(f"ìŠ¤íƒœí‚¹ ë°ì´í„° í¬ê¸°:")
        print(f"  Train: {X_train_stack.shape}")
        print(f"  Val: {X_val_stack.shape}")
        print(f"  Test: {X_test_stack.shape}")
        
        # 6. XGBoost ìŠ¤íƒœí‚¹ ëª¨ë¸ í•™ìŠµ
        print("\n" + "="*80)
        print("XGBoost ìŠ¤íƒœí‚¹ ëª¨ë¸ í•™ìŠµ")
        print("="*80)
        
        feature_names = ['LSTM_prediction', 'GRU_prediction']
        
        xgb_stacking_regressor = xgb.XGBRegressor(
            n_estimators=200,
            max_depth=5,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            tree_method='hist',
            device='cuda' if torch.cuda.is_available() else 'cpu',
            eval_metric='mae'
        )
        
        print("XGBoost ìŠ¤íƒœí‚¹ ëª¨ë¸ í•™ìŠµ ì¤‘...")
        xgb_stacking_regressor.fit(
            X_train_stack, 
            y_train_stack, 
            eval_set=[(X_val_stack, y_val_stack)], 
            verbose=100
        )
        
        # ìŠ¤íƒœí‚¹ ëª¨ë¸ ì˜ˆì¸¡ ë° í‰ê°€
        stacked_pred_test = xgb_stacking_regressor.predict(X_test_stack)

        # ìŠ¤íƒœí‚¹ ëª¨ë¸ í‰ê°€
        stacked_metrics = calculate_all_metrics(y_test_stack, stacked_pred_test, print_details=True)
        nmae_stacked = stacked_metrics['nmae']
        nrmse_stacked = stacked_metrics['nrmse']
        r2_stacked = stacked_metrics['r2']
        mape_stacked = stacked_metrics['mape_improved']
        
        print(f"\n=== XGBoost ìŠ¤íƒœí‚¹ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ===")
        print(f"NMAE: {nmae_stacked:.4f}")
        print(f"NRMSE: {nrmse_stacked:.4f}")
        print(f"RÂ²: {r2_stacked:.4f}")
        print(f"MAPE: {mape_stacked:.4f}%")
        
        # 7. íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
        print("\n=== XGBoost ìŠ¤íƒœí‚¹ ëª¨ë¸ íŠ¹ì„± ì¤‘ìš”ë„ ===")
        importance_dict = dict(zip(feature_names, xgb_stacking_regressor.feature_importances_))
        for feature, importance in sorted(importance_dict.items(), key=lambda x: x[1], reverse=True):
            print(f"{feature}: {importance:.4f}")
        
        # 8. ìµœì¢… ì„±ëŠ¥ ë¹„êµ
        print(f"\n{'='*80}")
        print("ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
        print(f"{'='*80}")
        print(f"{'ëª¨ë¸':<20} {'NMAE':<10} {'NRMSE':<10} {'RÂ²':<10} {'MAPE':<10}")
        print("-" * 80)
        print(f"{'LSTM':<20} {nmae_lstm:<10.4f} {nrmse_lstm:<10.4f} {r2_lstm:<10.4f} {mape_lstm:<10.2f}%")
        print(f"{'GRU':<20} {nmae_gru:<10.4f} {nrmse_gru:<10.4f} {r2_gru:<10.4f} {mape_gru:<10.2f}%")
        print(f"{'XGBoost Stacking':<20} {nmae_stacked:<10.4f} {nrmse_stacked:<10.4f} {r2_stacked:<10.4f} {mape_stacked:<10.2f}%")
        
        # ì„±ëŠ¥ ê°œì„ ìœ¨ ê³„ì‚°
        best_individual = min(nmae_lstm, nmae_gru)
        improvement = ((best_individual - nmae_stacked) / best_individual) * 100
        print(f"\nìŠ¤íƒœí‚¹ìœ¼ë¡œ ì¸í•œ ì„±ëŠ¥ ê°œì„ : {improvement:.2f}%")
        
        # ========================================
        # 9. ëª¨ë¸ ì €ì¥ (ìƒˆë¡œ ì¶”ê°€ëœ ë¶€ë¶„)
        # ========================================
        saved_paths = save_models(
            lstm_model=lstm_model,
            gru_model=gru_model,
            xgb_model=xgb_stacking_regressor,
            scaler_X=scaler_X,
            scaler_y=scaler_y,
            feature_cols=feature_cols,
            lstm_metrics=lstm_metrics,
            gru_metrics=gru_metrics,
            stacked_metrics=stacked_metrics,
            model_dir=model_dir
        )
        
        # 10. ê²°ê³¼ ì‹œê°í™” (íŒŒì¼ë¡œ ì €ì¥)
        print("\nê²°ê³¼ ì‹œê°í™” ì¤‘...")
        
        # ì²« ë²ˆì§¸ ê·¸ë¦¼: ëª¨ë¸ë³„ ìƒì„¸ ë¶„ì„
        fig1 = plt.figure(figsize=(20, 15))
        
        # 1. í•™ìŠµ ê³¡ì„ 
        plt.subplot(3, 4, 1)
        if lstm_train_losses and lstm_val_losses:
            plt.plot(lstm_train_losses, label='LSTM Train Loss', alpha=0.7)
            plt.plot(lstm_val_losses, label='LSTM Val Loss', alpha=0.7)
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('LSTM Learning Curves')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.subplot(3, 4, 2)
        if gru_train_losses and gru_val_losses:
            plt.plot(gru_train_losses, label='GRU Train Loss', alpha=0.7)
            plt.plot(gru_val_losses, label='GRU Val Loss', alpha=0.7)
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('GRU Learning Curves')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 2. ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ
        test_range = min(200, len(y_test_stack))
        
        plt.subplot(3, 4, 3)
        plt.plot(lstm_actuals_original[:test_range], label='Actual', alpha=0.8, linewidth=2)
        plt.plot(lstm_predictions_original[:test_range], label='LSTM', alpha=0.7)
        plt.xlabel('Time')
        plt.ylabel('ë°œì „ëŸ‰ (MWh)')
        plt.title(f'LSTM ì˜ˆì¸¡ ê²°ê³¼\nNMAE: {nmae_lstm:.3f}')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.subplot(3, 4, 4)
        plt.plot(gru_actuals_original[:test_range], label='Actual', alpha=0.8, linewidth=2)
        plt.plot(gru_predictions_original[:test_range], label='GRU', alpha=0.7)
        plt.xlabel('Time')
        plt.ylabel('ë°œì „ëŸ‰ (MWh)')
        plt.title(f'GRU ì˜ˆì¸¡ ê²°ê³¼\nNMAE: {nmae_gru:.3f}')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.subplot(3, 4, 5)
        plt.plot(y_test_stack[:test_range], label='Actual', alpha=0.8, linewidth=2)
        plt.plot(stacked_pred_test[:test_range], label='Stacked', alpha=0.7)
        plt.xlabel('Time')
        plt.ylabel('ë°œì „ëŸ‰ (MWh)')
        plt.title(f'XGBoost ìŠ¤íƒœí‚¹ ê²°ê³¼\nNMAE: {nmae_stacked:.3f}')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 3. ì‚°ì ë„ (ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’)
        plt.subplot(3, 4, 6)
        plt.scatter(lstm_actuals_original, lstm_predictions_original, alpha=0.5)
        plt.plot([lstm_actuals_original.min(), lstm_actuals_original.max()], 
                [lstm_actuals_original.min(), lstm_actuals_original.max()], 'r--', lw=2)
        plt.xlabel('ì‹¤ì œê°’')
        plt.ylabel('ì˜ˆì¸¡ê°’')
        plt.title('LSTM: ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’')
        plt.grid(True, alpha=0.3)
        
        plt.subplot(3, 4, 7)
        plt.scatter(gru_actuals_original, gru_predictions_original, alpha=0.5)
        plt.plot([gru_actuals_original.min(), gru_actuals_original.max()], 
                [gru_actuals_original.min(), gru_actuals_original.max()], 'r--', lw=2)
        plt.xlabel('ì‹¤ì œê°’')
        plt.ylabel('ì˜ˆì¸¡ê°’')
        plt.title('GRU: ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’')
        plt.grid(True, alpha=0.3)
        
        plt.subplot(3, 4, 8)
        plt.scatter(y_test_stack, stacked_pred_test, alpha=0.5)
        plt.plot([y_test_stack.min(), y_test_stack.max()], 
                [y_test_stack.min(), y_test_stack.max()], 'r--', lw=2)
        plt.xlabel('ì‹¤ì œê°’')
        plt.ylabel('ì˜ˆì¸¡ê°’')
        plt.title('Stacked: ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’')
        plt.grid(True, alpha=0.3)
        
        # 4. íŠ¹ì„± ì¤‘ìš”ë„
        plt.subplot(3, 4, 9)
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': xgb_stacking_regressor.feature_importances_
        }).sort_values('importance', ascending=True)
        
        plt.barh(importance_df['feature'], importance_df['importance'])
        plt.xlabel('ì¤‘ìš”ë„')
        plt.title('XGBoost ìŠ¤íƒœí‚¹ íŠ¹ì„± ì¤‘ìš”ë„')
        plt.grid(True, alpha=0.3)
        
        # 5. ì”ì°¨ ë¶„ì„
        lstm_residuals = lstm_actuals_original - lstm_predictions_original
        gru_residuals = gru_actuals_original - gru_predictions_original
        stacked_residuals = y_test_stack - stacked_pred_test
        
        plt.subplot(3, 4, 10)
        plt.scatter(lstm_predictions_original, lstm_residuals, alpha=0.5)
        plt.axhline(y=0, color='r', linestyle='--')
        plt.xlabel('ì˜ˆì¸¡ê°’')
        plt.ylabel('ì”ì°¨')
        plt.title('LSTM ì”ì°¨ í”Œë¡¯')
        plt.grid(True, alpha=0.3)
        
        plt.subplot(3, 4, 11)
        plt.scatter(gru_predictions_original, gru_residuals, alpha=0.5)
        plt.axhline(y=0, color='r', linestyle='--')
        plt.xlabel('ì˜ˆì¸¡ê°’')
        plt.ylabel('ì”ì°¨')
        plt.title('GRU ì”ì°¨ í”Œë¡¯')
        plt.grid(True, alpha=0.3)
        
        plt.subplot(3, 4, 12)
        plt.scatter(stacked_pred_test, stacked_residuals, alpha=0.5)
        plt.axhline(y=0, color='r', linestyle='--')
        plt.xlabel('ì˜ˆì¸¡ê°’')
        plt.ylabel('ì”ì°¨')
        plt.title('Stacked ì”ì°¨ í”Œë¡¯')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plot1_path = os.path.join(output_dir, '01_detailed_analysis.png')
        plt.savefig(plot1_path, dpi=300, bbox_inches='tight')
        print(f"âœ… ìƒì„¸ ë¶„ì„ ê·¸ë˜í”„ ì €ì¥: {plot1_path}")
        plt.close()
        
        # 11. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¹„êµ ë§‰ëŒ€ ê·¸ë˜í”„
        fig2 = plt.figure(figsize=(15, 5))
        
        models = ['LSTM', 'GRU', 'XGBoost\nStacking']
        nmae_scores = [nmae_lstm, nmae_gru, nmae_stacked]
        nrmse_scores = [nrmse_lstm, nrmse_gru, nrmse_stacked]
        r2_scores = [r2_lstm, r2_gru, r2_stacked]
        mape_scores = [mape_lstm, mape_gru, mape_stacked]
        
        x = np.arange(len(models))
        width = 0.25
        
        plt.subplot(1, 4, 1)
        plt.bar(x, nmae_scores, width, label='NMAE', alpha=0.8)
        plt.xlabel('ëª¨ë¸')
        plt.ylabel('NMAE')
        plt.title('NMAE ë¹„êµ')
        plt.xticks(x, models)
        plt.grid(True, alpha=0.3)
        
        plt.subplot(1, 4, 2)
        plt.bar(x, nrmse_scores, width, label='NRMSE', alpha=0.8, color='orange')
        plt.xlabel('ëª¨ë¸')
        plt.ylabel('NRMSE')
        plt.title('NRMSE ë¹„êµ')
        plt.xticks(x, models)
        plt.grid(True, alpha=0.3)

        plt.subplot(1, 4, 3)
        plt.bar(x, r2_scores, width, label='RÂ²', alpha=0.8, color='red')
        plt.xlabel('ëª¨ë¸')
        plt.ylabel('RÂ²')
        plt.title('RÂ² ë¹„êµ')
        plt.xticks(x, models)
        plt.grid(True, alpha=0.3)

        plt.subplot(1, 4, 4)
        plt.bar(x, mape_scores, width, label='MAPE (%)', alpha=0.8, color='green')
        plt.xlabel('ëª¨ë¸')
        plt.ylabel('MAPE (%)')
        plt.title('MAPE ë¹„êµ')
        plt.xticks(x, models)
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plot2_path = os.path.join(output_dir, '02_metrics_comparison.png')
        plt.savefig(plot2_path, dpi=300, bbox_inches='tight')
        print(f"âœ… ì„±ëŠ¥ ë¹„êµ ê·¸ë˜í”„ ì €ì¥: {plot2_path}")
        plt.close()
        
        print(f"\n{'='*80}")
        print("ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ì™„ë£Œ!")
        print(f"{'='*80}")
        print(f"\nğŸ“Š ëª¨ë“  ê·¸ë˜í”„ê°€ '{output_dir}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:")
        print(f"  - {plot1_path}")
        print(f"  - {plot2_path}")
        
        print(f"\nğŸ’¾ ì €ì¥ëœ ëª¨ë¸ íŒŒì¼:")
        for model_type, path in saved_paths.items():
            print(f"  - {model_type}: {path}")
        
    except FileNotFoundError:
        print(f"Error: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”: {data_path}")
    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()