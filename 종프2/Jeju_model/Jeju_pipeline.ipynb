{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer  # 추가\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "\n",
    "# 데이터 경로\n",
    "data_path = \"C:/Users/rlask/종프2/dataset/jeju_solar_utf8.csv\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "def calculate_rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero_mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100\n",
    "\n",
    "\n",
    "def preprocess_data(data_df):\n",
    "    \"\"\"\n",
    "    데이터 전처리 함수 - 결측값 처리 및 데이터 클리닝\n",
    "    \"\"\"\n",
    "    print(\"데이터 전처리 시작...\")\n",
    "    \n",
    "    # 데이터 정보 출력\n",
    "    print(f\"원본 데이터 크기: {data_df.shape}\")\n",
    "    print(f\"결측값 개수:\")\n",
    "    missing_counts = data_df.isnull().sum()\n",
    "    for col, count in missing_counts.items():\n",
    "        if count > 0:\n",
    "            print(f\"  {col}: {count} ({count/len(data_df)*100:.1f}%)\")\n",
    "    \n",
    "    # 필요한 컬럼만 선택\n",
    "    required_columns = [\"기온\", \"강수량(mm)\", \"일조(hr)\", \"일사량\", \"태양광 발전량(MWh)\"]\n",
    "    \n",
    "    # 컬럼 존재 여부 확인\n",
    "    missing_cols = [col for col in required_columns if col not in data_df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"누락된 컬럼: {missing_cols}\")\n",
    "        return None, None\n",
    "    \n",
    "    # 데이터 추출\n",
    "    processed_df = data_df[required_columns].copy()\n",
    "    \n",
    "    # 결측값 처리\n",
    "    # 1. 기온: 전후 값의 평균으로 보간\n",
    "    processed_df['기온'] = processed_df['기온'].interpolate(method='linear')\n",
    "    \n",
    "    # 2. 강수량: 0으로 채움 (비가 안 온 것으로 가정)\n",
    "    processed_df['강수량(mm)'] = processed_df['강수량(mm)'].fillna(0)\n",
    "    \n",
    "    # 3. 일조와 일사량: 계절성과 시간대를 고려한 보간\n",
    "    if '일시' in data_df.columns:\n",
    "        # 날짜 정보가 있다면 시간대별 평균으로 보간\n",
    "        data_df['datetime'] = pd.to_datetime(data_df['일시'])\n",
    "        data_df['hour'] = data_df['datetime'].dt.hour\n",
    "        data_df['month'] = data_df['datetime'].dt.month\n",
    "        \n",
    "        # 시간대별, 월별 평균값으로 결측값 보간\n",
    "        for col in ['일조(hr)', '일사량']:\n",
    "            if col in processed_df.columns:\n",
    "                # 먼저 시간대별 평균으로 보간\n",
    "                hourly_mean = data_df.groupby('hour')[col].transform('mean')\n",
    "                processed_df[col] = processed_df[col].fillna(hourly_mean)\n",
    "                \n",
    "                # 여전히 NaN이 있다면 전체 평균으로 보간\n",
    "                processed_df[col] = processed_df[col].fillna(processed_df[col].mean())\n",
    "    else:\n",
    "        # 날짜 정보가 없다면 단순 보간\n",
    "        processed_df['일조(hr)'] = processed_df['일조(hr)'].interpolate(method='linear')\n",
    "        processed_df['일사량'] = processed_df['일사량'].interpolate(method='linear')\n",
    "        \n",
    "        # 여전히 NaN이 있다면 평균값으로 채움\n",
    "        processed_df['일조(hr)'] = processed_df['일조(hr)'].fillna(processed_df['일조(hr)'].mean())\n",
    "        processed_df['일사량'] = processed_df['일사량'].fillna(processed_df['일사량'].mean())\n",
    "    \n",
    "    # 4. 발전량: 0으로 채움 (발전이 안 된 것으로 가정)\n",
    "    processed_df['태양광 발전량(MWh)'] = processed_df['태양광 발전량(MWh)'].fillna(0)\n",
    "    \n",
    "    # 이상값 제거 (IQR 방법 사용)\n",
    "    def remove_outliers(series, factor=1.5):\n",
    "        Q1 = series.quantile(0.25)\n",
    "        Q3 = series.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - factor * IQR\n",
    "        upper = Q3 + factor * IQR\n",
    "        return series.clip(lower, upper)\n",
    "    \n",
    "    # 기온과 발전량에 대해서만 이상값 처리\n",
    "    processed_df['기온'] = remove_outliers(processed_df['기온'])\n",
    "    processed_df['태양광 발전량(MWh)'] = remove_outliers(processed_df['태양광 발전량(MWh)'])\n",
    "    \n",
    "    # 최종 결측값 확인\n",
    "    final_missing = processed_df.isnull().sum()\n",
    "    if final_missing.sum() > 0:\n",
    "        print(\"전처리 후 남은 결측값:\")\n",
    "        for col, count in final_missing.items():\n",
    "            if count > 0:\n",
    "                print(f\"  {col}: {count}\")\n",
    "        \n",
    "        # 남은 결측값이 있다면 해당 행 제거\n",
    "        processed_df = processed_df.dropna()\n",
    "        print(f\"결측값이 있는 행 제거 후 데이터 크기: {processed_df.shape}\")\n",
    "    \n",
    "    # 특징과 타겟 분리\n",
    "    features = processed_df[[\"기온\", \"강수량(mm)\", \"일조(hr)\", \"일사량\"]].values\n",
    "    targets = processed_df[\"태양광 발전량(MWh)\"].values\n",
    "    \n",
    "    print(f\"전처리 완료 - 최종 데이터 크기: {len(features)} 행\")\n",
    "    print(f\"특징 데이터 형태: {features.shape}\")\n",
    "    print(f\"타겟 데이터 형태: {targets.shape}\")\n",
    "    \n",
    "    # 데이터 통계 정보 출력\n",
    "    print(\"\\n=== 데이터 통계 정보 ===\")\n",
    "    stats_df = processed_df.describe()\n",
    "    print(stats_df)\n",
    "    \n",
    "    return features, targets\n",
    "\n",
    "\n",
    "class PatternExtractor:\n",
    "    \"\"\"\n",
    "    기상 데이터에서 태양광 발전량과 관련된 패턴을 추출하는 클래스\n",
    "    \"\"\"\n",
    "    def __init__(self, n_patterns=5):\n",
    "        self.n_patterns = n_patterns\n",
    "        self.kmeans = None\n",
    "        self.pattern_labels = None\n",
    "        self.pattern_centers = None\n",
    "        self.imputer = SimpleImputer(strategy='mean')  # 추가 안전장치\n",
    "        \n",
    "    def extract_weather_patterns(self, features, targets):\n",
    "        \"\"\"\n",
    "        기상 조건과 발전량의 관계에서 패턴 추출\n",
    "        \"\"\"\n",
    "        # 입력 데이터 검증 및 결측값 처리\n",
    "        print(\"패턴 추출을 위한 데이터 검증 중...\")\n",
    "        \n",
    "        # NaN 체크\n",
    "        if np.isnan(features).any():\n",
    "            print(\"Warning: features에 NaN 값이 발견되어 imputer로 처리합니다.\")\n",
    "            features = self.imputer.fit_transform(features)\n",
    "        \n",
    "        if np.isnan(targets).any():\n",
    "            print(\"Warning: targets에 NaN 값이 발견되어 평균값으로 처리합니다.\")\n",
    "            targets = np.nan_to_num(targets, nan=np.nanmean(targets))\n",
    "        \n",
    "        # 무한값 체크 및 처리\n",
    "        if np.isinf(features).any():\n",
    "            print(\"Warning: features에 무한값이 발견되어 처리합니다.\")\n",
    "            features = np.nan_to_num(features, posinf=np.nanmax(features[np.isfinite(features)]), \n",
    "                                   neginf=np.nanmin(features[np.isfinite(features)]))\n",
    "        \n",
    "        if np.isinf(targets).any():\n",
    "            print(\"Warning: targets에 무한값이 발견되어 처리합니다.\")\n",
    "            targets = np.nan_to_num(targets, posinf=np.nanmax(targets[np.isfinite(targets)]), \n",
    "                                  neginf=np.nanmin(targets[np.isfinite(targets)]))\n",
    "        \n",
    "        # 기상 데이터와 발전량을 결합하여 패턴 분석\n",
    "        combined_data = np.column_stack([features, targets.reshape(-1, 1)])\n",
    "        \n",
    "        # 데이터 정규화 (클러스터링 성능 향상을 위해)\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        combined_data_scaled = scaler.fit_transform(combined_data)\n",
    "        \n",
    "        # K-means 클러스터링으로 패턴 추출\n",
    "        self.kmeans = KMeans(n_clusters=self.n_patterns, random_state=42, n_init=10)\n",
    "        self.pattern_labels = self.kmeans.fit_predict(combined_data_scaled)\n",
    "        \n",
    "        # 원본 스케일의 센터 계산 (해석을 위해)\n",
    "        self.pattern_centers = []\n",
    "        for i in range(self.n_patterns):\n",
    "            mask = self.pattern_labels == i\n",
    "            if mask.sum() > 0:\n",
    "                center = combined_data[mask].mean(axis=0)\n",
    "                self.pattern_centers.append(center)\n",
    "            else:\n",
    "                # 빈 클러스터인 경우 전체 평균 사용\n",
    "                self.pattern_centers.append(combined_data.mean(axis=0))\n",
    "        \n",
    "        self.pattern_centers = np.array(self.pattern_centers)\n",
    "        \n",
    "        print(f\"추출된 패턴 수: {self.n_patterns}\")\n",
    "        self._analyze_patterns()\n",
    "        \n",
    "        return self.pattern_labels\n",
    "    \n",
    "    def _analyze_patterns(self):\n",
    "        \"\"\"\n",
    "        추출된 패턴 분석 및 출력\n",
    "        \"\"\"\n",
    "        print(\"\\n=== 패턴 분석 결과 ===\")\n",
    "        pattern_names = [\"저발전\", \"저중발전\", \"중발전\", \"중고발전\", \"고발전\"]\n",
    "        \n",
    "        # 발전량 기준으로 패턴 정렬\n",
    "        pattern_power = [(i, center[4]) for i, center in enumerate(self.pattern_centers)]\n",
    "        pattern_power.sort(key=lambda x: x[1])\n",
    "        \n",
    "        for idx, (pattern_idx, power) in enumerate(pattern_power):\n",
    "            center = self.pattern_centers[pattern_idx]\n",
    "            pattern_name = pattern_names[idx] if idx < len(pattern_names) else f\"패턴{idx}\"\n",
    "            count = (self.pattern_labels == pattern_idx).sum()\n",
    "            \n",
    "            print(f\"패턴 {pattern_idx+1} ({pattern_name}) - 데이터 수: {count}\")\n",
    "            print(f\"  - 평균 기온: {center[0]:.2f}°C\")\n",
    "            print(f\"  - 평균 강수량: {center[1]:.2f}mm\")\n",
    "            print(f\"  - 평균 일조시간: {center[2]:.2f}hr\")\n",
    "            print(f\"  - 평균 일사량: {center[3]:.2f}\")\n",
    "            print(f\"  - 평균 발전량: {center[4]:.2f}MWh\")\n",
    "            print()\n",
    "    \n",
    "    def get_pattern_features(self, features):\n",
    "        \"\"\"\n",
    "        새로운 데이터에 대해 패턴 레이블 예측\n",
    "        \"\"\"\n",
    "        if self.kmeans is None:\n",
    "            raise ValueError(\"먼저 extract_weather_patterns를 실행해주세요.\")\n",
    "        \n",
    "        # NaN 처리\n",
    "        if np.isnan(features).any():\n",
    "            features = self.imputer.transform(features)\n",
    "        \n",
    "        # 발전량이 없는 경우, 기상 데이터만으로 패턴 예측\n",
    "        dummy_targets = np.zeros((len(features), 1))\n",
    "        combined_data = np.column_stack([features, dummy_targets])\n",
    "        \n",
    "        return self.kmeans.predict(combined_data)\n",
    "\n",
    "\n",
    "class SolarPatternDataset(Dataset):\n",
    "    \"\"\"\n",
    "    패턴 정보를 포함한 태양광 발전 데이터셋\n",
    "    \"\"\"\n",
    "    def __init__(self, features, targets, patterns, seq_len=24):\n",
    "        self.X, self.y, self.patterns = [], [], []\n",
    "        \n",
    "        # patterns는 1D 배열 (정수 레이블) 이라고 가정\n",
    "        patterns = np.array(patterns).astype(np.int64)\n",
    "        \n",
    "        # 시퀀스 데이터 생성 (패턴 정보 포함)\n",
    "        for i in range(len(features) - seq_len):\n",
    "            self.X.append(features[i:i+seq_len])\n",
    "            self.y.append(targets[i+seq_len])\n",
    "            self.patterns.append(patterns[i:i+seq_len])  # 패턴 시퀀스\n",
    "\n",
    "        self.X = np.array(self.X, dtype=np.float32)            # (N, seq_len, feature_dim)\n",
    "        self.y = np.array(self.y, dtype=np.float32)\n",
    "        self.patterns = np.array(self.patterns, dtype=np.int64)  # 정수로 저장\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 기상 데이터와 패턴 정보를 결합\n",
    "        # 패턴은 정수이므로 float로 변환하여 concat\n",
    "        pattern_seq = self.patterns[idx].reshape(-1, 1).astype(np.float32)\n",
    "        features_with_patterns = np.concatenate([\n",
    "            self.X[idx], \n",
    "            pattern_seq\n",
    "        ], axis=1)\n",
    "        \n",
    "        return torch.tensor(features_with_patterns), torch.tensor(self.y[idx])\n",
    "\n",
    "\n",
    "class CNN_LSTM_Pattern(nn.Module):\n",
    "    \"\"\"\n",
    "    패턴 정보를 활용한 CNN + LSTM 모델\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=5, seq_len=24, hidden_dim=64, num_layers=2, n_patterns=5):\n",
    "        super(CNN_LSTM_Pattern, self).__init__()\n",
    "        \n",
    "        self.n_patterns = n_patterns\n",
    "        \n",
    "        # Pattern Embedding Layer\n",
    "        self.pattern_embed = nn.Embedding(n_patterns, 8)\n",
    "        \n",
    "        # CNN layers (기상 데이터용)\n",
    "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        \n",
    "        # LSTM (CNN 출력 + 패턴 임베딩)\n",
    "        self.lstm = nn.LSTM(input_size=64+8, hidden_size=hidden_dim, \n",
    "                           num_layers=num_layers, batch_first=True, dropout=0.2)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=4, batch_first=True)\n",
    "        \n",
    "        # FC layers\n",
    "        self.fc1 = nn.Linear(hidden_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(128)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, total_features = x.shape\n",
    "        \n",
    "        # 기상 데이터와 패턴 데이터 분리\n",
    "        weather_data = x[:, :, :4]  # 기온, 강수량, 일조, 일사량\n",
    "        pattern_data = x[:, :, 4].long()  # 패턴 레이블\n",
    "        \n",
    "        # CNN for weather data\n",
    "        weather_data = weather_data.permute(0, 2, 1)  # (batch, features, seq_len)\n",
    "        conv_out = self.conv1(weather_data)\n",
    "        conv_out = self.relu(conv_out)\n",
    "        conv_out = self.conv2(conv_out)\n",
    "        conv_out = self.relu(conv_out)\n",
    "        conv_out = self.pool(conv_out)  # (batch, 64, seq_len//2)\n",
    "        conv_out = conv_out.permute(0, 2, 1)  # (batch, seq_len//2, 64)\n",
    "        \n",
    "        # Pattern embedding\n",
    "        pattern_emb = self.pattern_embed(pattern_data)  # (batch, seq_len, 8)\n",
    "        \n",
    "        # Adjust sequence length for pattern embedding\n",
    "        target_seq_len = conv_out.size(1)\n",
    "        if pattern_emb.size(1) != target_seq_len:\n",
    "            # Simple interpolation or pooling to match sequence lengths\n",
    "            pattern_emb = pattern_emb[:, :target_seq_len, :]\n",
    "        \n",
    "        # Combine CNN output with pattern embedding\n",
    "        combined_features = torch.cat([conv_out, pattern_emb], dim=2)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, (h_n, c_n) = self.lstm(combined_features)\n",
    "        \n",
    "        # Self-attention\n",
    "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        \n",
    "        # Use last hidden state\n",
    "        out = attn_out[:, -1, :]  # (batch, hidden_dim)\n",
    "        \n",
    "        # FC layers with batch normalization\n",
    "        out = self.fc1(out)\n",
    "        out = self.batch_norm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.batch_norm2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.fc3(out)\n",
    "        return out.squeeze()\n",
    "    \n",
    "    def train_model(self, train_loader, val_loader=None, epochs=50, lr=0.001):\n",
    "        \"\"\"\n",
    "        모델 학습 함수\n",
    "        \"\"\"\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "        \n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        print(f\"총 {epochs} 에포크 학습을 시작합니다...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience = 10\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            # Training\n",
    "            self.train()\n",
    "            train_loss = 0\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                preds = self(batch_X)\n",
    "                loss = criterion(preds, batch_y)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            train_losses.append(avg_train_loss)\n",
    "            \n",
    "            # Validation\n",
    "            if val_loader:\n",
    "                self.eval()\n",
    "                val_loss = 0\n",
    "                with torch.no_grad():\n",
    "                    for batch_X, batch_y in val_loader:\n",
    "                        preds = self(batch_X)\n",
    "                        loss = criterion(preds, batch_y)\n",
    "                        val_loss += loss.item()\n",
    "                \n",
    "                avg_val_loss = val_loss / len(val_loader)\n",
    "                val_losses.append(avg_val_loss)\n",
    "                \n",
    "                # Early stopping\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "                \n",
    "                scheduler.step()\n",
    "                \n",
    "                # 진행상황 출력\n",
    "                epoch_time = time.time() - epoch_start_time\n",
    "                elapsed_time = time.time() - start_time\n",
    "                remaining_epochs = epochs - epoch - 1\n",
    "                estimated_remaining = (elapsed_time / (epoch + 1)) * remaining_epochs\n",
    "                \n",
    "                print(f\"Epoch [{epoch+1:3d}/{epochs}] | \"\n",
    "                      f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "                      f\"Val Loss: {avg_val_loss:.4f} | \"\n",
    "                      f\"LR: {scheduler.get_last_lr()[0]:.6f} | \"\n",
    "                      f\"시간: {epoch_time:.1f}s | \"\n",
    "                      f\"예상 남은 시간: {estimated_remaining/60:.1f}분\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\n학습 완료! 총 소요시간: {total_time/60:.1f}분\")\n",
    "        \n",
    "        return train_losses, val_losses\n",
    "    \n",
    "    def predict(self, test_loader):\n",
    "        \"\"\"\n",
    "        모델 예측 함수\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in test_loader:\n",
    "                preds = self(batch_X)\n",
    "                predictions.extend(preds.cpu().numpy())\n",
    "                actuals.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        return np.array(predictions), np.array(actuals)\n",
    "\n",
    "\n",
    "def create_pattern_features(features, targets, pattern_extractor):\n",
    "    \"\"\"\n",
    "    패턴 기반 특징 생성\n",
    "    \"\"\"\n",
    "    # 기본 패턴 추출\n",
    "    patterns = pattern_extractor.extract_weather_patterns(features, targets)\n",
    "    \n",
    "    # 추가 패턴 기반 특징 생성\n",
    "    pattern_features = []\n",
    "    \n",
    "    for i in range(len(features)):\n",
    "        pattern_feat = [\n",
    "            patterns[i],  # 현재 패턴\n",
    "            # 계절성 특징\n",
    "            np.sin(2 * np.pi * i / 365),  # 연간 주기\n",
    "            np.cos(2 * np.pi * i / 365),\n",
    "            np.sin(2 * np.pi * i / 24),   # 일간 주기 (가정)\n",
    "            np.cos(2 * np.pi * i / 24),\n",
    "            # 기상 조건 조합 특징\n",
    "            features[i][0] * features[i][3],  # 기온 * 일사량\n",
    "            features[i][2] * features[i][3],  # 일조 * 일사량\n",
    "            # 강수량이 발전에 미치는 영향\n",
    "            1 if features[i][1] < 0.1 else 0,  # 무강수 여부\n",
    "        ]\n",
    "        pattern_features.append(pattern_feat)\n",
    "    \n",
    "    return np.array(pattern_features)\n",
    "\n",
    "\n",
    "def xgb_model_with_patterns(X_train, y_train, X_val, y_val, pattern_features_train, \n",
    "                           pattern_features_val, plotting=False):\n",
    "    \"\"\"\n",
    "    패턴 정보를 활용한 XGBoost 모델\n",
    "    \"\"\"\n",
    "    # 패턴 특징과 원본 특징 결합\n",
    "    X_train_combined = np.column_stack([X_train, pattern_features_train])\n",
    "    X_val_combined = np.column_stack([X_val, pattern_features_val])\n",
    "    \n",
    "    # XGBoost 모델 정의 (하이퍼파라미터 튜닝)\n",
    "    xgb_regressor = xgb.XGBRegressor(\n",
    "        gamma=0.5, \n",
    "        n_estimators=300, \n",
    "        learning_rate=0.08, \n",
    "        max_depth=6,\n",
    "        min_child_weight=3,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        early_stopping_rounds=50,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=0.1\n",
    "    )\n",
    "    \n",
    "    # 모델 학습\n",
    "    xgb_regressor.fit(\n",
    "        X_train_combined, \n",
    "        y_train, \n",
    "        eval_set=[(X_val_combined, y_val)], \n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # 검증 데이터에 대한 예측\n",
    "    pred_val = xgb_regressor.predict(X_val_combined)\n",
    "    mae = mean_absolute_error(y_val, pred_val)\n",
    "    rmse = calculate_rmse(y_val, pred_val)\n",
    "    \n",
    "    # 특성 중요도 출력\n",
    "    feature_names = ['기온', '강수량', '일조', '일사량'] + \\\n",
    "                   ['패턴', '연간_sin', '연간_cos', '일간_sin', '일간_cos', \n",
    "                    '기온×일사량', '일조×일사량', '무강수여부']\n",
    "    \n",
    "    importance_dict = dict(zip(feature_names, xgb_regressor.feature_importances_))\n",
    "    print(\"\\n=== XGBoost 특성 중요도 ===\")\n",
    "    for feature, importance in sorted(importance_dict.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{feature}: {importance:.4f}\")\n",
    "    \n",
    "    # 결과 시각화\n",
    "    if plotting:\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # 예측 결과 플롯\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(y_val[:200], label='Actual', alpha=0.7)\n",
    "        plt.plot(pred_val[:200], label='Predicted', alpha=0.7)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"DC Power\")\n",
    "        plt.title(f\"XGBoost with Patterns - MAE: {mae:.3f}, RMSE: {rmse:.3f}\")\n",
    "        plt.legend()\n",
    "        \n",
    "        # 산점도\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.scatter(y_val, pred_val, alpha=0.5)\n",
    "        plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\n",
    "        plt.xlabel('Actual')\n",
    "        plt.ylabel('Predicted')\n",
    "        plt.title('Actual vs Predicted')\n",
    "        \n",
    "        # 특성 중요도\n",
    "        plt.subplot(2, 2, 3)\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': xgb_regressor.feature_importances_\n",
    "        }).sort_values('importance', ascending=True)\n",
    "        \n",
    "        plt.barh(importance_df['feature'], importance_df['importance'])\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title('Feature Importance')\n",
    "        \n",
    "        # 잔차 분석\n",
    "        plt.subplot(2, 2, 4)\n",
    "        residuals = y_val - pred_val\n",
    "        plt.scatter(pred_val, residuals, alpha=0.5)\n",
    "        plt.axhline(y=0, color='r', linestyle='--')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Residuals')\n",
    "        plt.title('Residual Plot')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    return mae, rmse, xgb_regressor\n",
    "\n",
    "\n",
    "def create_sequences_and_split_with_patterns(features, targets, pattern_features, \n",
    "                                           seq_len=24, test_size=0.2, val_size=0.1):\n",
    "    \"\"\"\n",
    "    패턴 정보를 포함한 시퀀스 데이터 생성 및 분할\n",
    "    \"\"\"\n",
    "    # 스케일링 (기상 피처와 타겟만 스케일)\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    target_scaler = MinMaxScaler()\n",
    "    \n",
    "    features_scaled = feature_scaler.fit_transform(features)\n",
    "    targets_scaled = target_scaler.fit_transform(targets.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # pattern_features의 첫 열은 '패턴 라벨' (정수)\n",
    "    # 절대 스케일링하지 않고 정수로 유지\n",
    "    patterns = pattern_features[:, 0].astype(int)  # e.g. 0,1,2,...,n_patterns-1\n",
    "    \n",
    "    # 데이터셋 생성\n",
    "    dataset = SolarPatternDataset(features_scaled, targets_scaled, patterns, seq_len)\n",
    "    \n",
    "    # 데이터 분할 (비율 기반)\n",
    "    dataset_size = len(dataset)\n",
    "    test_split = int(dataset_size * test_size)\n",
    "    val_split = int(dataset_size * val_size)\n",
    "    train_split = dataset_size - test_split - val_split\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "        dataset, [train_split, val_split, test_split]\n",
    "    )\n",
    "    \n",
    "    return (train_dataset, val_dataset, test_dataset, \n",
    "            feature_scaler, target_scaler)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        print(\"데이터 로딩 중...\")\n",
    "        data_df = pd.read_csv(data_path)\n",
    "\n",
    "        features = data_df[[\"기온\", \"강수량(mm)\", \"일조(hr)\", \"일사량\"]].values\n",
    "        targets = data_df[\"태양광 발전량(MWh)\"].values\n",
    "        \n",
    "        print(f\"데이터 크기: {len(data_df)} 행\")\n",
    "        print(\"패턴 추출 중...\")\n",
    "        \n",
    "        # 패턴 추출기 생성 및 패턴 추출\n",
    "        pattern_extractor = PatternExtractor(n_patterns=5)\n",
    "        pattern_features = create_pattern_features(features, targets, pattern_extractor)\n",
    "        \n",
    "        print(\"데이터 전처리 중...\")\n",
    "        seq_len = 24\n",
    "        \n",
    "        # 패턴 정보를 포함한 시퀀스 데이터 생성\n",
    "        (train_dataset, val_dataset, test_dataset, \n",
    "        feature_scaler, target_scaler) = create_sequences_and_split_with_patterns(\n",
    "            features, targets, pattern_features, seq_len=seq_len\n",
    "        )\n",
    "        \n",
    "        # DataLoader 생성\n",
    "        batch_size = 32\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        print(f\"Train batches: {len(train_loader)}\")\n",
    "        print(f\"Validation batches: {len(val_loader)}\")\n",
    "        print(f\"Test batches: {len(test_loader)}\")\n",
    "        \n",
    "        # CNN + LSTM 모델 생성 및 학습\n",
    "        model = CNN_LSTM_Pattern(input_dim=5, seq_len=seq_len, \n",
    "                                hidden_dim=128, num_layers=2, n_patterns=5)\n",
    "        print(\"CNN+LSTM 모델 생성 완료\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CNN+LSTM 모델 학습 시작...\")\n",
    "        print(\"=\"*60)\n",
    "        train_losses, val_losses = model.train_model(\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            epochs=100,\n",
    "            lr=0.001\n",
    "        )\n",
    "        \n",
    "        # CNN+LSTM 테스트 예측\n",
    "        print(\"\\nCNN+LSTM 테스트 예측 중...\")\n",
    "        predictions, actuals = model.predict(test_loader)\n",
    "        \n",
    "        # 원본 스케일로 변환\n",
    "        predictions_original = target_scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "        actuals_original = target_scaler.inverse_transform(actuals.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # 성능 평가\n",
    "        mae_cnn_lstm = mean_absolute_error(actuals_original, predictions_original)\n",
    "        rmse_cnn_lstm = calculate_rmse(actuals_original, predictions_original)\n",
    "        mape_cnn_lstm = calculate_mape(actuals_original, predictions_original)\n",
    "        \n",
    "        print(f\"\\n=== CNN+LSTM 모델 성능 평가 ===\")\n",
    "        print(f\"MAE: {mae_cnn_lstm:.4f}\")\n",
    "        print(f\"RMSE: {rmse_cnn_lstm:.4f}\")\n",
    "        print(f\"MAPE: {mape_cnn_lstm:.4f}%\")\n",
    "        \n",
    "        # XGBoost 모델 학습 (비교용)\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"XGBoost 모델 학습 시작...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # 데이터 분할 (XGBoost용)\n",
    "        split_idx = int(len(features) * 0.8)\n",
    "        X_train_xgb, X_test_xgb = features[:split_idx], features[split_idx:]\n",
    "        y_train_xgb, y_test_xgb = targets[:split_idx], targets[split_idx:]\n",
    "        pattern_train_xgb = pattern_features[:split_idx]\n",
    "        pattern_test_xgb = pattern_features[split_idx:]\n",
    "        \n",
    "        # 검증 데이터 분할\n",
    "        val_split_idx = int(len(X_train_xgb) * 0.8)\n",
    "        X_train_final = X_train_xgb[:val_split_idx]\n",
    "        X_val_final = X_train_xgb[val_split_idx:]\n",
    "        y_train_final = y_train_xgb[:val_split_idx]\n",
    "        y_val_final = y_train_xgb[val_split_idx:]\n",
    "        pattern_train_final = pattern_train_xgb[:val_split_idx]\n",
    "        pattern_val_final = pattern_train_xgb[val_split_idx:]\n",
    "        \n",
    "        # XGBoost 학습\n",
    "        mae_xgb, rmse_xgb, xgb_model = xgb_model_with_patterns(\n",
    "            X_train_final, y_train_final, X_val_final, y_val_final,\n",
    "            pattern_train_final, pattern_val_final, plotting=True\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n=== XGBoost 모델 성능 평가 ===\")\n",
    "        print(f\"MAE: {mae_xgb:.4f}\")\n",
    "        print(f\"RMSE: {rmse_xgb:.4f}\")\n",
    "        \n",
    "        # 최종 비교\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"최종 모델 성능 비교\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"CNN+LSTM: MAE={mae_cnn_lstm:.4f}, RMSE={rmse_cnn_lstm:.4f}, MAPE={mape_cnn_lstm:.2f}%\")\n",
    "        print(f\"XGBoost:  MAE={mae_xgb:.4f}, RMSE={rmse_xgb:.4f}\")\n",
    "        \n",
    "        # 학습 곡선 시각화\n",
    "        if train_losses and val_losses:\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(train_losses, label='Train Loss')\n",
    "            plt.plot(val_losses, label='Validation Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('CNN+LSTM Training History')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(actuals_original[:200], label='Actual', alpha=0.7)\n",
    "            plt.plot(predictions_original[:200], label='CNN+LSTM Predicted', alpha=0.7)\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('Solar Power (MWh)')\n",
    "            plt.title('CNN+LSTM Prediction Results')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: 파일을 찾을 수 없습니다. 경로를 확인해주세요: {data_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
