"""
ì‹¤ì‹œê°„ íƒœì–‘ê´‘ ë°œì „ëŸ‰ ì˜ˆì¸¡ ì‹œìŠ¤í…œ
- í˜„ì¬ ì‹œì (2025-11-10) ê¸°ì¤€ ì˜ˆì¸¡
- 24ì‹œê°„, 48ì‹œê°„, 72ì‹œê°„ ì´í›„ íƒœì–‘ê´‘ ë°œì „ëŸ‰ ì˜ˆì¸¡
- ì¼ìë³„ íƒœì–‘ê´‘ ë°œì „ëŸ‰ MWh ì˜ˆì¸¡
- ëˆ„ì  ë°œì „ëŸ‰ í‘œì‹œ
"""

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import pickle
import json
import os
import xgboost as xgb
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# GPU/CUDA ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸš€ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")


# === ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ (í•™ìŠµ ì½”ë“œì™€ ë™ì¼) ===
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(
            input_size, 
            hidden_size, 
            num_layers, 
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        self.fc = nn.Linear(hidden_size, 1)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        lstm_out = self.dropout(lstm_out[:, -1, :])
        output = self.fc(lstm_out)
        return output


class GRUModel(nn.Module):
    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2):
        super(GRUModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.gru = nn.GRU(
            input_size, 
            hidden_size, 
            num_layers, 
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        self.fc = nn.Linear(hidden_size, 1)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        gru_out, _ = self.gru(x)
        gru_out = self.dropout(gru_out[:, -1, :])
        output = self.fc(gru_out)
        return output


# === ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ ===
def load_trained_models(model_dir='./saved_models', timestamp=None):
    """ì €ì¥ëœ ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ"""
    print(f"\n{'='*80}")
    print("ëª¨ë¸ ë¡œë“œ ì¤‘...")
    print(f"{'='*80}")
    
    # ìµœì‹  ëª¨ë¸ ì •ë³´ ë¡œë“œ
    if timestamp is None:
        latest_path = os.path.join(model_dir, 'latest_models.json')
        if not os.path.exists(latest_path):
            raise FileNotFoundError(f"ìµœì‹  ëª¨ë¸ ì •ë³´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {latest_path}")
        
        with open(latest_path, 'r', encoding='utf-8') as f:
            model_info = json.load(f)
        
        timestamp = model_info['timestamp']
        print(f"âœ… ìµœì‹  ëª¨ë¸ íƒ€ì„ìŠ¤íƒ¬í”„: {timestamp}")
    
    # ë©”íƒ€ë°ì´í„° ë¡œë“œ
    metadata_path = os.path.join(model_dir, f'metadata_{timestamp}.json')
    with open(metadata_path, 'r', encoding='utf-8') as f:
        metadata = json.load(f)
    
    print(f"âœ… ë©”íƒ€ë°ì´í„° ë¡œë“œ: {metadata_path}")
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
    scaler_path = os.path.join(model_dir, f'scalers_{timestamp}.pkl')
    with open(scaler_path, 'rb') as f:
        scalers = pickle.load(f)
    
    scaler_X = scalers['scaler_X']
    scaler_y = scalers['scaler_y']
    print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ: {scaler_path}")
    
    # LSTM ëª¨ë¸ ë¡œë“œ
    lstm_path = os.path.join(model_dir, f'lstm_model_{timestamp}.pth')
    lstm_checkpoint = torch.load(lstm_path, map_location=device)
    lstm_config = lstm_checkpoint['model_config']
    
    lstm_model = LSTMModel(
        input_size=lstm_config['input_size'],
        hidden_size=lstm_config['hidden_size'],
        num_layers=lstm_config['num_layers']
    ).to(device)
    lstm_model.load_state_dict(lstm_checkpoint['model_state_dict'])
    lstm_model.eval()
    print(f"âœ… LSTM ëª¨ë¸ ë¡œë“œ: {lstm_path}")
    
    # GRU ëª¨ë¸ ë¡œë“œ
    gru_path = os.path.join(model_dir, f'gru_model_{timestamp}.pth')
    gru_checkpoint = torch.load(gru_path, map_location=device)
    gru_config = gru_checkpoint['model_config']
    
    gru_model = GRUModel(
        input_size=gru_config['input_size'],
        hidden_size=gru_config['hidden_size'],
        num_layers=gru_config['num_layers']
    ).to(device)
    gru_model.load_state_dict(gru_checkpoint['model_state_dict'])
    gru_model.eval()
    print(f"âœ… GRU ëª¨ë¸ ë¡œë“œ: {gru_path}")
    
    # XGBoost ëª¨ë¸ ë¡œë“œ
    xgb_path = os.path.join(model_dir, f'xgboost_stacking_{timestamp}.json')
    xgb_model = xgb.XGBRegressor()
    xgb_model.load_model(xgb_path)
    print(f"âœ… XGBoost ìŠ¤íƒœí‚¹ ëª¨ë¸ ë¡œë“œ: {xgb_path}")
    
    print(f"\nâœ¨ ëª¨ë“  ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    return {
        'lstm_model': lstm_model,
        'gru_model': gru_model,
        'xgb_model': xgb_model,
        'scaler_X': scaler_X,
        'scaler_y': scaler_y,
        'metadata': metadata
    }


# === í˜„ì¬ ì‹œì  ë°ì´í„° ì¤€ë¹„ í•¨ìˆ˜ ===
def prepare_current_data(data_path, current_datetime, models_dict, hours_needed=96):
    """
    í˜„ì¬ ì‹œì ê¹Œì§€ì˜ ë°ì´í„°ë¥¼ ì¤€ë¹„
    
    Args:
        data_path: í•™ìŠµ ë°ì´í„° ê²½ë¡œ
        current_datetime: í˜„ì¬ ì‹œì  (datetime ê°ì²´)
        models_dict: ë¡œë“œëœ ëª¨ë¸ ë”•ì…”ë„ˆë¦¬
        hours_needed: í•„ìš”í•œ ë°ì´í„° ì‹œê°„ ìˆ˜ (72ì‹œê°„ ì˜ˆì¸¡ + ì‹œí€€ìŠ¤ ê¸¸ì´)
    
    Returns:
        DataFrame: í˜„ì¬ ì‹œì ê¹Œì§€ì˜ ë°ì´í„°
    """
    print(f"\n{'='*80}")
    print(f"ğŸ“Š í˜„ì¬ ì‹œì  ë°ì´í„° ì¤€ë¹„ ì¤‘... (ê¸°ì¤€: {current_datetime.strftime('%Y-%m-%d %H:%M')})")
    print(f"{'='*80}")
    
    feature_cols = models_dict['metadata']['feature_columns']
    
    if os.path.exists(data_path):
        print(f"âœ… ë°ì´í„° íŒŒì¼ ë°œê²¬: {data_path}")
        df = pd.read_csv(data_path)
        
        # datetime ì»¬ëŸ¼ í™•ì¸ ë° ë³€í™˜
        datetime_col = None
        for col in ['datetime', 'Datetime', 'date', 'Date', 'ì‹œê°„', 'ì¼ì‹œ']:
            if col in df.columns:
                datetime_col = col
                break
        
        if datetime_col:
            df['datetime'] = pd.to_datetime(df[datetime_col])
        else:
            # datetime ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì—´ì„ ì‹œê°„ìœ¼ë¡œ ì¶”ì •
            df['datetime'] = pd.to_datetime(df.iloc[:, 0])
        
        # í˜„ì¬ ì‹œì  ì´ì „ ë°ì´í„°ë§Œ í•„í„°ë§
        df_filtered = df[df['datetime'] <= current_datetime].copy()
        
        if len(df_filtered) == 0:
            print(f"âš ï¸ í˜„ì¬ ì‹œì ({current_datetime}) ì´ì „ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            print(f"ë°ì´í„° ë²”ìœ„: {df['datetime'].min()} ~ {df['datetime'].max()}")
            print("ê°€ì¥ ìµœê·¼ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            df_filtered = df.tail(hours_needed).copy()
        else:
            # ë§ˆì§€ë§‰ Nì‹œê°„ ë°ì´í„° ì‚¬ìš©
            df_filtered = df_filtered.tail(hours_needed).copy()
        
        print(f"  â€¢ ì‚¬ìš© ë°ì´í„° ê¸°ê°„: {df_filtered['datetime'].min()} ~ {df_filtered['datetime'].max()}")
        print(f"  â€¢ ë°ì´í„° í¬ì¸íŠ¸: {len(df_filtered)}ì‹œê°„")
        
        # í•„ìš”í•œ íŠ¹ì„± ì»¬ëŸ¼ë§Œ ì¶”ì¶œ
        available_features = [col for col in feature_cols if col in df_filtered.columns]
        missing_features = [col for col in feature_cols if col not in df_filtered.columns]
        
        if missing_features:
            print(f"  âš ï¸ ëˆ„ë½ëœ íŠ¹ì„±: {missing_features}")
            print(f"  â†’ ë”ë¯¸ ë°ì´í„°ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        
        # í•„ìš”í•œ íŠ¹ì„± ë°ì´í„° ì¤€ë¹„
        current_data = df_filtered[['datetime']].copy()
        for col in feature_cols:
            if col in available_features:
                current_data[col] = df_filtered[col].values
            else:
                # ëˆ„ë½ëœ íŠ¹ì„±ì€ 0ìœ¼ë¡œ ì±„ì›€
                current_data[col] = 0.0
        
    else:
        print(f"âš ï¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_path}")
        print("ë”ë¯¸ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
        
        # ë”ë¯¸ ë°ì´í„° ìƒì„±
        end_time = current_datetime
        start_time = end_time - timedelta(hours=hours_needed-1)
        
        datetime_range = pd.date_range(start=start_time, end=end_time, freq='H')
        
        current_data = pd.DataFrame({
            'datetime': datetime_range
        })
        
        # ëœë¤ íŠ¹ì„± ë°ì´í„° ìƒì„± (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì„¼ì„œ ë°ì´í„° ì‚¬ìš©)
        for col in feature_cols:
            current_data[col] = np.random.randn(len(datetime_range)) * 0.5 + 0.5
    
    print(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ (shape: {current_data.shape})")
    
    return current_data


# === ì˜ˆì¸¡ í•¨ìˆ˜ ===
def predict_solar_generation(new_data, models_dict, sequence_length=24):
    """ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ íƒœì–‘ê´‘ ë°œì „ëŸ‰ ì˜ˆì¸¡"""
    # ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ ì¶”ì¶œ
    lstm_model = models_dict['lstm_model']
    gru_model = models_dict['gru_model']
    xgb_model = models_dict['xgb_model']
    scaler_X = models_dict['scaler_X']
    scaler_y = models_dict['scaler_y']
    metadata = models_dict['metadata']
    
    # íŠ¹ì„± ì»¬ëŸ¼ ì •ë³´
    feature_cols = metadata['feature_columns']
    expected_features = len(feature_cols)
    
    # ë°ì´í„° í˜•ì‹ ë³€í™˜ ë° íŠ¹ì„± ìˆœì„œ ë³´ì¥
    if isinstance(new_data, pd.DataFrame):
        try:
            new_data_array = new_data[feature_cols].values
        except KeyError as e:
            missing_cols = set(feature_cols) - set(new_data.columns)
            raise ValueError(f"í•„ìˆ˜ íŠ¹ì„± ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_cols}")
    else:
        new_data_array = new_data
        if new_data_array.shape[1] != expected_features:
            raise ValueError(
                f"ì…ë ¥ ë°ì´í„°ì˜ íŠ¹ì„± ê°œìˆ˜({new_data_array.shape[1]})ê°€ "
                f"ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤(ì˜ˆìƒ: {expected_features})"
            )
    
    # ë°ì´í„°ê°€ ì‹œí€€ìŠ¤ ê¸¸ì´ë³´ë‹¤ ì‘ìœ¼ë©´ ì—ëŸ¬
    if len(new_data_array) < sequence_length:
        raise ValueError(f"ë°ì´í„° ê¸¸ì´({len(new_data_array)})ê°€ ì‹œí€€ìŠ¤ ê¸¸ì´({sequence_length})ë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤.")
    
    # ê²°ì¸¡ê°’ ì²˜ë¦¬
    new_data_imputed = new_data_array.copy()
    for col_idx in range(new_data_imputed.shape[1]):
        col_data = new_data_imputed[:, col_idx]
        nan_count = np.sum(np.isnan(col_data))
        
        if nan_count > 0:
            if np.all(np.isnan(col_data)):
                new_data_imputed[:, col_idx] = 0
            else:
                col_mean = np.nanmean(col_data)
                new_data_imputed[:, col_idx] = np.where(
                    np.isnan(col_data), 
                    col_mean, 
                    col_data
                )
    
    # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
    new_data_scaled = scaler_X.transform(new_data_imputed)
    
    # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
    X_sequences = []
    for i in range(len(new_data_scaled) - sequence_length + 1):
        X_sequences.append(new_data_scaled[i:i+sequence_length])
    
    X_sequences = np.array(X_sequences)
    
    # PyTorch í…ì„œë¡œ ë³€í™˜
    X_tensor = torch.FloatTensor(X_sequences).to(device)
    
    # LSTM ì˜ˆì¸¡
    lstm_model.eval()
    with torch.no_grad():
        lstm_predictions_scaled = lstm_model(X_tensor).cpu().numpy()
    
    lstm_predictions = scaler_y.inverse_transform(lstm_predictions_scaled)
    
    # GRU ì˜ˆì¸¡
    gru_model.eval()
    with torch.no_grad():
        gru_predictions_scaled = gru_model(X_tensor).cpu().numpy()
    
    gru_predictions = scaler_y.inverse_transform(gru_predictions_scaled)
    
    # ìŠ¤íƒœí‚¹ ëª¨ë¸ìš© íŠ¹ì„± ìƒì„±
    X_stacked = np.hstack([
        lstm_predictions.reshape(-1, 1),
        gru_predictions.reshape(-1, 1)
    ])
    
    # XGBoost ìŠ¤íƒœí‚¹ ì˜ˆì¸¡
    stacked_predictions = xgb_model.predict(X_stacked).reshape(-1, 1)
    
    return {
        'lstm_predictions': lstm_predictions.flatten(),
        'gru_predictions': gru_predictions.flatten(),
        'stacked_predictions': stacked_predictions.flatten(),
        'n_predictions': len(stacked_predictions)
    }


# === í˜„ì¬ ì‹œì  ì˜ˆì¸¡ í•¨ìˆ˜ ===
def predict_current_hour(current_data, models_dict, current_datetime, sequence_length=24):
    """
    í˜„ì¬ ì‹œì ì˜ íƒœì–‘ê´‘ ë°œì „ëŸ‰ ì˜ˆì¸¡
    
    Args:
        current_data: í˜„ì¬ ì‹œì ê¹Œì§€ì˜ ë°ì´í„°
        models_dict: ë¡œë“œëœ ëª¨ë¸ ë”•ì…”ë„ˆë¦¬
        current_datetime: í˜„ì¬ ì‹œì 
        sequence_length: ì‹œí€€ìŠ¤ ê¸¸ì´
    
    Returns:
        dict: í˜„ì¬ ì‹œì  ì˜ˆì¸¡ ê²°ê³¼
    """
    print(f"\n{'='*80}")
    print(f"âš¡ í˜„ì¬ ì‹œì  ì˜ˆì¸¡ ì¤‘... ({current_datetime.strftime('%Y-%m-%d %H:%M')})")
    print(f"{'='*80}")
    
    # ë§ˆì§€ë§‰ sequence_length ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í˜„ì¬ ì‹œì  ì˜ˆì¸¡
    if len(current_data) < sequence_length:
        raise ValueError(f"ìµœì†Œ {sequence_length}ì‹œê°„ì˜ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    predictions = predict_solar_generation(
        new_data=current_data.tail(sequence_length + 1),
        models_dict=models_dict,
        sequence_length=sequence_length
    )
    
    # ê°€ì¥ ë§ˆì§€ë§‰ ì˜ˆì¸¡ê°’ì´ í˜„ì¬ ì‹œì ì˜ ì˜ˆì¸¡
    current_prediction = {
        'datetime': current_datetime,
        'lstm': float(predictions['lstm_predictions'][-1]),
        'gru': float(predictions['gru_predictions'][-1]),
        'stacked': float(predictions['stacked_predictions'][-1])
    }
    
    print(f"\nğŸ“Š í˜„ì¬ ì‹œì  ì˜ˆì¸¡ ê²°ê³¼:")
    print(f"  â€¢ LSTM:    {current_prediction['lstm']:.2f} MWh")
    print(f"  â€¢ GRU:     {current_prediction['gru']:.2f} MWh")
    print(f"  â€¢ Stacked: {current_prediction['stacked']:.2f} MWh")
    
    return current_prediction


# === Nì‹œê°„ ì´í›„ ì˜ˆì¸¡ í•¨ìˆ˜ ===
def predict_n_hours_ahead(current_data, models_dict, hours_ahead=24, sequence_length=24):
    """Nì‹œê°„ ì´í›„ì˜ íƒœì–‘ê´‘ ë°œì „ëŸ‰ ì˜ˆì¸¡"""
    print(f"\n{'='*80}")
    print(f"ğŸ”® {hours_ahead}ì‹œê°„ ì´í›„ ì˜ˆì¸¡ ìˆ˜í–‰ ì¤‘...")
    print(f"{'='*80}")
    
    required_length = sequence_length + hours_ahead
    if len(current_data) < required_length:
        raise ValueError(f"ìµœì†Œ {required_length}ì‹œê°„ì˜ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤. (í˜„ì¬: {len(current_data)}ì‹œê°„)")
    
    # ê°€ì¥ ìµœê·¼ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡
    predictions = predict_solar_generation(
        new_data=current_data.tail(required_length),
        models_dict=models_dict,
        sequence_length=sequence_length
    )
    
    # ë§ˆì§€ë§‰ Nê°œ ì˜ˆì¸¡ê°’ ì¶”ì¶œ (Nì‹œê°„ í›„ ì˜ˆì¸¡)
    future_predictions = {
        'lstm': predictions['lstm_predictions'][-hours_ahead:],
        'gru': predictions['gru_predictions'][-hours_ahead:],
        'stacked': predictions['stacked_predictions'][-hours_ahead:],
        'hours_ahead': hours_ahead
    }
    
    print(f"âœ… {hours_ahead}ì‹œê°„ ì´í›„ ì˜ˆì¸¡ ì™„ë£Œ ({len(future_predictions['stacked'])}ì‹œê°„)")
    
    return future_predictions


# === ë‹¤ì¤‘ ì‹œê°„ëŒ€ ì˜ˆì¸¡ ë° ì €ì¥ ===
def predict_multiple_horizons_realtime(current_data, models_dict, current_datetime, 
                                       output_dir='./prediction_results', sequence_length=24):
    """
    í˜„ì¬ ì‹œì  + 24H, 48H, 72H ì´í›„ì˜ íƒœì–‘ê´‘ ë°œì „ëŸ‰ ì˜ˆì¸¡ ë° CSV íŒŒì¼ë¡œ ì €ì¥
    """
    print(f"\n{'='*80}")
    print(f"ğŸ“Š ì‹¤ì‹œê°„ ë‹¤ì¤‘ ì‹œê°„ëŒ€ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ì‹œì‘")
    print(f"   ê¸°ì¤€ ì‹œê°: {current_datetime.strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ')}")
    print(f"{'='*80}")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dir, exist_ok=True)
    
    results = {}
    
    # 1. í˜„ì¬ ì‹œì  ì˜ˆì¸¡
    print(f"\n{'â”€'*80}")
    print(f"âš¡ í˜„ì¬ ì‹œì  ì˜ˆì¸¡")
    print(f"{'â”€'*80}")
    
    current_pred = predict_current_hour(current_data, models_dict, current_datetime, sequence_length)
    
    # í˜„ì¬ ì‹œì  ê²°ê³¼ ì €ì¥
    current_df = pd.DataFrame([{
        'ì˜ˆì¸¡ì¼ì‹œ': current_datetime,
        'ì˜ˆì¸¡ì‹œê°„': 'í˜„ì¬',
        'LSTM_ì˜ˆì¸¡(MWh)': current_pred['lstm'],
        'GRU_ì˜ˆì¸¡(MWh)': current_pred['gru'],
        'Stacked_ì˜ˆì¸¡(MWh)': current_pred['stacked']
    }])
    
    current_csv = os.path.join(output_dir, 'prediction_current.csv')
    current_df.to_csv(current_csv, index=False, encoding='utf-8-sig')
    print(f"  ğŸ’¾ í˜„ì¬ ì‹œì  ì˜ˆì¸¡ ì €ì¥: {current_csv}")
    
    results['current'] = {
        'dataframe': current_df,
        'csv_path': current_csv,
        'prediction': current_pred
    }
    
    # 2. 24H, 48H, 72H ì˜ˆì¸¡ ìˆ˜í–‰
    for hours in [24, 48, 72]:
        print(f"\n{'â”€'*80}")
        print(f"ğŸ”® {hours}ì‹œê°„ í›„ ì˜ˆì¸¡ ìˆ˜í–‰")
        print(f"{'â”€'*80}")
        
        try:
            # ì˜ˆì¸¡ ìˆ˜í–‰
            predictions = predict_n_hours_ahead(
                current_data=current_data,
                models_dict=models_dict,
                hours_ahead=hours,
                sequence_length=sequence_length
            )
            
            # ì‹œê°„ ì •ë³´ ìƒì„±
            time_labels = []
            datetime_labels = []
            for i in range(hours):
                time_labels.append(f'+{i+1}ì‹œê°„')
                datetime_labels.append(current_datetime + timedelta(hours=i+1))
            
            # DataFrame ìƒì„±
            df = pd.DataFrame({
                'ì˜ˆì¸¡ì‹œê°„': time_labels,
                'ì˜ˆì¸¡ì¼ì‹œ': datetime_labels,
                'LSTM_ì˜ˆì¸¡(MWh)': predictions['lstm'],
                'GRU_ì˜ˆì¸¡(MWh)': predictions['gru'],
                'Stacked_ì˜ˆì¸¡(MWh)': predictions['stacked']
            })
            
            # ëˆ„ì  ë°œì „ëŸ‰ ê³„ì‚°
            df['LSTM_ëˆ„ì (MWh)'] = df['LSTM_ì˜ˆì¸¡(MWh)'].cumsum()
            df['GRU_ëˆ„ì (MWh)'] = df['GRU_ì˜ˆì¸¡(MWh)'].cumsum()
            df['Stacked_ëˆ„ì (MWh)'] = df['Stacked_ì˜ˆì¸¡(MWh)'].cumsum()
            
            # CSV íŒŒì¼ë¡œ ì €ì¥
            csv_filename = f'prediction_{hours}H_{current_datetime.strftime("%Y%m%d_%H%M")}.csv'
            csv_path = os.path.join(output_dir, csv_filename)
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            
            print(f"  âœ… {hours}ì‹œê°„ ì˜ˆì¸¡ ì™„ë£Œ")
            print(f"  ğŸ’¾ íŒŒì¼ ì €ì¥: {csv_path}")
            print(f"  ğŸ“ˆ ì´ ì˜ˆì¸¡ ë°œì „ëŸ‰ (Stacked): {predictions['stacked'].sum():.2f} MWh")
            print(f"  ğŸ“Š ì‹œê°„ë‹¹ í‰ê·  (Stacked): {predictions['stacked'].mean():.2f} MWh")
            
            # ê²°ê³¼ ì €ì¥
            results[f'{hours}H'] = {
                'dataframe': df,
                'csv_path': csv_path,
                'summary': {
                    'lstm_total': float(predictions['lstm'].sum()),
                    'gru_total': float(predictions['gru'].sum()),
                    'stacked_total': float(predictions['stacked'].sum()),
                    'lstm_mean': float(predictions['lstm'].mean()),
                    'gru_mean': float(predictions['gru'].mean()),
                    'stacked_mean': float(predictions['stacked'].mean()),
                    'lstm_max': float(predictions['lstm'].max()),
                    'gru_max': float(predictions['gru'].max()),
                    'stacked_max': float(predictions['stacked'].max()),
                }
            }
            
        except Exception as e:
            print(f"  âŒ {hours}ì‹œê°„ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            results[f'{hours}H'] = None
    
    # 3. í†µí•© ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
    print(f"\n{'='*80}")
    print("ğŸ“‹ í†µí•© ì˜ˆì¸¡ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±")
    print(f"{'='*80}")
    
    summary_data = [{
        'ì˜ˆì¸¡êµ¬ê°„': 'í˜„ì¬',
        'LSTM_ë°œì „ëŸ‰(MWh)': current_pred['lstm'],
        'GRU_ë°œì „ëŸ‰(MWh)': current_pred['gru'],
        'Stacked_ë°œì „ëŸ‰(MWh)': current_pred['stacked'],
    }]
    
    for hours in [24, 48, 72]:
        if results[f'{hours}H'] is not None:
            summary = results[f'{hours}H']['summary']
            summary_data.append({
                'ì˜ˆì¸¡êµ¬ê°„': f'{hours}ì‹œê°„',
                'LSTM_ë°œì „ëŸ‰(MWh)': summary['lstm_total'],
                'GRU_ë°œì „ëŸ‰(MWh)': summary['gru_total'],
                'Stacked_ë°œì „ëŸ‰(MWh)': summary['stacked_total'],
            })
    
    summary_df = pd.DataFrame(summary_data)
    summary_path = os.path.join(output_dir, f'prediction_summary_{current_datetime.strftime("%Y%m%d_%H%M")}.csv')
    summary_df.to_csv(summary_path, index=False, encoding='utf-8-sig')
    
    print(f"  ğŸ’¾ í†µí•© ìš”ì•½ ì €ì¥: {summary_path}")
    
    # ì½˜ì†” ì¶œë ¥
    print(f"\n{'='*80}")
    print("ğŸ“Š ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ ìš”ì•½")
    print(f"{'='*80}")
    print(summary_df.to_string(index=False))
    
    print(f"\n{'='*80}")
    print(f"âœ¨ ëª¨ë“  ì˜ˆì¸¡ ê²°ê³¼ê°€ '{output_dir}' ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"{'='*80}")
    print(f"\nì €ì¥ëœ íŒŒì¼:")
    print(f"  ğŸ“„ {current_csv}")
    for hours in [24, 48, 72]:
        if results[f'{hours}H'] is not None:
            print(f"  ğŸ“„ {results[f'{hours}H']['csv_path']}")
    print(f"  ğŸ“„ {summary_path}")
    
    return results


# === ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ===
if __name__ == "__main__":
    try:
        # í˜„ì¬ ë‚ ì§œ ë° ì‹œê°„ ìë™ ì„¤ì • (2025ë…„ 11ì›” 10ì¼)
        CURRENT_DATETIME = datetime(2025, 11, 10, datetime.now().hour)
        
        print("\n" + "="*80)
        print("ğŸš€ ì‹¤ì‹œê°„ íƒœì–‘ê´‘ ë°œì „ëŸ‰ ì˜ˆì¸¡ ì‹œìŠ¤í…œ")
        print(f"   ğŸ“… ê¸°ì¤€ ì‹œê°: {CURRENT_DATETIME.strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ')}")
        print("="*80)
        
        # 1. ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ
        models = load_trained_models(model_dir='./saved_models')
        
        # 2. í˜„ì¬ ì‹œì ê¹Œì§€ì˜ ë°ì´í„° ì¤€ë¹„
        data_path = "./dataset/jeju_solar_utf8.csv"
        
        current_data = prepare_current_data(
            data_path=data_path,
            current_datetime=CURRENT_DATETIME,
            models_dict=models,
            hours_needed=96  # 72ì‹œê°„ ì˜ˆì¸¡ + 24ì‹œê°„ ì‹œí€€ìŠ¤
        )
        
        # 3. í˜„ì¬ ì‹œì  + 24H/48H/72H ì˜ˆì¸¡ ìˆ˜í–‰ ë° ì €ì¥
        results = predict_multiple_horizons_realtime(
            current_data=current_data,
            models_dict=models,
            current_datetime=CURRENT_DATETIME,
            output_dir='./prediction_results',
            sequence_length=24
        )
        
        # 4. ìƒì„¸ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
        print(f"\n{'='*80}")
        print("ğŸ“‹ ìƒì„¸ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°")
        print(f"{'='*80}")
        
        # í˜„ì¬ ì‹œì  ê²°ê³¼
        print(f"\nâš¡ í˜„ì¬ ì‹œì  ({CURRENT_DATETIME.strftime('%Y-%m-%d %H:00')}):")
        print(results['current']['dataframe'].to_string(index=False))
        
        # ê° ì‹œê°„ëŒ€ ì˜ˆì¸¡ ê²°ê³¼ (ì²˜ìŒ 5ê°œì™€ ë§ˆì§€ë§‰ 5ê°œ)
        for hours in [24, 48, 72]:
            if results[f'{hours}H'] is not None:
                print(f"\nğŸ”® {hours}ì‹œê°„ í›„ ì˜ˆì¸¡ (ì²˜ìŒ 5ì‹œê°„):")
                print(results[f'{hours}H']['dataframe'].head().to_string(index=False))
                print(f"\nğŸ”® {hours}ì‹œê°„ í›„ ì˜ˆì¸¡ (ë§ˆì§€ë§‰ 5ì‹œê°„):")
                print(results[f'{hours}H']['dataframe'].tail().to_string(index=False))
        
        print(f"\n{'='*80}")
        print("âœ… ì˜ˆì¸¡ ì™„ë£Œ!")
        print(f"{'='*80}")
        
    except FileNotFoundError as e:
        print(f"\nâŒ ì˜¤ë¥˜: {e}")
        print("ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()