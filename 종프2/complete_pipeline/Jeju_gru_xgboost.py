import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F 
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import os
import time
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.cluster import KMeans
from sklearn.impute import SimpleImputer
import xgboost as xgb
from xgboost import plot_importance

# GPU/CUDA ì„¤ì •
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸš€ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")


# ë°ì´í„° ê²½ë¡œ
data_path = "./dataset/jeju_solar_utf8.csv"
warnings.filterwarnings("ignore")

plt.style.use('seaborn-v0_8-whitegrid')
# í•œê¸€ í°íŠ¸ ì„¤ì • (Windows ì‚¬ìš©ììš©)
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€


def calculate_rmse(y_true, y_pred):
    """RMSE ê³„ì‚°"""
    return np.sqrt(mean_squared_error(y_true, y_pred))


def calculate_r2(y_true, y_pred):
    """RÂ² ê²°ì •ê³„ìˆ˜ ê³„ì‚°"""
    return r2_score(y_true, y_pred)


def calculate_mape(y_true, y_pred, method='improved'):
    """
    ê°œì„ ëœ MAPE ê³„ì‚° - íƒœì–‘ê´‘ ë°œì „ëŸ‰ íŠ¹ì„±ì„ ê³ ë ¤í•œ ì—¬ëŸ¬ ë°©ë²• ì œê³µ
    
    Args:
        y_true: ì‹¤ì œê°’
        y_pred: ì˜ˆì¸¡ê°’
        method: ê³„ì‚° ë°©ë²•
            - 'improved': ê°œì„ ëœ MAPE (ê¸°ë³¸ê°’)
            - 'threshold': ì„ê³„ê°’ ê¸°ë°˜ MAPE
            - 'weighted': ê°€ì¤‘ MAPE
            - 'symmetric': ëŒ€ì¹­ MAPE
    """
    y_true = np.array(y_true, dtype=np.float64)
    y_pred = np.array(y_pred, dtype=np.float64)
    
    # NaN ë° ë¬´í•œê°’ í™•ì¸ ë° ì œê±°
    valid_mask = np.isfinite(y_true) & np.isfinite(y_pred)
    y_true_clean = y_true[valid_mask]
    y_pred_clean = y_pred[valid_mask]
    
    if len(y_true_clean) == 0:
        print("Warning: ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ì–´ MAPEë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return np.nan
    
    if method == 'improved':
        """
        ê°œì„ ëœ MAPE: ì‘ì€ ê°’ë“¤ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ì—¬ 
        ì „ì²´ì ì¸ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë” ì˜ ë°˜ì˜
        """
        # ì „ì²´ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ê³ ë ¤í•œ ì„ê³„ê°’ ì„¤ì •
        threshold = np.percentile(y_true_clean, 10)  # í•˜ìœ„ 10% ê°’ì„ ì„ê³„ê°’ìœ¼ë¡œ ì‚¬ìš©
        
        # ì„ê³„ê°’ ì´ìƒì¸ ë°ì´í„°ì— ëŒ€í•´ì„œë§Œ MAPE ê³„ì‚°
        significant_mask = y_true_clean >= threshold
        
        if not np.any(significant_mask):
            # ëª¨ë“  ê°’ì´ ì„ê³„ê°’ ë¯¸ë§Œì¸ ê²½ìš°, ì ˆëŒ€ ì˜¤ì°¨ ê¸°ë°˜ ê³„ì‚°
            abs_errors = np.abs(y_true_clean - y_pred_clean)
            mean_actual = np.mean(y_true_clean)
            if mean_actual > 0:
                return (np.mean(abs_errors) / mean_actual) * 100
            else:
                return 0.0
        
        y_true_sig = y_true_clean[significant_mask]
        y_pred_sig = y_pred_clean[significant_mask]
        
        # ê°€ì¤‘ í‰ê·  MAPE ê³„ì‚°
        weights = y_true_sig / np.sum(y_true_sig)  # ì‹¤ì œê°’ì— ë¹„ë¡€í•œ ê°€ì¤‘ì¹˜
        percentage_errors = np.abs((y_true_sig - y_pred_sig) / y_true_sig)
        
        # ê·¹ë‹¨ì ì¸ ì˜¤ì°¨ ì œí•œ
        percentage_errors = np.clip(percentage_errors, 0, 2)  # ìµœëŒ€ 200% ì˜¤ì°¨ë¡œ ì œí•œ
        
        mape_value = np.sum(weights * percentage_errors) * 100
        
        # ì œê±°ëœ ë°ì´í„° ë¹„ìœ¨ ì¶œë ¥
        removed_count = len(y_true_clean) - len(y_true_sig)
        if removed_count > 0:
            removal_rate = (removed_count / len(y_true_clean)) * 100
            print(f"MAPE ê³„ì‚° ì‹œ ì‘ì€ ê°’ ì œì™¸: {removed_count}ê°œ ({removal_rate:.1f}%)")
    
    elif method == 'threshold':
        """
        ì„ê³„ê°’ ê¸°ë°˜ MAPE: ì¼ì • ê°’ ì´ìƒì˜ ë°ì´í„°ë§Œ ì‚¬ìš©
        """
        # ë™ì  ì„ê³„ê°’ ê³„ì‚° (í‰ê· ì˜ 10%)
        threshold = np.mean(y_true_clean) * 0.1
        
        above_threshold = y_true_clean > threshold
        
        if not np.any(above_threshold):
            return 0.0
            
        y_true_filtered = y_true_clean[above_threshold]
        y_pred_filtered = y_pred_clean[above_threshold]
        
        percentage_errors = np.abs((y_true_filtered - y_pred_filtered) / y_true_filtered)
        percentage_errors = np.clip(percentage_errors, 0, 1.5)  # 150% ì œí•œ
        
        mape_value = np.mean(percentage_errors) * 100
        
        removed_count = len(y_true_clean) - len(y_true_filtered)
        print(f"ì„ê³„ê°’({threshold:.3f}) ë¯¸ë§Œ ì œì™¸: {removed_count}ê°œ")
    
    elif method == 'weighted':
        """
        ê°€ì¤‘ MAPE: ê°’ì˜ í¬ê¸°ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ë¶€ì—¬
        """
        # 0ì— ê°€ê¹Œìš´ ê°’ ì œì™¸
        non_zero_mask = y_true_clean > np.percentile(y_true_clean, 5)
        
        if not np.any(non_zero_mask):
            return 0.0
            
        y_true_nz = y_true_clean[non_zero_mask]
        y_pred_nz = y_pred_clean[non_zero_mask]
        
        # ì‹¤ì œê°’ì˜ í¬ê¸°ì— ë¹„ë¡€í•œ ê°€ì¤‘ì¹˜
        weights = y_true_nz / np.sum(y_true_nz)
        
        percentage_errors = np.abs((y_true_nz - y_pred_nz) / y_true_nz)
        percentage_errors = np.clip(percentage_errors, 0, 1.0)  # 100% ì œí•œ
        
        mape_value = np.sum(weights * percentage_errors) * 100
    
    elif method == 'symmetric':
        """
        ëŒ€ì¹­ MAPE (SMAPE): ë¶„ëª¨ì— ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ í‰ê·  ì‚¬ìš©
        """
        # ë§¤ìš° ì‘ì€ ê°’ë“¤ ì œì™¸
        min_threshold = np.percentile(np.abs(y_true_clean), 5)
        valid_mask = np.abs(y_true_clean) > min_threshold
        
        if not np.any(valid_mask):
            return 0.0
            
        y_true_filtered = y_true_clean[valid_mask]
        y_pred_filtered = y_pred_clean[valid_mask]
        
        denominator = (np.abs(y_true_filtered) + np.abs(y_pred_filtered)) / 2
        percentage_errors = np.abs(y_true_filtered - y_pred_filtered) / denominator
        percentage_errors = np.clip(percentage_errors, 0, 1.0)  # 100% ì œí•œ
        
        mape_value = np.mean(percentage_errors) * 100
    
    else:
        raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” MAPE ê³„ì‚° ë°©ë²•ì…ë‹ˆë‹¤.")
    
    return mape_value


def calculate_normalized_mape(y_true, y_pred):
    """
    ì •ê·œí™”ëœ MAPE: ë°ì´í„° ë²”ìœ„ì— ë”°ë¼ ì •ê·œí™”
    """
    y_true = np.array(y_true, dtype=np.float64)
    y_pred = np.array(y_pred, dtype=np.float64)
    
    # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì„ íƒ
    valid_mask = np.isfinite(y_true) & np.isfinite(y_pred) & (y_true > 0)
    y_true_valid = y_true[valid_mask]
    y_pred_valid = y_pred[valid_mask]
    
    if len(y_true_valid) == 0:
        return 0.0
    
    # ë°ì´í„° ë²”ìœ„ ê³„ì‚°
    data_range = np.max(y_true_valid) - np.min(y_true_valid)
    
    if data_range == 0:
        return 0.0
    
    # ì ˆëŒ€ ì˜¤ì°¨ë¥¼ ë°ì´í„° ë²”ìœ„ë¡œ ì •ê·œí™”
    absolute_errors = np.abs(y_true_valid - y_pred_valid)
    normalized_errors = absolute_errors / data_range
    
    # ë°±ë¶„ìœ¨ë¡œ ë³€í™˜
    return np.mean(normalized_errors) * 100


def calculate_all_metrics(y_true, y_pred, print_details=False):
    """
    ëª¨ë“  ì„±ëŠ¥ ì§€í‘œë¥¼ ì¼ê´„ ê³„ì‚°
    """
    mae = mean_absolute_error(y_true, y_pred)
    rmse = calculate_rmse(y_true, y_pred)
    r2 = calculate_r2(y_true, y_pred)
    
    # ë‹¤ì–‘í•œ MAPE ê³„ì‚°
    mape_improved = calculate_mape(y_true, y_pred, method='improved')
    mape_threshold = calculate_mape(y_true, y_pred, method='threshold')
    mape_weighted = calculate_mape(y_true, y_pred, method='weighted')
    mape_symmetric = calculate_mape(y_true, y_pred, method='symmetric')
    mape_normalized = calculate_normalized_mape(y_true, y_pred)
    
    if print_details:
        print(f"MAE: {mae:.4f}")
        print(f"RMSE: {rmse:.4f}")
        print(f"RÂ²: {r2:.4f}")
        print(f"MAPE (ê°œì„ ë¨): {mape_improved:.2f}%")
        print(f"MAPE (ì„ê³„ê°’): {mape_threshold:.2f}%")
        print(f"MAPE (ê°€ì¤‘): {mape_weighted:.2f}%")
        print(f"MAPE (ëŒ€ì¹­): {mape_symmetric:.2f}%")
        print(f"MAPE (ì •ê·œí™”): {mape_normalized:.2f}%")
        
        # ì¶”ì²œ MAPE ì„ íƒ
        print(f"\nì¶”ì²œ MAPE (ê°œì„ ë¨): {mape_improved:.2f}%")
    
    return {
        'mae': mae,
        'rmse': rmse,
        'r2': r2,
        'mape_improved': mape_improved,
        'mape_threshold': mape_threshold,
        'mape_weighted': mape_weighted,
        'mape_symmetric': mape_symmetric,
        'mape_normalized': mape_normalized
    }


def preprocess_data(data_df):
    """
    ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ - ê²°ì¸¡ê°’ ì²˜ë¦¬ ë° ë°ì´í„° í´ë¦¬ë‹
    """
    print("ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
    
    # ë°ì´í„° ì •ë³´ ì¶œë ¥
    print(f"ì›ë³¸ ë°ì´í„° í¬ê¸°: {data_df.shape}")
    print(f"ê²°ì¸¡ê°’ ê°œìˆ˜:")
    missing_counts = data_df.isnull().sum()
    for col, count in missing_counts.items():
        if count > 0:
            print(f"  {col}: {count} ({count/len(data_df)*100:.1f}%)")
    
    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
    required_columns = ["ê¸°ì˜¨", "ê°•ìˆ˜ëŸ‰(mm)", "ì¼ì¡°(hr)", "ì¼ì‚¬ëŸ‰", "íƒœì–‘ê´‘ ë°œì „ëŸ‰(MWh)"]
    
    # ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    missing_cols = [col for col in required_columns if col not in data_df.columns]
    if missing_cols:
        print(f"ëˆ„ë½ëœ ì»¬ëŸ¼: {missing_cols}")
        return None, None
    
    # ë°ì´í„° ì¶”ì¶œ
    processed_df = data_df[required_columns].copy()
    
    # ê²°ì¸¡ê°’ ì²˜ë¦¬
    # 1. ê¸°ì˜¨: ì „í›„ ê°’ì˜ í‰ê· ìœ¼ë¡œ ë³´ê°„
    processed_df['ê¸°ì˜¨'] = processed_df['ê¸°ì˜¨'].interpolate(method='linear')
    
    # 2. ê°•ìˆ˜ëŸ‰: 0ìœ¼ë¡œ ì±„ì›€ (ë¹„ê°€ ì•ˆ ì˜¨ ê²ƒìœ¼ë¡œ ê°€ì •)
    processed_df['ê°•ìˆ˜ëŸ‰(mm)'] = processed_df['ê°•ìˆ˜ëŸ‰(mm)'].fillna(0)
    
    # 3. ì¼ì¡°ì™€ ì¼ì‚¬ëŸ‰: ê³„ì ˆì„±ê³¼ ì‹œê°„ëŒ€ë¥¼ ê³ ë ¤í•œ ë³´ê°„
    if 'ì¼ì‹œ' in data_df.columns:
        # ë‚ ì§œ ì •ë³´ê°€ ìˆë‹¤ë©´ ì‹œê°„ëŒ€ë³„ í‰ê· ìœ¼ë¡œ ë³´ê°„
        data_df['datetime'] = pd.to_datetime(data_df['ì¼ì‹œ'])
        data_df['hour'] = data_df['datetime'].dt.hour
        data_df['month'] = data_df['datetime'].dt.month
        
        # ì‹œê°„ëŒ€ë³„, ì›”ë³„ í‰ê· ê°’ìœ¼ë¡œ ê²°ì¸¡ê°’ ë³´ê°„
        for col in ['ì¼ì¡°(hr)', 'ì¼ì‚¬ëŸ‰']:
            if col in processed_df.columns:
                # ë¨¼ì € ì‹œê°„ëŒ€ë³„ í‰ê· ìœ¼ë¡œ ë³´ê°„
                hourly_mean = data_df.groupby('hour')[col].transform('mean')
                processed_df[col] = processed_df[col].fillna(hourly_mean)
                
                # ì—¬ì „íˆ NaNì´ ìˆë‹¤ë©´ ì „ì²´ í‰ê· ìœ¼ë¡œ ë³´ê°„
                processed_df[col] = processed_df[col].fillna(processed_df[col].mean())
    else:
        # ë‚ ì§œ ì •ë³´ê°€ ì—†ë‹¤ë©´ ë‹¨ìˆœ ë³´ê°„
        processed_df['ì¼ì¡°(hr)'] = processed_df['ì¼ì¡°(hr)'].interpolate(method='linear')
        processed_df['ì¼ì‚¬ëŸ‰'] = processed_df['ì¼ì‚¬ëŸ‰'].interpolate(method='linear')
        
        # ì—¬ì „íˆ NaNì´ ìˆë‹¤ë©´ í‰ê· ê°’ìœ¼ë¡œ ì±„ì›€
        processed_df['ì¼ì¡°(hr)'] = processed_df['ì¼ì¡°(hr)'].fillna(processed_df['ì¼ì¡°(hr)'].mean())
        processed_df['ì¼ì‚¬ëŸ‰'] = processed_df['ì¼ì‚¬ëŸ‰'].fillna(processed_df['ì¼ì‚¬ëŸ‰'].mean())
    
    # 4. ë°œì „ëŸ‰: 0ìœ¼ë¡œ ì±„ì›€ (ë°œì „ì´ ì•ˆ ëœ ê²ƒìœ¼ë¡œ ê°€ì •)
    processed_df['íƒœì–‘ê´‘ ë°œì „ëŸ‰(MWh)'] = processed_df['íƒœì–‘ê´‘ ë°œì „ëŸ‰(MWh)'].fillna(0)
    
    # ì´ìƒê°’ ì œê±° (IQR ë°©ë²• ì‚¬ìš©)
    def remove_outliers(series, factor=1.5):
        Q1 = series.quantile(0.25)
        Q3 = series.quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - factor * IQR
        upper = Q3 + factor * IQR
        return series.clip(lower, upper)
    
    # ê¸°ì˜¨ê³¼ ë°œì „ëŸ‰ì— ëŒ€í•´ì„œë§Œ ì´ìƒê°’ ì²˜ë¦¬
    processed_df['ê¸°ì˜¨'] = remove_outliers(processed_df['ê¸°ì˜¨'])
    processed_df['íƒœì–‘ê´‘ ë°œì „ëŸ‰(MWh)'] = remove_outliers(processed_df['íƒœì–‘ê´‘ ë°œì „ëŸ‰(MWh)'])
    
    # ìµœì¢… ê²°ì¸¡ê°’ í™•ì¸
    final_missing = processed_df.isnull().sum()
    if final_missing.sum() > 0:
        print("ì „ì²˜ë¦¬ í›„ ë‚¨ì€ ê²°ì¸¡ê°’:")
        for col, count in final_missing.items():
            if count > 0:
                print(f"  {col}: {count}")
        
        # ë‚¨ì€ ê²°ì¸¡ê°’ì´ ìˆë‹¤ë©´ í•´ë‹¹ í–‰ ì œê±°
        processed_df = processed_df.dropna()
        print(f"ê²°ì¸¡ê°’ì´ ìˆëŠ” í–‰ ì œê±° í›„ ë°ì´í„° í¬ê¸°: {processed_df.shape}")
    
    # íŠ¹ì§•ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
    features = processed_df[["ê¸°ì˜¨", "ê°•ìˆ˜ëŸ‰(mm)", "ì¼ì¡°(hr)", "ì¼ì‚¬ëŸ‰"]].values
    targets = processed_df["íƒœì–‘ê´‘ ë°œì „ëŸ‰(MWh)"].values
    
    print(f"ì „ì²˜ë¦¬ ì™„ë£Œ - ìµœì¢… ë°ì´í„° í¬ê¸°: {len(features)} í–‰")
    print(f"íŠ¹ì§• ë°ì´í„° í˜•íƒœ: {features.shape}")
    print(f"íƒ€ê²Ÿ ë°ì´í„° í˜•íƒœ: {targets.shape}")
    
    return features, targets


class PatternExtractor:
    """
    ê¸°ìƒ ë°ì´í„°ì—ì„œ íƒœì–‘ê´‘ ë°œì „ëŸ‰ê³¼ ê´€ë ¨ëœ íŒ¨í„´ì„ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤
    """
    def __init__(self, n_patterns=5):
        self.n_patterns = n_patterns
        self.kmeans = None
        self.pattern_labels = None
        self.pattern_centers = None
        self.imputer = SimpleImputer(strategy='mean')  # ì¶”ê°€ ì•ˆì „ì¥ì¹˜
        
    def extract_weather_patterns(self, features, targets):
        """
        ê¸°ìƒ ì¡°ê±´ê³¼ ë°œì „ëŸ‰ì˜ ê´€ê³„ì—ì„œ íŒ¨í„´ ì¶”ì¶œ
        """
        # ì…ë ¥ ë°ì´í„° ê²€ì¦ ë° ê²°ì¸¡ê°’ ì²˜ë¦¬
        print("íŒ¨í„´ ì¶”ì¶œì„ ìœ„í•œ ë°ì´í„° ê²€ì¦ ì¤‘...")
        
        # NaN ì²´í¬
        if np.isnan(features).any():
            print("Warning: featuresì— NaN ê°’ì´ ë°œê²¬ë˜ì–´ imputerë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            features = self.imputer.fit_transform(features)
        
        if np.isnan(targets).any():
            print("Warning: targetsì— NaN ê°’ì´ ë°œê²¬ë˜ì–´ í‰ê· ê°’ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            targets = np.nan_to_num(targets, nan=np.nanmean(targets))
        
        # ë¬´í•œê°’ ì²´í¬ ë° ì²˜ë¦¬
        if np.isinf(features).any():
            print("Warning: featuresì— ë¬´í•œê°’ì´ ë°œê²¬ë˜ì–´ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            features = np.nan_to_num(features, posinf=np.nanmax(features[np.isfinite(features)]), 
                                     neginf=np.nanmin(features[np.isfinite(features)]))
        
        if np.isinf(targets).any():
            print("Warning: targetsì— ë¬´í•œê°’ì´ ë°œê²¬ë˜ì–´ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            targets = np.nan_to_num(targets, posinf=np.nanmax(targets[np.isfinite(targets)]), 
                                     neginf=np.nanmin(targets[np.isfinite(targets)]))
        
        # ê¸°ìƒ ë°ì´í„°ì™€ ë°œì „ëŸ‰ì„ ê²°í•©í•˜ì—¬ íŒ¨í„´ ë¶„ì„
        combined_data = np.column_stack([features, targets.reshape(-1, 1)])
        
        # ë°ì´í„° ì •ê·œí™” (í´ëŸ¬ìŠ¤í„°ë§ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´)
        scaler = StandardScaler()
        combined_data_scaled = scaler.fit_transform(combined_data)
        
        # K-means í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ íŒ¨í„´ ì¶”ì¶œ
        self.kmeans = KMeans(n_clusters=self.n_patterns, random_state=42, n_init=10)
        self.pattern_labels = self.kmeans.fit_predict(combined_data_scaled)
        
        # ì›ë³¸ ìŠ¤ì¼€ì¼ì˜ ì„¼í„° ê³„ì‚° (í•´ì„ì„ ìœ„í•´)
        self.pattern_centers = []
        for i in range(self.n_patterns):
            mask = self.pattern_labels == i
            if mask.sum() > 0:
                center = combined_data[mask].mean(axis=0)
                self.pattern_centers.append(center)
            else:
                # ë¹ˆ í´ëŸ¬ìŠ¤í„°ì¸ ê²½ìš° ì „ì²´ í‰ê·  ì‚¬ìš©
                self.pattern_centers.append(combined_data.mean(axis=0))
        
        self.pattern_centers = np.array(self.pattern_centers)
        
        print(f"ì¶”ì¶œëœ íŒ¨í„´ ìˆ˜: {self.n_patterns}")
        self._analyze_patterns()
        
        return self.pattern_labels
    
    def _analyze_patterns(self):
        """
        ì¶”ì¶œëœ íŒ¨í„´ ë¶„ì„ ë° ì¶œë ¥
        """
        print("\n=== íŒ¨í„´ ë¶„ì„ ê²°ê³¼ ===")
        pattern_names = ["ì €ë°œì „", "ì €ì¤‘ë°œì „", "ì¤‘ë°œì „", "ì¤‘ê³ ë°œì „", "ê³ ë°œì „"]
        
        # ë°œì „ëŸ‰ ê¸°ì¤€ìœ¼ë¡œ íŒ¨í„´ ì •ë ¬
        pattern_power = [(i, center[4]) for i, center in enumerate(self.pattern_centers)]
        pattern_power.sort(key=lambda x: x[1])
        
        for idx, (pattern_idx, power) in enumerate(pattern_power):
            center = self.pattern_centers[pattern_idx]
            pattern_name = pattern_names[idx] if idx < len(pattern_names) else f"íŒ¨í„´{idx}"
            count = (self.pattern_labels == pattern_idx).sum()
            
            print(f"íŒ¨í„´ {pattern_idx+1} ({pattern_name}) - ë°ì´í„° ìˆ˜: {count}")
            print(f"  - í‰ê·  ê¸°ì˜¨: {center[0]:.2f}Â°C")
            print(f"  - í‰ê·  ê°•ìˆ˜ëŸ‰: {center[1]:.2f}mm")
            print(f"  - í‰ê·  ì¼ì¡°ì‹œê°„: {center[2]:.2f}hr")
            print(f"  - í‰ê·  ì¼ì‚¬ëŸ‰: {center[3]:.2f}")
            print(f"  - í‰ê·  ë°œì „ëŸ‰: {center[4]:.2f}MWh")
            print()
    
    def get_pattern_features(self, features):
        """
        ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ íŒ¨í„´ ë ˆì´ë¸” ì˜ˆì¸¡
        """
        if self.kmeans is None:
            raise ValueError("ë¨¼ì € extract_weather_patternsë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        
        # NaN ì²˜ë¦¬
        if np.isnan(features).any():
            features = self.imputer.transform(features)
        
        # ë°œì „ëŸ‰ì´ ì—†ëŠ” ê²½ìš°, ê¸°ìƒ ë°ì´í„°ë§Œìœ¼ë¡œ íŒ¨í„´ ì˜ˆì¸¡
        dummy_targets = np.zeros((len(features), 1))
        combined_data = np.column_stack([features, dummy_targets])
        
        return self.kmeans.predict(combined_data)


class SolarPatternDataset(Dataset):
    """
    íŒ¨í„´ ì •ë³´ë¥¼ í¬í•¨í•œ íƒœì–‘ê´‘ ë°œì „ ë°ì´í„°ì…‹
    """
    def __init__(self, features, targets, patterns, seq_len=24):
        self.X, self.y, self.patterns = [], [], []
        
        # patternsëŠ” 1D ë°°ì—´ (ì •ìˆ˜ ë ˆì´ë¸”) ì´ë¼ê³  ê°€ì •
        patterns = np.array(patterns).astype(np.int64)
        
        # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± (íŒ¨í„´ ì •ë³´ í¬í•¨)
        for i in range(len(features) - seq_len):
            self.X.append(features[i:i+seq_len])
            self.y.append(targets[i+seq_len])
            self.patterns.append(patterns[i:i+seq_len])  # íŒ¨í„´ ì‹œí€€ìŠ¤

        self.X = np.array(self.X, dtype=np.float32)      # (N, seq_len, feature_dim)
        self.y = np.array(self.y, dtype=np.float32)
        self.patterns = np.array(self.patterns, dtype=np.int64)  # ì •ìˆ˜ë¡œ ì €ì¥

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        # ê¸°ìƒ ë°ì´í„°ì™€ íŒ¨í„´ ì •ë³´ë¥¼ ê²°í•©
        # íŒ¨í„´ì€ ì •ìˆ˜ì´ë¯€ë¡œ floatë¡œ ë³€í™˜í•˜ì—¬ concat
        pattern_seq = self.patterns[idx].reshape(-1, 1).astype(np.float32)
        features_with_patterns = np.concatenate([
            self.X[idx], 
            pattern_seq
        ], axis=1)
        
        return torch.tensor(features_with_patterns), torch.tensor(self.y[idx])




def create_pattern_features(features, targets, pattern_extractor):
    """
    íŒ¨í„´ ê¸°ë°˜ íŠ¹ì§• ìƒì„±
    """
    # ê¸°ë³¸ íŒ¨í„´ ì¶”ì¶œ
    patterns = pattern_extractor.extract_weather_patterns(features, targets)
    
    # ì¶”ê°€ íŒ¨í„´ ê¸°ë°˜ íŠ¹ì§• ìƒì„±
    pattern_features = []
    
    for i in range(len(features)):
        pattern_feat = [
            patterns[i],  # í˜„ì¬ íŒ¨í„´
            # ê³„ì ˆì„± íŠ¹ì§•
            np.sin(2 * np.pi * i / 365),  # ì—°ê°„ ì£¼ê¸°
            np.cos(2 * np.pi * i / 365),
            np.sin(2 * np.pi * i / 24),   # ì¼ê°„ ì£¼ê¸° (ê°€ì •)
            np.cos(2 * np.pi * i / 24),
            # ê¸°ìƒ ì¡°ê±´ ì¡°í•© íŠ¹ì§•
            features[i][0] * features[i][3],  # ê¸°ì˜¨ * ì¼ì‚¬ëŸ‰
            features[i][2] * features[i][3],  # ì¼ì¡° * ì¼ì‚¬ëŸ‰
            # ê°•ìˆ˜ëŸ‰ì´ ë°œì „ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
            1 if features[i][1] < 0.1 else 0,  # ë¬´ê°•ìˆ˜ ì—¬ë¶€
        ]
        pattern_features.append(pattern_feat)
    
    return np.array(pattern_features)


def xgb_stacking_model(X_train, y_train, X_val, y_val, X_test, y_test, plotting=False):
    """
    LSTM ì˜ˆì¸¡ì„ í¬í•¨í•œ ìŠ¤íƒœí‚¹ XGBoost ëª¨ë¸
    """
    # XGBoost ëª¨ë¸ ì •ì˜ (í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹)
    xgb_regressor = xgb.XGBRegressor(
        gamma=0.5, 
        n_estimators=300, 
        learning_rate=0.08, 
        max_depth=6,
        min_child_weight=3,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        early_stopping_rounds=50,
        reg_alpha=0.1,
        reg_lambda=0.1
    )
    
    print("XGBoost ìŠ¤íƒœí‚¹ ëª¨ë¸ í•™ìŠµ ì¤‘...")
    # ëª¨ë¸ í•™ìŠµ
    xgb_regressor.fit(
        X_train, 
        y_train, 
        eval_set=[(X_val, y_val)], 
        verbose=False
    )
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡
    pred_test = xgb_regressor.predict(X_test)
    mae = mean_absolute_error(y_test, pred_test)
    rmse = calculate_rmse(y_test, pred_test)
    r2 = calculate_r2(y_test, pred_test)
    mape = calculate_mape(y_test, pred_test)
    
    # íŠ¹ì„± ì¤‘ìš”ë„ ì¶œë ¥
    feature_names = ['ê¸°ì˜¨', 'ê°•ìˆ˜ëŸ‰', 'ì¼ì¡°', 'ì¼ì‚¬ëŸ‰'] + \
                      ['íŒ¨í„´', 'ì—°ê°„_sin', 'ì—°ê°„_cos', 'ì¼ê°„_sin', 'ì¼ê°„_cos', 
                       'ê¸°ì˜¨Ã—ì¼ì‚¬ëŸ‰', 'ì¼ì¡°Ã—ì¼ì‚¬ëŸ‰', 'ë¬´ê°•ìˆ˜ì—¬ë¶€'] + \
                      ['GRU_ì˜ˆì¸¡ê°’'] # ìŠ¤íƒœí‚¹ íŠ¹ì„± ì´ë¦„ ë³€ê²½
    
    importance_dict = dict(zip(feature_names, xgb_regressor.feature_importances_))
    print("\n=== XGBoost ìŠ¤íƒœí‚¹ ëª¨ë¸ íŠ¹ì„± ì¤‘ìš”ë„ ===")
    for feature, importance in sorted(importance_dict.items(), key=lambda x: x[1], reverse=True):
        print(f"{feature}: {importance:.4f}")
    
    # ê²°ê³¼ ì‹œê°í™”
    if plotting:
        plt.figure(figsize=(15, 10))
        
        # ì˜ˆì¸¡ ê²°ê³¼ í”Œë¡¯
        plt.subplot(2, 2, 1)
        plt.plot(y_test[:200], label='Actual', alpha=0.7)
        plt.plot(pred_test[:200], label='Predicted', alpha=0.7)
        plt.xlabel("Time")
        plt.ylabel("ë°œì „ëŸ‰ (MWh)")
        plt.title(f"XGBoost ìŠ¤íƒœí‚¹ ëª¨ë¸\nMAE: {mae:.3f}, RMSE: {rmse:.3f}, RÂ²: {r2:.3f}")
        plt.legend()
        
        # ì‚°ì ë„
        plt.subplot(2, 2, 2)
        plt.scatter(y_test, pred_test, alpha=0.5)
        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
        plt.xlabel('ì‹¤ì œê°’ (Actual)')
        plt.ylabel('ì˜ˆì¸¡ê°’ (Predicted)')
        plt.title('ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’')
        
        # íŠ¹ì„± ì¤‘ìš”ë„
        plt.subplot(2, 2, 3)
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': xgb_regressor.feature_importances_
        }).sort_values('importance', ascending=True)
        
        plt.barh(importance_df['feature'], importance_df['importance'])
        plt.xlabel('ì¤‘ìš”ë„ (Importance)')
        plt.title('íŠ¹ì„± ì¤‘ìš”ë„')
        
        # ì”ì°¨ ë¶„ì„
        plt.subplot(2, 2, 4)
        residuals = y_test - pred_test
        plt.scatter(pred_test, residuals, alpha=0.5)
        plt.axhline(y=0, color='r', linestyle='--')
        plt.xlabel('ì˜ˆì¸¡ê°’ (Predicted)')
        plt.ylabel('ì”ì°¨ (Residuals)')
        plt.title('ì”ì°¨ í”Œë¡¯')
        
        plt.tight_layout()
        plt.show()
        
    return mae, rmse, r2, mape, xgb_regressor

def create_sequences_and_split_with_patterns(features, targets, pattern_features, 
                                            seq_len=24, test_size=0.2, val_size=0.1):
    """
    íŒ¨í„´ ì •ë³´ë¥¼ í¬í•¨í•œ ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± ë° ë¶„í• 
    """
    # ìŠ¤ì¼€ì¼ë§ (ê¸°ìƒ í”¼ì²˜ì™€ íƒ€ê²Ÿë§Œ ìŠ¤ì¼€ì¼)
    feature_scaler = MinMaxScaler()
    target_scaler = MinMaxScaler()
    
    features_scaled = feature_scaler.fit_transform(features)
    targets_scaled = target_scaler.fit_transform(targets.reshape(-1, 1)).flatten()
    
    # pattern_featuresì˜ ì²« ì—´ì€ 'íŒ¨í„´ ë¼ë²¨' (ì •ìˆ˜)
    # ì ˆëŒ€ ìŠ¤ì¼€ì¼ë§í•˜ì§€ ì•Šê³  ì •ìˆ˜ë¡œ ìœ ì§€
    patterns = pattern_features[:, 0].astype(int)  # e.g. 0,1,2,...,n_patterns-1
    
    # ë°ì´í„°ì…‹ ìƒì„±
    dataset = SolarPatternDataset(features_scaled, targets_scaled, patterns, seq_len)
    
    # ë°ì´í„° ë¶„í•  (ë¹„ìœ¨ ê¸°ë°˜)
    dataset_size = len(dataset)
    test_split = int(dataset_size * test_size)
    val_split = int(dataset_size * val_size)
    train_split = dataset_size - test_split - val_split
    
    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(
        dataset, [train_split, val_split, test_split]
    )
    
    return (train_dataset, val_dataset, test_dataset, 
            feature_scaler, target_scaler)


class GRU_Pattern(nn.Module):
    """
    íŒ¨í„´ ì •ë³´ë¥¼ í™œìš©í•œ GRU ëª¨ë¸
    """
    def __init__(self, weather_feature_dim=4, pattern_embedding_dim=8, hidden_dim=64, num_layers=2, n_patterns=5):
        super(GRU_Pattern, self).__init__()
        
        self.n_patterns = n_patterns
        
        # Pattern Embedding Layer
        self.pattern_embed = nn.Embedding(n_patterns, pattern_embedding_dim)
        
        # GRU (ê¸°ìƒ ë°ì´í„° + íŒ¨í„´ ì„ë² ë”©)
        gru_input_size = weather_feature_dim + pattern_embedding_dim
        self.gru = nn.GRU(input_size=gru_input_size, hidden_size=hidden_dim, 
                            num_layers=num_layers, batch_first=True, dropout=0.2)
        
        # Attention mechanism
        self.attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=4, batch_first=True)
        
        # FC layers
        self.fc1 = nn.Linear(hidden_dim, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 1)
        self.dropout = nn.Dropout(0.3)
        self.batch_norm1 = nn.BatchNorm1d(128)
        self.batch_norm2 = nn.BatchNorm1d(64)

    def forward(self, x):
        batch_size, seq_len, total_features = x.shape
        
        # ê¸°ìƒ ë°ì´í„°ì™€ íŒ¨í„´ ë°ì´í„° ë¶„ë¦¬
        weather_data = x[:, :, :4]  # ê¸°ì˜¨, ê°•ìˆ˜ëŸ‰, ì¼ì¡°, ì¼ì‚¬ëŸ‰
        pattern_data = x[:, :, 4].long()  # íŒ¨í„´ ë ˆì´ë¸”
        
        # Pattern embedding
        pattern_emb = self.pattern_embed(pattern_data)  # (batch, seq_len, embedding_dim)
        
        # Combine weather data with pattern embedding
        combined_features = torch.cat([weather_data, pattern_emb], dim=2)
        
        # GRU
        gru_out, h_n = self.gru(combined_features)
        
        # Self-attention
        attn_out, _ = self.attention(gru_out, gru_out, gru_out)
        
        # Use last hidden state
        out = attn_out[:, -1, :]  # (batch, hidden_dim)
        
        # FC layers with batch normalization
        out = self.fc1(out)
        out = self.batch_norm1(out)
        out = F.relu(out)
        out = self.dropout(out)
        
        out = self.fc2(out)
        out = self.batch_norm2(out)
        out = F.relu(out)
        out = self.dropout(out)
        
        out = self.fc3(out)
        return out.squeeze()
    
    def train_model(self, train_loader, val_loader=None, epochs=10, lr=0.001):
        """
        ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜
        """
        criterion = nn.MSELoss()
        optimizer = optim.AdamW(self.parameters(), lr=lr, weight_decay=1e-4)
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
        
        train_losses = []
        val_losses = []
        
        print(f"ì´ {epochs} ì—í¬í¬ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        start_time = time.time()
        
        best_val_loss = float('inf')
        patience = 10
        patience_counter = 0
        
        for epoch in range(epochs):
            epoch_start_time = time.time()
            
            # Training
            self.train()
            train_loss = 0
            for batch_X, batch_y in train_loader:
                # CUDAë¡œ ì´ë™
                batch_X = batch_X.to(device)
                batch_y = batch_y.to(device)
                optimizer.zero_grad()
                preds = self(batch_X)
                loss = criterion(preds, batch_y)
                loss.backward()
                
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)
                
                optimizer.step()
                train_loss += loss.item()
            
            avg_train_loss = train_loss / len(train_loader)
            train_losses.append(avg_train_loss)
            
            # Validation
            if val_loader:
                self.eval()
                val_loss = 0
                with torch.no_grad():
                    for batch_X, batch_y in val_loader:
                        # CUDAë¡œ ì´ë™
                        batch_X = batch_X.to(device)
                        batch_y = batch_y.to(device)
                        preds = self(batch_X)
                        loss = criterion(preds, batch_y)
                        val_loss += loss.item()
                
                avg_val_loss = val_loss / len(val_loader)
                val_losses.append(avg_val_loss)
                
                # Early stopping
                if avg_val_loss < best_val_loss:
                    best_val_loss = avg_val_loss
                    patience_counter = 0
                else:
                    patience_counter += 1
                
                if patience_counter >= patience:
                    print(f"Early stopping at epoch {epoch+1}")
                    break
                
                scheduler.step()
                
                # ì§„í–‰ìƒí™© ì¶œë ¥
                epoch_time = time.time() - epoch_start_time
                elapsed_time = time.time() - start_time
                remaining_epochs = epochs - epoch - 1
                estimated_remaining = (elapsed_time / (epoch + 1)) * remaining_epochs
                
                print(f"Epoch [{epoch+1:3d}/{epochs}] | "
                      f"Train Loss: {avg_train_loss:.4f} | "
                      f"Val Loss: {avg_val_loss:.4f} | "
                      f"LR: {scheduler.get_last_lr()[0]:.6f} | "
                      f"ì‹œê°„: {epoch_time:.1f}s | "
                      f"ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {estimated_remaining/60:.1f}ë¶„")
        
        total_time = time.time() - start_time
        print(f"\ní•™ìŠµ ì™„ë£Œ! ì´ ì†Œìš”ì‹œê°„: {total_time/60:.1f}ë¶„")
        
        return train_losses, val_losses
    
    def predict(self, test_loader):
        """
        ëª¨ë¸ ì˜ˆì¸¡ í•¨ìˆ˜
        """
        self.eval()
        predictions = []
        actuals = []
        
        with torch.no_grad():
            for batch_X, batch_y in test_loader:
                # CUDAë¡œ ì´ë™
                batch_X = batch_X.to(device)
                batch_y = batch_y.to(device)
                preds = self(batch_X)
                
                # Convert tensors to numpy arrays
                preds_np = preds.cpu().numpy()
                actuals_np = batch_y.cpu().numpy()
                
                # Ensure the numpy arrays are always at least 1-dimensional.
                # This prevents the "iteration over a 0-d array" error for single-item batches.
                if preds_np.ndim == 0:
                    predictions.append(preds_np.item())
                    actuals.append(actuals_np.item())
                else:
                    predictions.extend(preds_np)
                    actuals.extend(actuals_np)
        
        return np.array(predictions), np.array(actuals)
    
if __name__ == "__main__":
    try:
        print("ë°ì´í„° ë¡œë”© ì¤‘...")
        data_df = pd.read_csv(data_path)
        
        # 1. ë°ì´í„° ì „ì²˜ë¦¬
        features, targets = preprocess_data(data_df)
        if features is None or targets is None:
            raise ValueError("ë°ì´í„° ì „ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì›ë³¸ ë°ì´í„°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

        # 2. íŒ¨í„´ ì¶”ì¶œ ë° íŒ¨í„´ ê¸°ë°˜ íŠ¹ì„± ìƒì„±
        print("\níŒ¨í„´ ì¶”ì¶œ ì¤‘...")
        pattern_extractor = PatternExtractor(n_patterns=5)
        pattern_features = create_pattern_features(features, targets, pattern_extractor)
        
        seq_len = 24
        
        # 3. ë°ì´í„°ì…‹ ì‹œê°„ìˆœ ë¶„í•  (70% train, 15% validation, 15% test)
        print("\nì‹œê°„ìˆœìœ¼ë¡œ ë°ì´í„° ë¶„í• ...")
        total_size = len(features)
        train_end = int(total_size * 0.7)
        val_end = int(total_size * 0.85)

        features_train, targets_train, pattern_train = features[:train_end], targets[:train_end], pattern_features[:train_end]
        features_val, targets_val, pattern_val = features[train_end:val_end], targets[train_end:val_end], pattern_features[train_end:val_end]
        features_test, targets_test, pattern_test = features[val_end:], targets[val_end:], pattern_features[val_end:]
        
        print(f"Train set size: {len(features_train)}")
        print(f"Validation set size: {len(features_val)}")
        print(f"Test set size: {len(features_test)}")

        # 4. ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ë° PyTorch Dataset/DataLoader ìƒì„±
        feature_scaler = MinMaxScaler()
        target_scaler = MinMaxScaler()

        features_train_scaled = feature_scaler.fit_transform(features_train)
        features_val_scaled = feature_scaler.transform(features_val)
        features_test_scaled = feature_scaler.transform(features_test)

        targets_train_scaled = target_scaler.fit_transform(targets_train.reshape(-1, 1)).flatten()
        targets_val_scaled = target_scaler.transform(targets_val.reshape(-1, 1)).flatten()
        targets_test_scaled = target_scaler.transform(targets_test.reshape(-1, 1)).flatten()
        
        train_dataset = SolarPatternDataset(features_train_scaled, targets_train_scaled, pattern_train[:, 0], seq_len)
        val_dataset = SolarPatternDataset(features_val_scaled, targets_val_scaled, pattern_val[:, 0], seq_len)
        test_dataset = SolarPatternDataset(features_test_scaled, targets_test_scaled, pattern_test[:, 0], seq_len)
        
        batch_size = 32
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        # ìˆœì„œ ìœ ì§€ë¥¼ ìœ„í•œ shuffle=False DataLoader
        train_loader_stack = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)
        val_loader_stack = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        
        # 6. GRU ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
        print("\n" + "="*60)
        print("GRU ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        print("="*60)
        
        gru_model = GRU_Pattern(weather_feature_dim=4, pattern_embedding_dim=8, 
                               hidden_dim=128, num_layers=2, n_patterns=5)
        
        gru_model.to(device)
        
        gru_train_losses, gru_val_losses = gru_model.train_model(
            train_loader=train_loader,
            val_loader=val_loader,
            epochs=100,
            lr=0.001
        )
        
        
        # GRU ì˜ˆì¸¡ ë° í‰ê°€
        gru_predictions_scaled, gru_actuals_scaled = gru_model.predict(test_loader)
        gru_predictions_original = target_scaler.inverse_transform(gru_predictions_scaled.reshape(-1, 1)).flatten()
        gru_actuals_original = target_scaler.inverse_transform(gru_actuals_scaled.reshape(-1, 1)).flatten()
        
        # ê¸°ì¡´ ê°œë³„ ë©”íŠ¸ë¦­ ê³„ì‚° â†’ calculate_all_metricsë¡œ ë³€ê²½
        gru_metrics = calculate_all_metrics(gru_actuals_original, gru_predictions_original, print_details=True)
        mae_gru = gru_metrics['mae']
        rmse_gru = gru_metrics['rmse']
        r2_gru = gru_metrics['r2']
        mape_gru = gru_metrics['mape_improved']
        
        print(f"\n=== GRU ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ===")
        print(f"MAE: {mae_gru:.4f}")
        print(f"RMSE: {rmse_gru:.4f}")
        print(f"RÂ²: {r2_gru:.4f}")
        print(f"MAPE: {mape_gru:.4f}%")
        
        # 8. ìŠ¤íƒœí‚¹ì„ ìœ„í•œ ì˜ˆì¸¡ê°’ ìƒì„±
        print("\n" + "="*60)
        print("ìŠ¤íƒœí‚¹ì„ ìœ„í•œ GRU ì˜ˆì¸¡ê°’ ìƒì„± ì¤‘...")
        print("="*60)
        
        # Train set ì˜ˆì¸¡ê°’ ìƒì„±
        gru_preds_train_scaled, _ = gru_model.predict(train_loader_stack)
        
        # Validation set ì˜ˆì¸¡ê°’ ìƒì„±
        gru_preds_val_scaled, _ = gru_model.predict(val_loader_stack)
        
        # ì›ë³¸ ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
        gru_preds_train = target_scaler.inverse_transform(gru_preds_train_scaled.reshape(-1, 1))
        
        gru_preds_val = target_scaler.inverse_transform(gru_preds_val_scaled.reshape(-1, 1))
        
        gru_preds_test = gru_predictions_original.reshape(-1, 1)
        
        # 9. XGBoost ìŠ¤íƒœí‚¹ í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
        print("\nXGBoost ìŠ¤íƒœí‚¹ì„ ìœ„í•œ íŠ¹ì„± ê²°í•© ì¤‘...")
        
        # ì‹œí€€ìŠ¤ ìƒì„±ìœ¼ë¡œ ì¸í•´ ë°ì´í„° ê¸¸ì´ê°€ ì¤„ì–´ë“  ê²ƒì„ ë°˜ì˜
        X_train_stack = np.hstack([
            features_train[seq_len:],           # ì›ë³¸ ê¸°ìƒ íŠ¹ì„± (4ê°œ)
            pattern_train[seq_len:],            # íŒ¨í„´ íŠ¹ì„± (8ê°œ)             
            gru_preds_train                     # GRU ì˜ˆì¸¡ê°’ (1ê°œ)
        ])
        y_train_stack = targets_train[seq_len:]
        
        X_val_stack = np.hstack([
            features_val[seq_len:],
            pattern_val[seq_len:],
            gru_preds_val
        ])
        y_val_stack = targets_val[seq_len:]
        
        X_test_stack = np.hstack([
            features_test[seq_len:],
            pattern_test[seq_len:],
            gru_preds_test
        ])
        y_test_stack = targets_test[seq_len:]
        
        print(f"ìŠ¤íƒœí‚¹ íŠ¹ì„± ì°¨ì›: {X_train_stack.shape[1]} (ê¸°ìƒ 4 + íŒ¨í„´ 8 + GRU 1)")
        
        # 10. XGBoost ìŠ¤íƒœí‚¹ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
        print("\n" + "="*60)
        print("XGBoost ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        print("="*60)
        
        # íŠ¹ì„± ì´ë¦„ ì •ì˜
        feature_names = ['ê¸°ì˜¨', 'ê°•ìˆ˜ëŸ‰', 'ì¼ì¡°', 'ì¼ì‚¬ëŸ‰'] + \
                       ['íŒ¨í„´', 'ì—°ê°„_sin', 'ì—°ê°„_cos', 'ì¼ê°„_sin', 'ì¼ê°„_cos', 
                        'ê¸°ì˜¨Ã—ì¼ì‚¬ëŸ‰', 'ì¼ì¡°Ã—ì¼ì‚¬ëŸ‰', 'ë¬´ê°•ìˆ˜ì—¬ë¶€'] + \
                       ['GRU_ì˜ˆì¸¡ê°’']
        
        # XGBoost ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ
        xgb_stacking_regressor = xgb.XGBRegressor(
            gamma=0.3,
            n_estimators=500,
            learning_rate=0.05,
            max_depth=7,
            min_child_weight=2,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            early_stopping_rounds=50,
            reg_alpha=0.05,
            reg_lambda=0.1,
            eval_metric='mae'
        )
        
        print("XGBoost ìŠ¤íƒœí‚¹ ëª¨ë¸ í•™ìŠµ ì¤‘...")
        xgb_stacking_regressor.fit(
            X_train_stack, 
            y_train_stack, 
            eval_set=[(X_val_stack, y_val_stack)], 
            verbose=100
        )
        
        # ìŠ¤íƒœí‚¹ ëª¨ë¸ ì˜ˆì¸¡ ë° í‰ê°€
        stacked_pred_test = xgb_stacking_regressor.predict(X_test_stack)
        
        # ê¸°ì¡´ ê°œë³„ ë©”íŠ¸ë¦­ ê³„ì‚° â†’ calculate_all_metricsë¡œ ë³€ê²½
        stacked_metrics = calculate_all_metrics(y_test_stack, stacked_pred_test, print_details=True)
        mae_stacked = stacked_metrics['mae']
        rmse_stacked = stacked_metrics['rmse']
        r2_stacked = stacked_metrics['r2']
        mape_stacked = stacked_metrics['mape_improved']
        
        print(f"\n=== XGBoost ìŠ¤íƒœí‚¹ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ===")
        print(f"MAE: {mae_stacked:.4f}")
        print(f"RMSE: {rmse_stacked:.4f}")
        print(f"RÂ²: {r2_stacked:.4f}")
        print(f"MAPE: {mape_stacked:.4f}%")
        
        # 11. íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
        print("\n=== XGBoost ìŠ¤íƒœí‚¹ ëª¨ë¸ íŠ¹ì„± ì¤‘ìš”ë„ ===")
        importance_dict = dict(zip(feature_names, xgb_stacking_regressor.feature_importances_))
        for feature, importance in sorted(importance_dict.items(), key=lambda x: x[1], reverse=True):
            print(f"{feature}: {importance:.4f}")
        
        # 12. ìµœì¢… ì„±ëŠ¥ ë¹„êµ
        print(f"\n{'='*80}")
        print("ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
        print(f"{'='*80}")
        print(f"{'ëª¨ë¸':<20} {'MAE':<10} {'RMSE':<10} {'RÂ²':<10} {'MAPE (%)':<10}")
        print("-" * 60)
        print(f"{'GRU':<20} {mae_gru:<10.4f} {rmse_gru:<10.4f} {r2_gru:<10.4f} {mape_gru:<10.2f}")
        print(f"{'XGBoost Stacking':<20} {mae_stacked:<10.4f} {rmse_stacked:<10.4f} {r2_stacked:<10.4f} {mape_stacked:<10.2f}")
        
        
        # 13. ê²°ê³¼ ì‹œê°í™”
        print("\nê²°ê³¼ ì‹œê°í™” ì¤‘...")
        plt.figure(figsize=(20, 15))
        
        # 1. í•™ìŠµ ê³¡ì„ 
        plt.subplot(3, 4, 2)
        if gru_train_losses and gru_val_losses:
            plt.plot(gru_train_losses, label='GRU Train Loss', alpha=0.7)
            plt.plot(gru_val_losses, label='GRU Val Loss', alpha=0.7)
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('GRU Learning Curves')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 2. ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ
        test_range = min(200, len(y_test_stack))
        
        
        plt.subplot(3, 4, 4)
        plt.plot(gru_actuals_original[:test_range], label='Actual', alpha=0.8, linewidth=2)
        plt.plot(gru_predictions_original[:test_range], label='GRU', alpha=0.7)
        plt.xlabel('Time')
        plt.ylabel('ë°œì „ëŸ‰ (MWh)')
        plt.title(f'GRU ì˜ˆì¸¡ ê²°ê³¼\nMAE: {mae_gru:.3f}, RÂ²: {r2_gru:.3f}')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.subplot(3, 4, 5)
        plt.plot(y_test_stack[:test_range], label='Actual', alpha=0.8, linewidth=2)
        plt.plot(stacked_pred_test[:test_range], label='Stacked', alpha=0.7)
        plt.xlabel('Time')
        plt.ylabel('ë°œì „ëŸ‰ (MWh)')
        plt.title(f'XGBoost ìŠ¤íƒœí‚¹ ê²°ê³¼\nMAE: {mae_stacked:.3f}, RÂ²: {r2_stacked:.3f}')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 3. ì‚°ì ë„ (ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’)
        
        plt.subplot(3, 4, 7)
        plt.scatter(gru_actuals_original, gru_predictions_original, alpha=0.5)
        plt.plot([gru_actuals_original.min(), gru_actuals_original.max()], 
                [gru_actuals_original.min(), gru_actuals_original.max()], 'r--', lw=2)
        plt.xlabel('ì‹¤ì œê°’')
        plt.ylabel('ì˜ˆì¸¡ê°’')
        plt.title('GRU: ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’')
        plt.grid(True, alpha=0.3)
        
        plt.subplot(3, 4, 8)
        plt.scatter(y_test_stack, stacked_pred_test, alpha=0.5)
        plt.plot([y_test_stack.min(), y_test_stack.max()], 
                [y_test_stack.min(), y_test_stack.max()], 'r--', lw=2)
        plt.xlabel('ì‹¤ì œê°’')
        plt.ylabel('ì˜ˆì¸¡ê°’')
        plt.title('Stacked: ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’')
        plt.grid(True, alpha=0.3)
        # for debug
        print("feature_names ê¸¸ì´:", len(feature_names))
        print("feature_importances_ ê¸¸ì´:", len(xgb_stacking_regressor.feature_importances_))
        # 4. íŠ¹ì„± ì¤‘ìš”ë„
        plt.subplot(3, 4, 9)
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': xgb_stacking_regressor.feature_importances_
        }).sort_values('importance', ascending=True)
        
        plt.barh(importance_df['feature'], importance_df['importance'])
        plt.xlabel('ì¤‘ìš”ë„')
        plt.title('XGBoost ìŠ¤íƒœí‚¹ íŠ¹ì„± ì¤‘ìš”ë„')
        plt.grid(True, alpha=0.3)
        
        # 5. ì”ì°¨ ë¶„ì„
        gru_residuals = gru_actuals_original - gru_predictions_original
        stacked_residuals = y_test_stack - stacked_pred_test
        
        
        plt.subplot(3, 4, 11)
        plt.scatter(gru_predictions_original, gru_residuals, alpha=0.5)
        plt.axhline(y=0, color='r', linestyle='--')
        plt.xlabel('ì˜ˆì¸¡ê°’')
        plt.ylabel('ì”ì°¨')
        plt.title('GRU ì”ì°¨ í”Œë¡¯')
        plt.grid(True, alpha=0.3)
        
        plt.subplot(3, 4, 12)
        plt.scatter(stacked_pred_test, stacked_residuals, alpha=0.5)
        plt.axhline(y=0, color='r', linestyle='--')
        plt.xlabel('ì˜ˆì¸¡ê°’')
        plt.ylabel('ì”ì°¨')
        plt.title('Stacked ì”ì°¨ í”Œë¡¯')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        # 14. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¹„êµ ë§‰ëŒ€ ê·¸ë˜í”„
        plt.figure(figsize=(20, 5))
        
        models = ['GRU', 'XGBoost\nStacking']
        mae_scores = [mae_gru, mae_stacked]
        rmse_scores = [rmse_gru, rmse_stacked]
        r2_scores = [r2_gru, r2_stacked]
        mape_scores = [mape_gru, mape_stacked]
        
        x = np.arange(len(models))
        width = 0.25
        
        plt.subplot(1, 4, 1)
        plt.bar(x, mae_scores, width, label='MAE', alpha=0.8)
        plt.xlabel('ëª¨ë¸')
        plt.ylabel('MAE')
        plt.title('MAE ë¹„êµ')
        plt.xticks(x, models)
        plt.grid(True, alpha=0.3)
        
        plt.subplot(1, 4, 2)
        plt.bar(x, rmse_scores, width, label='RMSE', alpha=0.8, color='orange')
        plt.xlabel('ëª¨ë¸')
        plt.ylabel('RMSE')
        plt.title('RMSE ë¹„êµ')
        plt.xticks(x, models)
        plt.grid(True, alpha=0.3)
        
        plt.subplot(1, 4, 3)
        plt.bar(x, r2_scores, width, label='RÂ²', alpha=0.8, color='green')
        plt.xlabel('ëª¨ë¸')
        plt.ylabel('RÂ²')
        plt.title('RÂ² ë¹„êµ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)')
        plt.xticks(x, models)
        plt.grid(True, alpha=0.3)
        
        plt.subplot(1, 4, 4)
        plt.bar(x, mape_scores, width, label='MAPE (%)', alpha=0.8, color='red')
        plt.xlabel('ëª¨ë¸')
        plt.ylabel('MAPE (%)')
        plt.title('MAPE ë¹„êµ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)')
        plt.xticks(x, models)
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        
    except FileNotFoundError:
        print(f"Error: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”: {data_path}")
    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()